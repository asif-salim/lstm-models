{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Concatenate, Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,  mean_absolute_error, r2_score\n",
    " \n",
    "# tensorflow.reset_default_graph()\n",
    "tensorflow.random.set_seed(0)\n",
    "# random.seed(0)\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=14, day_offset=5):\n",
    "    dataY= []\n",
    "    dataX1=numpy.zeros([(len(dataset)-look_back),2,look_back])\n",
    "    dataX2=numpy.zeros([(len(dataset)-look_back),2,look_back])\n",
    "    dataX3=numpy.zeros([(len(dataset)-look_back),2,look_back])\n",
    "    \n",
    "    #print(dataX.shape)\n",
    "    for i in range(look_back,len(dataset)):\n",
    "       # print(i)\n",
    "        a = numpy.zeros([6,look_back])\n",
    "        t1=dataset[(i-look_back):i, 0]\n",
    "        t1=numpy.reshape(t1,[1,look_back])\n",
    "        t4=dataset[(i-look_back):i, 48]\n",
    "        t4=numpy.reshape(t4,[1,look_back])\n",
    "        #print(t1.shape)\n",
    "        t2=dataset[i, 48-look_back:48]\n",
    "        t2=numpy.reshape(t2,[1,look_back])\n",
    "        t6=dataset[i,-(look_back+3):-3]\n",
    "        t6=numpy.reshape(t6,[1,look_back])\n",
    "        t3=numpy.zeros([1,look_back])\n",
    "        if i>=((day_offset+1)*7+look_back):\n",
    "            t3[0,0:day_offset]=[dataset[j,0] for j in range(i-(((day_offset+1)*7)+look_back),i-(look_back+7),7)]\n",
    "        t5=numpy.zeros([1,look_back])\n",
    "        if i>=((day_offset+1)*7+look_back):\n",
    "            t5[0,0:day_offset]=[dataset[j,48] for j in range(i-(((day_offset+1)*7)+look_back),i-(look_back+7),7)]\n",
    "            \n",
    "            \n",
    "        #print(t2.shape)\n",
    "        a[0,:] = t1\n",
    "        a[1,:] = t4\n",
    "        a[2,:] = t2\n",
    "        a[3,:] = t6\n",
    "        a[4,:] = t3\n",
    "        a[5,:] = t5\n",
    "        dataX1[i-look_back,:,:]=a[0:2,:]\n",
    "        dataX2[i-look_back,:,:]=a[2:4,:]\n",
    "        dataX3[i-look_back,:,:]=a[4:,:]\n",
    "#         a = numpy.concatenate([dataset[(i-look_back-7):i-7, 0], dataset[i,-14:]],axis=1)\n",
    "        \n",
    "        #dataX.append(a)\n",
    "        dataY.append(dataset[i,-1])\n",
    "    return numpy.array(dataX1),numpy.array(dataX2),numpy.array(dataX3),numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(datarange, categorical=[]):  \n",
    "    datarange= pd.DataFrame(datarange)\n",
    "    if not categorical:\n",
    "        meandata=datarange.mean()\n",
    "        meandata=meandata.to_numpy()\n",
    "    else:\n",
    "        meandata=datarange.mean()\n",
    "        meandata=meandata.to_numpy()\n",
    "        \n",
    "        modedata = datarange.mode()\n",
    "        modedata = modedata.to_numpy()\n",
    "        modedata = modedata[0,:]\n",
    "        \n",
    "        for i in categorical:\n",
    "                meandata[i-1] = modedata[i]\n",
    "                \n",
    "    datetime_series = pd.to_datetime(datarange['fltdat'])\n",
    "    miss_idx=pd.date_range(start = '01-01-2015', end = '31-12-2019' ).difference(datetime_series)\n",
    "    datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
    "    datarange=datarange.set_index(datetime_index)\n",
    "\n",
    "    datarange.drop('fltdat',axis=1,inplace=True)\n",
    "    newidx = pd.date_range('01-01-2015', '31-12-2019')\n",
    "    datarange = datarange.reindex(newidx, fill_value=0)\n",
    "    \n",
    "    meandata=meandata.reshape(1,meandata.shape[0])\n",
    "    dat = numpy.tile(meandata, [miss_idx.shape[0],1])\n",
    "    datarange.loc[miss_idx]=dat\n",
    "    return datarange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_test, y_pred):\n",
    "        import numpy as np\n",
    "        t = np.array(y_test)\n",
    "        p = np.array(y_pred)\n",
    "        mae = list()\n",
    "        mape = list()\n",
    "        for i in range(len(t)):\n",
    "            if (t[i] == 0):\n",
    "                mae.append(abs(p[i]))\n",
    "            else:\n",
    "                mae.append(float(abs(t[i] - p[i])))\n",
    "                mape.append(float(abs((t[i] - p[i])/t[i])))\n",
    "        return np.mean(mae) , np.mean(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fltdat', 'paxcntfc', 'fltnum', 'legorg', 'legdst', 'acrtypcod',\n",
      "       'keyidr', 'totpaylodwgt', 'totpaylodvol', 'totpaylodpos', 'totsetfc',\n",
      "       'totpaxwgt', 'dp_51', 'dp_50', 'dp_49', 'dp_48', 'dp_47', 'dp_46',\n",
      "       'dp_45', 'dp_44', 'dp_43', 'dp_42', 'dp_41', 'dp_40', 'dp_39', 'dp_38',\n",
      "       'dp_37', 'dp_36', 'dp_35', 'dp_34', 'dp_33', 'dp_32', 'dp_31', 'dp_30',\n",
      "       'dp_29', 'dp_28', 'dp_27', 'dp_26', 'dp_25', 'dp_24', 'dp_23', 'dp_22',\n",
      "       'dp_21', 'dp_20', 'dp_19', 'dp_18', 'dp_17', 'dp_16', 'dp_15', 'dp_14',\n",
      "       'dp_13', 'dp_12', 'dp_11', 'dp_10', 'dp_9', 'dp_8', 'dp_7', 'dp_6',\n",
      "       'dp_5', 'dp_4', 'dp_3', 'dp_2', 'dp_1'],\n",
      "      dtype='object')\n",
      "Index(['fltdat', 'paxcntfc', 'acrtypcod', 'totpaylodwgt', 'totpaxwgt', 'dp_51',\n",
      "       'dp_50', 'dp_49', 'dp_48', 'dp_47', 'dp_46', 'dp_45', 'dp_44', 'dp_43',\n",
      "       'dp_42', 'dp_41', 'dp_40', 'dp_39', 'dp_38', 'dp_37', 'dp_36', 'dp_35',\n",
      "       'dp_34', 'dp_33', 'dp_32', 'dp_31', 'dp_30', 'dp_29', 'dp_28', 'dp_27',\n",
      "       'dp_26', 'dp_25', 'dp_24', 'dp_23', 'dp_22', 'dp_21', 'dp_20', 'dp_19',\n",
      "       'dp_18', 'dp_17', 'dp_16', 'dp_15', 'dp_14', 'dp_13', 'dp_12', 'dp_11',\n",
      "       'dp_10', 'dp_9', 'dp_8', 'paxcnty', 'dcp_51', 'dcp_50', 'dcp_49',\n",
      "       'dcp_48', 'dcp_47', 'dcp_46', 'dcp_45', 'dcp_44', 'dcp_43', 'dcp_42',\n",
      "       'dcp_41', 'dcp_40', 'dcp_39', 'dcp_38', 'dcp_37', 'dcp_36', 'dcp_35',\n",
      "       'dcp_34', 'dcp_33', 'dcp_3', 'dcp_31', 'dcp_30', 'dcp_29', 'dcp_28',\n",
      "       'dcp_27', 'dcp_26', 'dcp_25', 'dcp_24', 'dcp_23', 'dcp_22', 'dcp_21',\n",
      "       'dcp_20', 'dcp_19', 'dcp_18', 'dcp_17', 'dcp_16', 'dcp_15', 'dcp_14',\n",
      "       'dcp_13', 'dcp_12', 'dcp_11', 'dcp_10', 'dcp_9', 'dcp_8'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('data/rmscapfc.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "print(dataframe.columns)\n",
    "dataframe.drop(dataframe.columns[[2,3,4,6,8,9,10,56,57,58,59,60,61,62] ], axis=1, inplace=True)\n",
    "\n",
    "dataframe2 = read_csv('data/rmscapy.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "# print(dataframe2.columns)\n",
    "f_column = dataframe2[[\"paxcnty\", \"dcp_51\", \"dcp_50\", \"dcp_49\", \"dcp_48\", \"dcp_47\", \"dcp_46\",\n",
    "       \"dcp_45\", \"dcp_44\", \"dcp_43\", \"dcp_42\", \"dcp_41\", \"dcp_40\", \"dcp_39\",\n",
    "       \"dcp_38\", \"dcp_37\", \"dcp_36\", \"dcp_35\", \"dcp_34\", \"dcp_33\", \"dcp_3\",\n",
    "       \"dcp_31\", \"dcp_30\", \"dcp_29\", \"dcp_28\", \"dcp_27\", \"dcp_26\", \"dcp_25\",\n",
    "       \"dcp_24\", \"dcp_23\", \"dcp_22\", \"dcp_21\", \"dcp_20\", \"dcp_19\", \"dcp_18\",\n",
    "       \"dcp_17\", \"dcp_16\", \"dcp_15\", \"dcp_14\", \"dcp_13\", \"dcp_12\", \"dcp_11\",\n",
    "       \"dcp_10\", \"dcp_9\", \"dcp_8\"]]\n",
    " \n",
    "\n",
    "dataframe = pd.concat([dataframe,f_column], axis = 1)\n",
    "# print(dataframe.columns)\n",
    "dataframe3 = read_csv('data/uldfc.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "\n",
    "dataframe3.drop(dataframe3.columns[[1,2,3] ], axis=1, inplace=True)\n",
    "dataframe4 = read_csv('data/uldy.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "dataframe4.drop(dataframe4.columns[[1,2,3] ], axis=1, inplace=True)\n",
    "f_column = dataframe4[[\"county\"]]\n",
    "dataframe3 = pd.concat([dataframe3,f_column], axis = 1)\n",
    "print(dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM=[0,1825,1825,1817,1816,1812,1821,1825,1824,1819,1825,1825,1824,1826,1819,1825,1822,1823,1813, 1826, 1820]\n",
    "NUMuld=[0,1817,1574,1808,1802,1807,1808,1730,1817,1385,1820,1816,1606,1819,1810,1817,421,1813,434,1532,1814]\n",
    "cat_inp=[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iist\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  import sys\n",
      "C:\\Users\\iist\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# dataset, datasetY = numpy.empty([1805,3,look_back]), []\n",
    "for i in range(0,len(NUM)-1):\n",
    "#     print(i)\n",
    "    datasub = dataframe.iloc[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "#     datasub=datasetall[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "    datasub = missing_values(datasub, cat_inp)\n",
    "    datasub = datasub.values\n",
    "    datasub = datasub.astype('float32')\n",
    "    \n",
    "    datasubuld = dataframe3.iloc[sum(NUMuld[0:i+1]):sum(NUMuld[0:i+2]),:]\n",
    "#     datasub=datasetall[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "    datasubuld = missing_values(datasubuld)\n",
    "    datasubuld = datasubuld.values\n",
    "    datasubuld = datasubuld.astype('float32')\n",
    "    \n",
    "    datasub = numpy.concatenate([datasub, datasubuld], axis=1)\n",
    "    if i==0:\n",
    "        data = datasub\n",
    "    else:\n",
    "        data = numpy.concatenate([data, datasub], axis =0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36520, 96)\n"
     ]
    }
   ],
   "source": [
    "out = data[:,2] - data[:,3] - (data[:,-1]*data[:,-2]*114)\n",
    "out = out.reshape(out.shape[0],1)\n",
    "data = numpy.concatenate([data,out], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_data_build(data, look_back):\n",
    "    for i in range(0,20):\n",
    "        datasub = data[i*1826:((i+1)*1826),:]\n",
    "        X1, X2, X3, Y = create_dataset(datasub, look_back)\n",
    "        Y = Y.reshape(Y.shape[0],1)\n",
    "        if i==0: \n",
    "            data_LSTM_X1, data_LSTM_X2, data_LSTM_X3 = X1, X2, X3\n",
    "            data_LSTM_Y = Y\n",
    "        else:\n",
    "            data_LSTM_X1,data_LSTM_X2, data_LSTM_X3 = numpy.concatenate([data_LSTM_X1, X1],axis=0), numpy.concatenate([data_LSTM_X2, X2],axis=0),  numpy.concatenate([data_LSTM_X3, X3],axis=0)  \n",
    "            data_LSTM_Y = numpy.concatenate([data_LSTM_Y, Y],axis=0)\n",
    "    return data_LSTM_X1, data_LSTM_X2, data_LSTM_X3, data_LSTM_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_build(X, Y, m1, m2):\n",
    "    spliter = int(X.shape[0]/20)\n",
    "    tot_len = m1\n",
    "    train_len = m2\n",
    "    for i in range(0,20):\n",
    "        Xsub = X[i*spliter:((i+1)*spliter),:]\n",
    "        Ysub = Y[i*spliter:((i+1)*spliter)]\n",
    "        if i==0:\n",
    "            trainX = Xsub[0:m2,:]\n",
    "            testX = Xsub[m2:m1,:]\n",
    "            trainY = Ysub[0:m2]\n",
    "            testY = Ysub[m2:m1]\n",
    "        else:\n",
    "            trainX = numpy.concatenate([trainX, Xsub[0:m2,:]], axis=0)\n",
    "            testX = numpy.concatenate([testX, Xsub[m2:m1,:]], axis=0)\n",
    "            trainY = numpy.concatenate([trainY, Ysub[0:m2]], axis=0)\n",
    "            testY = numpy.concatenate([testY, Ysub[m2:m1]], axis=0)\n",
    "        \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(trainX1, trainX2, trainX3, testX1, testX2, testX3, trainY, testY, units, saving =False, month=None, EarlyStop = False):\n",
    "    \n",
    "    if EarlyStop:\n",
    "        callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                                            mode = 'min', restore_best_weights=True)\n",
    "    \n",
    "    inp1 = Input(shape=(2, trainX1.shape[2]))\n",
    "    inp2 = Input(shape=(2, trainX2.shape[2]))\n",
    "    inp3 = Input(shape=(2, trainX3.shape[2]))\n",
    "\n",
    "    LS1 = LSTM(units, input_shape=(2, trainX1.shape[2]))\n",
    "    out1 = LS1(inp1)\n",
    "    LS2 = LSTM(units, input_shape=(2, trainX2.shape[2]))\n",
    "    out2 = LS2(inp2)\n",
    "    LS3 = LSTM(units, input_shape=(2, trainX3.shape[2]))\n",
    "    out3 = LS3(inp3)\n",
    "\n",
    "    mrg = Concatenate(axis=1)([out1,out2,out3])\n",
    "    op = Dense(1)(mrg)\n",
    "\n",
    "    model = Model(inputs=[inp1, inp2, inp3], outputs=op)\n",
    "#     model.add(Dense(1))\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    if EarlyStop:\n",
    "        history =model.fit([trainX1, trainX2, trainX3], trainY, epochs=100, batch_size=150, \n",
    "                           validation_data=([testX1,testX2,testX3], testY),verbose=1,callbacks=[callback])\n",
    "    else:\n",
    "        history =model.fit([trainX1, trainX2, trainX3], trainY, epochs=100, batch_size=150, \n",
    "                           validation_data=([testX1,testX2,testX3], testY),verbose=1)\n",
    "                \n",
    "    testPredict = model.predict([testX1,testX2,testX3])\n",
    "                \n",
    "    sh = testPredict.shape\n",
    "    inv_yhat = testPredict.reshape(sh[0]*sh[1],1)\n",
    "    inv_yhat = numpy.concatenate([ data[0:inv_yhat.shape[0],0:95], inv_yhat], axis=1)\n",
    "\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "    testY = testY.reshape(sh[0]*sh[1],1)\n",
    " \n",
    "    inv_y = numpy.concatenate([data[0:testY.shape[0],0:95], testY], axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-1]\n",
    "             \n",
    "    rmse = numpy.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    mae, mape = mean_absolute_percentage_error(inv_y, inv_yhat) \n",
    "    r2 = r2_score(inv_y, inv_yhat)\n",
    "    \n",
    "    res=[]\n",
    "    if saving:\n",
    "        inv_y = inv_y.reshape(inv_y.shape[0],1) \n",
    "        inv_yhat = inv_yhat.reshape(inv_yhat.shape[0],1) \n",
    "        res = numpy.concatenate([inv_y, inv_yhat], axis=1)\n",
    "        df = pd.DataFrame(res)\n",
    "        res = df.to_csv(\"ds_\" + month + \".csv\", index = False)\n",
    "    return rmse, mape,mae,r2, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(lag_vec = [7,14,21,28], units_vec = [2,4,8,16,32,64,128]):\n",
    "    results = numpy.zeros([4,3,len(lag_vec),len(units_vec)])\n",
    "    \n",
    "    for folds in range(0,3):\n",
    "        for i in range(0,len(lag_vec)):\n",
    "            data_LSTM_X1,data_LSTM_X2,data_LSTM_X3,data_LSTM_Y = sequence_data_build(data, lag_vec[i])\n",
    "            totday = int(data_LSTM_X1.shape[0]/20)\n",
    "            m1 = [totday-152, totday-121, totday-91, ]\n",
    "            m2 = [m1[0]-31, m1[1]-31, m1[2]-30, ]\n",
    "            lag = lag_vec[i]\n",
    "            trainX1, trainY, testX1, testY= train_test_build(data_LSTM_X1, data_LSTM_Y, m1[folds], m2[folds])\n",
    "            trainX2, trainY, testX2, testY= train_test_build(data_LSTM_X2, data_LSTM_Y, m1[folds], m2[folds])\n",
    "            trainX3, trainY, testX3, testY= train_test_build(data_LSTM_X3, data_LSTM_Y, m1[folds], m2[folds])\n",
    "            testYcopy=testY\n",
    "            for j in range(0, len(units_vec)):\n",
    "                print(\" \")\n",
    "                print(\" \")\n",
    "                print(\" \")\n",
    "                print(\"------------------------------------------------\")\n",
    "                print(\"fold: {0}, lag: {1}, units: {2}\".format(folds, lag_vec[i], units_vec[j]))\n",
    "                units = units_vec[j]\n",
    "                rmse, mape, mae, r2, his = model_build(trainX1, trainX2, trainX3, testX1, testX2, testX3, trainY, testY, units, EarlyStop = True) \n",
    "                results[0,folds,i,j] = rmse \n",
    "                results[2,folds,i,j], results[1,folds,i,j] = mae, mape\n",
    "                results[3,folds,i,j] = r2\n",
    "    return results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 2\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 11s 341us/sample - loss: 0.2193 - val_loss: 0.1510\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1670 - val_loss: 0.1211\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1425 - val_loss: 0.0993\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1306 - val_loss: 0.1015\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 3s 87us/sample - loss: 0.1271 - val_loss: 0.0954\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1250 - val_loss: 0.0935\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1235 - val_loss: 0.0933\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1223 - val_loss: 0.0926\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1213 - val_loss: 0.0929\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1204 - val_loss: 0.0960\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1197 - val_loss: 0.0962\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1192 - val_loss: 0.0955\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 3s 88us/sample - loss: 0.1187 - val_loss: 0.0927\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 3s 90us/sample - loss: 0.1183 - val_loss: 0.0922\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 3s 89us/sample - loss: 0.1177 - val_loss: 0.0910\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 3s 90us/sample - loss: 0.1174 - val_loss: 0.0909\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1170 - val_loss: 0.0933\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1167 - val_loss: 0.0921\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1163 - val_loss: 0.0919\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1160 - val_loss: 0.0894\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1157 - val_loss: 0.0913\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1155 - val_loss: 0.0912\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1151 - val_loss: 0.0945\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1149 - val_loss: 0.0922\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1146 - val_loss: 0.0879\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1144 - val_loss: 0.0916\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1141 - val_loss: 0.0887\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1139 - val_loss: 0.0902\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1136 - val_loss: 0.0900\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1134 - val_loss: 0.0905\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1131 - val_loss: 0.0908\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1129 - val_loss: 0.0913\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1127 - val_loss: 0.0912\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1125 - val_loss: 0.0891\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1122 - val_loss: 0.0889\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 4\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 9s 289us/sample - loss: 0.1646 - val_loss: 0.1123\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1323 - val_loss: 0.1014\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1264 - val_loss: 0.0984\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1241 - val_loss: 0.1004\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.1226 - val_loss: 0.0943\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1214 - val_loss: 0.0977\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 3s 87us/sample - loss: 0.1202 - val_loss: 0.0960\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1192 - val_loss: 0.0936\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1182 - val_loss: 0.0933\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.1173 - val_loss: 0.0918\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1166 - val_loss: 0.0962\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1160 - val_loss: 0.0948\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1154 - val_loss: 0.0915\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1150 - val_loss: 0.0899\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1145 - val_loss: 0.0873\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1142 - val_loss: 0.0898\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1138 - val_loss: 0.0914\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 3s 93us/sample - loss: 0.1135 - val_loss: 0.0880\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 3s 90us/sample - loss: 0.1132 - val_loss: 0.0886\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1129 - val_loss: 0.0861\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1126 - val_loss: 0.0890\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1123 - val_loss: 0.0862\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1120 - val_loss: 0.0873\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1118 - val_loss: 0.0844\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1116 - val_loss: 0.0858\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 3s 89us/sample - loss: 0.1114 - val_loss: 0.0882\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1110 - val_loss: 0.0814\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 3s 89us/sample - loss: 0.1108 - val_loss: 0.0888\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 3s 89us/sample - loss: 0.1106 - val_loss: 0.0895\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1104 - val_loss: 0.0850\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1101 - val_loss: 0.0844\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1100 - val_loss: 0.0851\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1097 - val_loss: 0.0864\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1095 - val_loss: 0.0828\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1094 - val_loss: 0.0852\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1091 - val_loss: 0.0816\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 3s 87us/sample - loss: 0.1089 - val_loss: 0.0815\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 8\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 9s 290us/sample - loss: 0.1588 - val_loss: 0.1061\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1264 - val_loss: 0.0983\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1222 - val_loss: 0.0962\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1204 - val_loss: 0.0967\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1189 - val_loss: 0.0934\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1180 - val_loss: 0.0960\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1171 - val_loss: 0.1007\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1165 - val_loss: 0.0904\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1158 - val_loss: 0.0951\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1154 - val_loss: 0.0922\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1147 - val_loss: 0.0946\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1143 - val_loss: 0.0944\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1138 - val_loss: 0.0908\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1136 - val_loss: 0.0891\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1132 - val_loss: 0.0886\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1130 - val_loss: 0.0898\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1126 - val_loss: 0.0906\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1123 - val_loss: 0.0911\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1121 - val_loss: 0.0923\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 3s 88us/sample - loss: 0.1119 - val_loss: 0.0869\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 3s 87us/sample - loss: 0.1114 - val_loss: 0.0896\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1112 - val_loss: 0.0876\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1109 - val_loss: 0.0878\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1108 - val_loss: 0.0845\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1106 - val_loss: 0.0872\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.1104 - val_loss: 0.0860\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 3s 87us/sample - loss: 0.1100 - val_loss: 0.0840\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1098 - val_loss: 0.0901\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1096 - val_loss: 0.0913\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1094 - val_loss: 0.0881\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1090 - val_loss: 0.0853\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1089 - val_loss: 0.0894\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1086 - val_loss: 0.0877\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1084 - val_loss: 0.0829\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1084 - val_loss: 0.0840\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1080 - val_loss: 0.0808\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1078 - val_loss: 0.0840\n",
      "Epoch 38/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1076 - val_loss: 0.0872\n",
      "Epoch 39/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1073 - val_loss: 0.0786\n",
      "Epoch 40/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1071 - val_loss: 0.0800\n",
      "Epoch 41/100\n",
      "32720/32720 [==============================] - 3s 88us/sample - loss: 0.1070 - val_loss: 0.0854\n",
      "Epoch 42/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1067 - val_loss: 0.0797\n",
      "Epoch 43/100\n",
      "32720/32720 [==============================] - 3s 87us/sample - loss: 0.1065 - val_loss: 0.0791\n",
      "Epoch 44/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1063 - val_loss: 0.0777\n",
      "Epoch 45/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1061 - val_loss: 0.0837\n",
      "Epoch 46/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1062 - val_loss: 0.0767\n",
      "Epoch 47/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1059 - val_loss: 0.0857\n",
      "Epoch 48/100\n",
      "32720/32720 [==============================] - 3s 87us/sample - loss: 0.1055 - val_loss: 0.0777\n",
      "Epoch 49/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1053 - val_loss: 0.0764\n",
      "Epoch 50/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1052 - val_loss: 0.0806\n",
      "Epoch 51/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1054 - val_loss: 0.0789\n",
      "Epoch 52/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1047 - val_loss: 0.0805\n",
      "Epoch 53/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1048 - val_loss: 0.0784\n",
      "Epoch 54/100\n",
      "32720/32720 [==============================] - 3s 90us/sample - loss: 0.1048 - val_loss: 0.0800\n",
      "Epoch 55/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1045 - val_loss: 0.0783\n",
      "Epoch 56/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1042 - val_loss: 0.0763\n",
      "Epoch 57/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1045 - val_loss: 0.0775\n",
      "Epoch 58/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1038 - val_loss: 0.0739\n",
      "Epoch 59/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1038 - val_loss: 0.0818\n",
      "Epoch 60/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.1035 - val_loss: 0.0726\n",
      "Epoch 61/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1039 - val_loss: 0.0740\n",
      "Epoch 62/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1036 - val_loss: 0.0714\n",
      "Epoch 63/100\n",
      "32720/32720 [==============================] - 3s 77us/sample - loss: 0.1031 - val_loss: 0.0764\n",
      "Epoch 64/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1030 - val_loss: 0.0745\n",
      "Epoch 65/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1027 - val_loss: 0.0729\n",
      "Epoch 66/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1028 - val_loss: 0.0732\n",
      "Epoch 67/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1025 - val_loss: 0.0737\n",
      "Epoch 68/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1023 - val_loss: 0.0731\n",
      "Epoch 69/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1022 - val_loss: 0.0714\n",
      "Epoch 70/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1023 - val_loss: 0.0694\n",
      "Epoch 71/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1020 - val_loss: 0.0728\n",
      "Epoch 72/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1020 - val_loss: 0.0667\n",
      "Epoch 73/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.1018 - val_loss: 0.0721\n",
      "Epoch 74/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1018 - val_loss: 0.0727\n",
      "Epoch 75/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1017 - val_loss: 0.0729\n",
      "Epoch 76/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1016 - val_loss: 0.0714\n",
      "Epoch 77/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1015 - val_loss: 0.0714\n",
      "Epoch 78/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1014 - val_loss: 0.0675\n",
      "Epoch 79/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1013 - val_loss: 0.0697\n",
      "Epoch 80/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1010 - val_loss: 0.0712\n",
      "Epoch 81/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1010 - val_loss: 0.0718\n",
      "Epoch 82/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1009 - val_loss: 0.0706\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 16\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 10s 308us/sample - loss: 0.1540 - val_loss: 0.1062\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1238 - val_loss: 0.0989\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 73us/sample - loss: 0.1206 - val_loss: 0.0954\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1193 - val_loss: 0.0955\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1179 - val_loss: 0.0975\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 3s 88us/sample - loss: 0.1170 - val_loss: 0.0960\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1162 - val_loss: 0.1013\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1157 - val_loss: 0.0873\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1152 - val_loss: 0.0985\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1152 - val_loss: 0.0909\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1144 - val_loss: 0.0958\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 73us/sample - loss: 0.1142 - val_loss: 0.0899\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.1135 - val_loss: 0.0893\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1134 - val_loss: 0.0907\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 76us/sample - loss: 0.1131 - val_loss: 0.0925\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.1126 - val_loss: 0.0922\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1125 - val_loss: 0.0933\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1121 - val_loss: 0.0921\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 32\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 10s 300us/sample - loss: 0.1468 - val_loss: 0.0983\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1231 - val_loss: 0.0975\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1205 - val_loss: 0.0935\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1193 - val_loss: 0.0989\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1181 - val_loss: 0.0985\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1171 - val_loss: 0.0981\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1163 - val_loss: 0.1035\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1163 - val_loss: 0.0864\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1157 - val_loss: 0.1012\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1150 - val_loss: 0.0961\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1146 - val_loss: 0.0969\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1144 - val_loss: 0.0909\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1134 - val_loss: 0.0928\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.1134 - val_loss: 0.0957\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1129 - val_loss: 0.0911\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1123 - val_loss: 0.0912\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.1124 - val_loss: 0.0948\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.1117 - val_loss: 0.0898\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 64\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 9s 287us/sample - loss: 0.1408 - val_loss: 0.0920\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1226 - val_loss: 0.1016\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1205 - val_loss: 0.0952\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1193 - val_loss: 0.1019\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.1180 - val_loss: 0.1014\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1169 - val_loss: 0.0938\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1162 - val_loss: 0.0994\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1156 - val_loss: 0.0817\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.1164 - val_loss: 0.0995\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1150 - val_loss: 0.0973\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1145 - val_loss: 0.0941\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1143 - val_loss: 0.0905\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1131 - val_loss: 0.0913\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1129 - val_loss: 0.0924\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1124 - val_loss: 0.0893\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 3s 77us/sample - loss: 0.1118 - val_loss: 0.0890\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1119 - val_loss: 0.0938\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1111 - val_loss: 0.0863\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 128\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 10s 298us/sample - loss: 0.1393 - val_loss: 0.0953\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.1226 - val_loss: 0.1021\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1210 - val_loss: 0.1019\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1200 - val_loss: 0.1067\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1192 - val_loss: 0.1067\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1180 - val_loss: 0.0993\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1171 - val_loss: 0.0984\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1159 - val_loss: 0.0820\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1158 - val_loss: 0.0971\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1150 - val_loss: 0.0978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1142 - val_loss: 0.0931\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1133 - val_loss: 0.0907\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1121 - val_loss: 0.0860\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 3s 86us/sample - loss: 0.1119 - val_loss: 0.0913\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1107 - val_loss: 0.0872\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.1101 - val_loss: 0.0863\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1092 - val_loss: 0.0834\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1080 - val_loss: 0.0799\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1068 - val_loss: 0.0766\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1056 - val_loss: 0.0740\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1053 - val_loss: 0.0769\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1044 - val_loss: 0.0759\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.1037 - val_loss: 0.0697\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.1033 - val_loss: 0.0698\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1024 - val_loss: 0.0669\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.1023 - val_loss: 0.0768\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 3s 85us/sample - loss: 0.1019 - val_loss: 0.0687\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.1012 - val_loss: 0.0772\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.1008 - val_loss: 0.0658\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 3s 84us/sample - loss: 0.1000 - val_loss: 0.0654\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.1000 - val_loss: 0.0682\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.0998 - val_loss: 0.0662\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.0994 - val_loss: 0.0645\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.0990 - val_loss: 0.0652\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.0986 - val_loss: 0.0664\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.0984 - val_loss: 0.0750\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 3s 77us/sample - loss: 0.0983 - val_loss: 0.0710\n",
      "Epoch 38/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.0978 - val_loss: 0.0669\n",
      "Epoch 39/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.0976 - val_loss: 0.0637\n",
      "Epoch 40/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.0974 - val_loss: 0.0646\n",
      "Epoch 41/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.0971 - val_loss: 0.0644\n",
      "Epoch 42/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.0971 - val_loss: 0.0652\n",
      "Epoch 43/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.0968 - val_loss: 0.0666\n",
      "Epoch 44/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.0966 - val_loss: 0.0645\n",
      "Epoch 45/100\n",
      "32720/32720 [==============================] - 3s 87us/sample - loss: 0.0964 - val_loss: 0.0650\n",
      "Epoch 46/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.0964 - val_loss: 0.0674\n",
      "Epoch 47/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.0963 - val_loss: 0.0676\n",
      "Epoch 48/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.0961 - val_loss: 0.0631\n",
      "Epoch 49/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.0958 - val_loss: 0.0636\n",
      "Epoch 50/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.0958 - val_loss: 0.0645\n",
      "Epoch 51/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.0955 - val_loss: 0.0642\n",
      "Epoch 52/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.0953 - val_loss: 0.0630\n",
      "Epoch 53/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.0953 - val_loss: 0.0635\n",
      "Epoch 54/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.0952 - val_loss: 0.0645\n",
      "Epoch 55/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.0951 - val_loss: 0.0610\n",
      "Epoch 56/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.0947 - val_loss: 0.0607\n",
      "Epoch 57/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.0949 - val_loss: 0.0637\n",
      "Epoch 58/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.0946 - val_loss: 0.0648\n",
      "Epoch 59/100\n",
      "32720/32720 [==============================] - 3s 78us/sample - loss: 0.0947 - val_loss: 0.0654\n",
      "Epoch 60/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.0943 - val_loss: 0.0631\n",
      "Epoch 61/100\n",
      "32720/32720 [==============================] - 2s 75us/sample - loss: 0.0942 - val_loss: 0.0657\n",
      "Epoch 62/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.0944 - val_loss: 0.0673\n",
      "Epoch 63/100\n",
      "32720/32720 [==============================] - 3s 81us/sample - loss: 0.0939 - val_loss: 0.0600\n",
      "Epoch 64/100\n",
      "32720/32720 [==============================] - 3s 79us/sample - loss: 0.0937 - val_loss: 0.0653\n",
      "Epoch 65/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.0937 - val_loss: 0.0626\n",
      "Epoch 66/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.0937 - val_loss: 0.0609\n",
      "Epoch 67/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.0937 - val_loss: 0.0643\n",
      "Epoch 68/100\n",
      "32720/32720 [==============================] - 3s 83us/sample - loss: 0.0938 - val_loss: 0.0700\n",
      "Epoch 69/100\n",
      "32720/32720 [==============================] - 3s 77us/sample - loss: 0.0935 - val_loss: 0.0622\n",
      "Epoch 70/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.0932 - val_loss: 0.0620\n",
      "Epoch 71/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.0934 - val_loss: 0.0634\n",
      "Epoch 72/100\n",
      "32720/32720 [==============================] - 3s 80us/sample - loss: 0.0933 - val_loss: 0.0674\n",
      "Epoch 73/100\n",
      "32720/32720 [==============================] - 3s 82us/sample - loss: 0.0931 - val_loss: 0.0616\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 2\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 10s 294us/sample - loss: 0.1676 - val_loss: 0.1042\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1303 - val_loss: 0.0965\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1236 - val_loss: 0.0950\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 3s 102us/sample - loss: 0.1205 - val_loss: 0.0908\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1187 - val_loss: 0.0907\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1176 - val_loss: 0.0903\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1166 - val_loss: 0.0906\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1159 - val_loss: 0.0988\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1153 - val_loss: 0.0917\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1146 - val_loss: 0.0907\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1141 - val_loss: 0.0929\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1136 - val_loss: 0.0961\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1134 - val_loss: 0.0901\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1129 - val_loss: 0.0904\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1125 - val_loss: 0.0916\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1122 - val_loss: 0.0879\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 3s 89us/sample - loss: 0.1118 - val_loss: 0.0914\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1116 - val_loss: 0.0892\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1110 - val_loss: 0.0906\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1107 - val_loss: 0.0885\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1104 - val_loss: 0.0883\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1099 - val_loss: 0.0857\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1097 - val_loss: 0.0901\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1095 - val_loss: 0.0876\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1091 - val_loss: 0.0870\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1088 - val_loss: 0.0888\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1086 - val_loss: 0.0880\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1084 - val_loss: 0.0876\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1081 - val_loss: 0.0874\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1079 - val_loss: 0.0876\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1077 - val_loss: 0.0849\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1075 - val_loss: 0.0823\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1074 - val_loss: 0.0848\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 3s 89us/sample - loss: 0.1071 - val_loss: 0.0843\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1069 - val_loss: 0.0848\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1066 - val_loss: 0.0852\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1065 - val_loss: 0.0842\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1063 - val_loss: 0.0846\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1062 - val_loss: 0.0828\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1060 - val_loss: 0.0826\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1058 - val_loss: 0.0810\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1057 - val_loss: 0.0813\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1055 - val_loss: 0.0845\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1053 - val_loss: 0.0874\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1052 - val_loss: 0.0828\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1051 - val_loss: 0.0803\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1049 - val_loss: 0.0813\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1047 - val_loss: 0.0804\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1046 - val_loss: 0.0798\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1045 - val_loss: 0.0849\n",
      "Epoch 51/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1044 - val_loss: 0.0841\n",
      "Epoch 52/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1043 - val_loss: 0.0801\n",
      "Epoch 53/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1043 - val_loss: 0.0818\n",
      "Epoch 54/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1039 - val_loss: 0.0800\n",
      "Epoch 55/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1040 - val_loss: 0.0798\n",
      "Epoch 56/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1038 - val_loss: 0.0816\n",
      "Epoch 57/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1037 - val_loss: 0.0809\n",
      "Epoch 58/100\n",
      "32580/32580 [==============================] - 2s 77us/sample - loss: 0.1036 - val_loss: 0.0787\n",
      "Epoch 59/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1035 - val_loss: 0.0808\n",
      "Epoch 60/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1033 - val_loss: 0.0792\n",
      "Epoch 61/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1032 - val_loss: 0.0797\n",
      "Epoch 62/100\n",
      "32580/32580 [==============================] - 3s 89us/sample - loss: 0.1032 - val_loss: 0.0852\n",
      "Epoch 63/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1031 - val_loss: 0.0805\n",
      "Epoch 64/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1029 - val_loss: 0.0788\n",
      "Epoch 65/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1029 - val_loss: 0.0804\n",
      "Epoch 66/100\n",
      "32580/32580 [==============================] - 3s 89us/sample - loss: 0.1028 - val_loss: 0.0759\n",
      "Epoch 67/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1028 - val_loss: 0.0777\n",
      "Epoch 68/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1026 - val_loss: 0.0783\n",
      "Epoch 69/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1025 - val_loss: 0.0769\n",
      "Epoch 70/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1025 - val_loss: 0.0794\n",
      "Epoch 71/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1024 - val_loss: 0.0787\n",
      "Epoch 72/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1023 - val_loss: 0.0807\n",
      "Epoch 73/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1023 - val_loss: 0.0776\n",
      "Epoch 74/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1022 - val_loss: 0.0793\n",
      "Epoch 75/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1021 - val_loss: 0.0759\n",
      "Epoch 76/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1021 - val_loss: 0.0789\n",
      "Epoch 77/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1020 - val_loss: 0.0783\n",
      "Epoch 78/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1020 - val_loss: 0.0775\n",
      "Epoch 79/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1018 - val_loss: 0.0771\n",
      "Epoch 80/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1018 - val_loss: 0.0781\n",
      "Epoch 81/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1017 - val_loss: 0.0776\n",
      "Epoch 82/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1017 - val_loss: 0.0789\n",
      "Epoch 83/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.1016 - val_loss: 0.0764\n",
      "Epoch 84/100\n",
      "32580/32580 [==============================] - 2s 73us/sample - loss: 0.1016 - val_loss: 0.0774\n",
      "Epoch 85/100\n",
      "32580/32580 [==============================] - 2s 73us/sample - loss: 0.1014 - val_loss: 0.0765\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 4\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32580/32580 [==============================] - 10s 302us/sample - loss: 0.1616 - val_loss: 0.1021\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1240 - val_loss: 0.1030\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1192 - val_loss: 0.1021\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1170 - val_loss: 0.0936\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 74us/sample - loss: 0.1154 - val_loss: 0.0948\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1144 - val_loss: 0.0936\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1134 - val_loss: 0.0926\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1127 - val_loss: 0.1000\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1124 - val_loss: 0.0930\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1114 - val_loss: 0.0925\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1109 - val_loss: 0.0900\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1105 - val_loss: 0.0905\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1105 - val_loss: 0.0852\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1097 - val_loss: 0.0875\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1094 - val_loss: 0.0903\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1090 - val_loss: 0.0880\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1086 - val_loss: 0.0892\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 3s 89us/sample - loss: 0.1083 - val_loss: 0.0863\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1080 - val_loss: 0.0881\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1077 - val_loss: 0.0848\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1073 - val_loss: 0.0834\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1069 - val_loss: 0.0844\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1068 - val_loss: 0.0855\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1064 - val_loss: 0.0829\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 3s 89us/sample - loss: 0.1061 - val_loss: 0.0825\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1057 - val_loss: 0.0816\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1056 - val_loss: 0.0884\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1055 - val_loss: 0.0833\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1051 - val_loss: 0.0810\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1049 - val_loss: 0.0813\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1048 - val_loss: 0.0787\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1045 - val_loss: 0.0804\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1044 - val_loss: 0.0812\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1042 - val_loss: 0.0812\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1039 - val_loss: 0.0814\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1037 - val_loss: 0.0831\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1036 - val_loss: 0.0791\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1034 - val_loss: 0.0762\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1033 - val_loss: 0.0771\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1030 - val_loss: 0.0758\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1029 - val_loss: 0.0780\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 3s 94us/sample - loss: 0.1028 - val_loss: 0.0754\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 3s 89us/sample - loss: 0.1026 - val_loss: 0.0766\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1024 - val_loss: 0.0825\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 3s 89us/sample - loss: 0.1021 - val_loss: 0.0763\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1021 - val_loss: 0.0805\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1021 - val_loss: 0.0763\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1018 - val_loss: 0.0778\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1016 - val_loss: 0.0737\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1015 - val_loss: 0.0788\n",
      "Epoch 51/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1014 - val_loss: 0.0778\n",
      "Epoch 52/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.1012 - val_loss: 0.0727\n",
      "Epoch 53/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1013 - val_loss: 0.0763\n",
      "Epoch 54/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1010 - val_loss: 0.0751\n",
      "Epoch 55/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1009 - val_loss: 0.0783\n",
      "Epoch 56/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1008 - val_loss: 0.0748\n",
      "Epoch 57/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1006 - val_loss: 0.0756\n",
      "Epoch 58/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1004 - val_loss: 0.0724\n",
      "Epoch 59/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1003 - val_loss: 0.0780\n",
      "Epoch 60/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1002 - val_loss: 0.0758\n",
      "Epoch 61/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1002 - val_loss: 0.0744\n",
      "Epoch 62/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1001 - val_loss: 0.0789\n",
      "Epoch 63/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1000 - val_loss: 0.0732\n",
      "Epoch 64/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.0999 - val_loss: 0.0742\n",
      "Epoch 65/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0999 - val_loss: 0.0780\n",
      "Epoch 66/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0997 - val_loss: 0.0718\n",
      "Epoch 67/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.0997 - val_loss: 0.0698\n",
      "Epoch 68/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.0995 - val_loss: 0.0732\n",
      "Epoch 69/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.0996 - val_loss: 0.0728\n",
      "Epoch 70/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.0994 - val_loss: 0.0766\n",
      "Epoch 71/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0993 - val_loss: 0.0750\n",
      "Epoch 72/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.0994 - val_loss: 0.0740\n",
      "Epoch 73/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0991 - val_loss: 0.0731\n",
      "Epoch 74/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.0991 - val_loss: 0.0717\n",
      "Epoch 75/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.0991 - val_loss: 0.0749\n",
      "Epoch 76/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0990 - val_loss: 0.0712\n",
      "Epoch 77/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0990 - val_loss: 0.0740\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 8\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 10s 305us/sample - loss: 0.1524 - val_loss: 0.0979\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1224 - val_loss: 0.0933\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1185 - val_loss: 0.1012\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1161 - val_loss: 0.0893\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1147 - val_loss: 0.0923\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1136 - val_loss: 0.0903\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1124 - val_loss: 0.0936\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1116 - val_loss: 0.0952\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1111 - val_loss: 0.0947\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1101 - val_loss: 0.0887\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 3s 88us/sample - loss: 0.1094 - val_loss: 0.0910\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1090 - val_loss: 0.0902\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1085 - val_loss: 0.0860\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1082 - val_loss: 0.0908\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1076 - val_loss: 0.0872\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1071 - val_loss: 0.0870\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1069 - val_loss: 0.0887\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1064 - val_loss: 0.0794\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1063 - val_loss: 0.0858\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1056 - val_loss: 0.0828\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.1055 - val_loss: 0.0819\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1050 - val_loss: 0.0841\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1048 - val_loss: 0.0831\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 3s 89us/sample - loss: 0.1044 - val_loss: 0.0837\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1040 - val_loss: 0.0833\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1036 - val_loss: 0.0816\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1033 - val_loss: 0.0839\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.1034 - val_loss: 0.0849\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 16\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 10s 301us/sample - loss: 0.1435 - val_loss: 0.1047\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1188 - val_loss: 0.0937\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1158 - val_loss: 0.1052\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1141 - val_loss: 0.0931\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1127 - val_loss: 0.0905\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1122 - val_loss: 0.0923\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1112 - val_loss: 0.0946\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 77us/sample - loss: 0.1105 - val_loss: 0.0957\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1097 - val_loss: 0.0933\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.1092 - val_loss: 0.0854\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.1085 - val_loss: 0.0919\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 75us/sample - loss: 0.1083 - val_loss: 0.0835\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.1075 - val_loss: 0.0862\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.1072 - val_loss: 0.0921\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1064 - val_loss: 0.0804\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1060 - val_loss: 0.0855\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1053 - val_loss: 0.0852\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.1051 - val_loss: 0.0762\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1045 - val_loss: 0.0830\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1041 - val_loss: 0.0787\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1037 - val_loss: 0.0776\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1035 - val_loss: 0.0781\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1030 - val_loss: 0.0820\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1026 - val_loss: 0.0753\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1020 - val_loss: 0.0821\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.1015 - val_loss: 0.0787\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 2s 75us/sample - loss: 0.1016 - val_loss: 0.0761\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1014 - val_loss: 0.0834\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1006 - val_loss: 0.0715\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1005 - val_loss: 0.0718\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1002 - val_loss: 0.0762\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1001 - val_loss: 0.0707\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0997 - val_loss: 0.0717\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1000 - val_loss: 0.0742\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.0992 - val_loss: 0.0739\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0988 - val_loss: 0.0783\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0989 - val_loss: 0.0721\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.0988 - val_loss: 0.0751\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0987 - val_loss: 0.0720\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.0982 - val_loss: 0.0662\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0983 - val_loss: 0.0741\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.0980 - val_loss: 0.0816\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0979 - val_loss: 0.0775\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0976 - val_loss: 0.0693\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 3s 87us/sample - loss: 0.0974 - val_loss: 0.0667\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0971 - val_loss: 0.0729\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0969 - val_loss: 0.0729\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0968 - val_loss: 0.0727\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0967 - val_loss: 0.0725\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0968 - val_loss: 0.0692\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 32\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 10s 301us/sample - loss: 0.1368 - val_loss: 0.1012\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1180 - val_loss: 0.0969\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1158 - val_loss: 0.1042\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1149 - val_loss: 0.0933\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1135 - val_loss: 0.0941\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1126 - val_loss: 0.0899\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1119 - val_loss: 0.0936\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1117 - val_loss: 0.0951\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1107 - val_loss: 0.0909\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1104 - val_loss: 0.0866\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1098 - val_loss: 0.0916\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1091 - val_loss: 0.0845\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1094 - val_loss: 0.0876\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1082 - val_loss: 0.0875\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1074 - val_loss: 0.0826\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1069 - val_loss: 0.0874\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1061 - val_loss: 0.0878\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1053 - val_loss: 0.0814\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1049 - val_loss: 0.0783\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1042 - val_loss: 0.0853\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.1031 - val_loss: 0.0778\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1027 - val_loss: 0.0804\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1021 - val_loss: 0.0863\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1014 - val_loss: 0.0803\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1010 - val_loss: 0.0778\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0999 - val_loss: 0.0762\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0998 - val_loss: 0.0831\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1002 - val_loss: 0.0794\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0991 - val_loss: 0.0728\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0985 - val_loss: 0.0695\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0983 - val_loss: 0.0684\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0981 - val_loss: 0.0738\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0978 - val_loss: 0.0677\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0977 - val_loss: 0.0733\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0976 - val_loss: 0.0716\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0972 - val_loss: 0.0737\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0964 - val_loss: 0.0717\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0967 - val_loss: 0.0727\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0965 - val_loss: 0.0724\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0965 - val_loss: 0.0633\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0962 - val_loss: 0.0719\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0956 - val_loss: 0.0724\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0954 - val_loss: 0.0717\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0953 - val_loss: 0.0669\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0952 - val_loss: 0.0671\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0947 - val_loss: 0.0693\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0946 - val_loss: 0.0731\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0945 - val_loss: 0.0710\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 2s 77us/sample - loss: 0.0943 - val_loss: 0.0697\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0942 - val_loss: 0.0652\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 64\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 9s 276us/sample - loss: 0.1362 - val_loss: 0.0894\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1178 - val_loss: 0.0966\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1152 - val_loss: 0.1004\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1151 - val_loss: 0.0966\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1136 - val_loss: 0.0928\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1130 - val_loss: 0.0891\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1122 - val_loss: 0.0922\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1117 - val_loss: 0.0977\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1105 - val_loss: 0.0886\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1101 - val_loss: 0.0852\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 73us/sample - loss: 0.1093 - val_loss: 0.0933\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.1088 - val_loss: 0.0875\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1090 - val_loss: 0.0913\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1075 - val_loss: 0.0817\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1071 - val_loss: 0.0823\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.1062 - val_loss: 0.0848\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1053 - val_loss: 0.0866\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1043 - val_loss: 0.0767\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.1041 - val_loss: 0.0781\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1034 - val_loss: 0.0812\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.1024 - val_loss: 0.0754\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1018 - val_loss: 0.0775\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1010 - val_loss: 0.0793\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1004 - val_loss: 0.0762\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.0999 - val_loss: 0.0755\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0985 - val_loss: 0.0751\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0978 - val_loss: 0.0792\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 2s 74us/sample - loss: 0.0975 - val_loss: 0.0716\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0967 - val_loss: 0.0717\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0961 - val_loss: 0.0671\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0956 - val_loss: 0.0709\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0954 - val_loss: 0.0684\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0947 - val_loss: 0.0660\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0943 - val_loss: 0.0706\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0944 - val_loss: 0.0635\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.0939 - val_loss: 0.0684\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0935 - val_loss: 0.0647\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0933 - val_loss: 0.0649\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0934 - val_loss: 0.0642\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0932 - val_loss: 0.0610\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.0928 - val_loss: 0.0619\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0925 - val_loss: 0.0644\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0922 - val_loss: 0.0685\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0924 - val_loss: 0.0605\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0921 - val_loss: 0.0638\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 2s 77us/sample - loss: 0.0919 - val_loss: 0.0633\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.0917 - val_loss: 0.0659\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0918 - val_loss: 0.0677\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 3s 86us/sample - loss: 0.0914 - val_loss: 0.0664\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.0913 - val_loss: 0.0644\n",
      "Epoch 51/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0912 - val_loss: 0.0613\n",
      "Epoch 52/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0913 - val_loss: 0.0675\n",
      "Epoch 53/100\n",
      "32580/32580 [==============================] - 2s 75us/sample - loss: 0.0913 - val_loss: 0.0611\n",
      "Epoch 54/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0911 - val_loss: 0.0601\n",
      "Epoch 55/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0912 - val_loss: 0.0632\n",
      "Epoch 56/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0907 - val_loss: 0.0597\n",
      "Epoch 57/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0904 - val_loss: 0.0617\n",
      "Epoch 58/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0904 - val_loss: 0.0607\n",
      "Epoch 59/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0905 - val_loss: 0.0662\n",
      "Epoch 60/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.0903 - val_loss: 0.0651\n",
      "Epoch 61/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0904 - val_loss: 0.0612\n",
      "Epoch 62/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0903 - val_loss: 0.0627\n",
      "Epoch 63/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0901 - val_loss: 0.0673\n",
      "Epoch 64/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0897 - val_loss: 0.0611\n",
      "Epoch 65/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0897 - val_loss: 0.0580\n",
      "Epoch 66/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0896 - val_loss: 0.0597\n",
      "Epoch 67/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.0898 - val_loss: 0.0598\n",
      "Epoch 68/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0893 - val_loss: 0.0619\n",
      "Epoch 69/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0894 - val_loss: 0.0623\n",
      "Epoch 70/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0893 - val_loss: 0.0667\n",
      "Epoch 71/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.0893 - val_loss: 0.0588\n",
      "Epoch 72/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.0894 - val_loss: 0.0598\n",
      "Epoch 73/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0891 - val_loss: 0.0577\n",
      "Epoch 74/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0891 - val_loss: 0.0583\n",
      "Epoch 75/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0890 - val_loss: 0.0577\n",
      "Epoch 76/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0889 - val_loss: 0.0610\n",
      "Epoch 77/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0888 - val_loss: 0.0590\n",
      "Epoch 78/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0885 - val_loss: 0.0596\n",
      "Epoch 79/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0888 - val_loss: 0.0580\n",
      "Epoch 80/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0888 - val_loss: 0.0598\n",
      "Epoch 81/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0885 - val_loss: 0.0598\n",
      "Epoch 82/100\n",
      "32580/32580 [==============================] - 2s 77us/sample - loss: 0.0883 - val_loss: 0.0590\n",
      "Epoch 83/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.0882 - val_loss: 0.0591\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 128\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 9s 289us/sample - loss: 0.1347 - val_loss: 0.0951\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1182 - val_loss: 0.0987\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.1159 - val_loss: 0.0993\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1154 - val_loss: 0.0929\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1144 - val_loss: 0.0894\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.1134 - val_loss: 0.0948\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1119 - val_loss: 0.0942\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.1112 - val_loss: 0.0996\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.1096 - val_loss: 0.0853\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1088 - val_loss: 0.0787\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1077 - val_loss: 0.0943\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.1071 - val_loss: 0.0828\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 3s 77us/sample - loss: 0.1067 - val_loss: 0.0870\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.1048 - val_loss: 0.0772\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1037 - val_loss: 0.0768\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1026 - val_loss: 0.0741\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.1007 - val_loss: 0.0795\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.0999 - val_loss: 0.0734\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0988 - val_loss: 0.0681\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0982 - val_loss: 0.0696\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0967 - val_loss: 0.0662\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0963 - val_loss: 0.0690\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0960 - val_loss: 0.0642\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 3s 83us/sample - loss: 0.0952 - val_loss: 0.0682\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0949 - val_loss: 0.0621\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0943 - val_loss: 0.0664\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.0941 - val_loss: 0.0719\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0939 - val_loss: 0.0649\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0938 - val_loss: 0.0629\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0935 - val_loss: 0.0584\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.0934 - val_loss: 0.0598\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.0933 - val_loss: 0.0610\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 3s 85us/sample - loss: 0.0929 - val_loss: 0.0632\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0924 - val_loss: 0.0607\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 3s 81us/sample - loss: 0.0926 - val_loss: 0.0580\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 3s 82us/sample - loss: 0.0922 - val_loss: 0.0606\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0918 - val_loss: 0.0610\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0917 - val_loss: 0.0570\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0919 - val_loss: 0.0622\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0915 - val_loss: 0.0575\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0915 - val_loss: 0.0600\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0913 - val_loss: 0.0605\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 3s 80us/sample - loss: 0.0910 - val_loss: 0.0649\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0912 - val_loss: 0.0583\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 2s 76us/sample - loss: 0.0907 - val_loss: 0.0592\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 3s 84us/sample - loss: 0.0907 - val_loss: 0.0601\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 3s 78us/sample - loss: 0.0905 - val_loss: 0.0608\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 3s 79us/sample - loss: 0.0905 - val_loss: 0.0619\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 2\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 9s 278us/sample - loss: 0.1822 - val_loss: 0.1302\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1404 - val_loss: 0.1043\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1272 - val_loss: 0.1035\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1235 - val_loss: 0.1028\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1219 - val_loss: 0.1018\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1206 - val_loss: 0.1031\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1200 - val_loss: 0.1043\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1191 - val_loss: 0.1007\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1186 - val_loss: 0.0998\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1181 - val_loss: 0.1019\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1176 - val_loss: 0.1011\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1173 - val_loss: 0.1013\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 3s 76us/sample - loss: 0.1170 - val_loss: 0.1002\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1167 - val_loss: 0.1014\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 3s 76us/sample - loss: 0.1164 - val_loss: 0.1006\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1162 - val_loss: 0.0962\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1159 - val_loss: 0.1025\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1157 - val_loss: 0.0958\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1154 - val_loss: 0.1003\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1153 - val_loss: 0.1010\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1151 - val_loss: 0.1002\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1149 - val_loss: 0.0995\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1147 - val_loss: 0.0997\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1146 - val_loss: 0.0989\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1143 - val_loss: 0.1007\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1141 - val_loss: 0.0968\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1140 - val_loss: 0.0985\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1138 - val_loss: 0.0940\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1136 - val_loss: 0.0969\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1134 - val_loss: 0.0961\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1132 - val_loss: 0.1019\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1132 - val_loss: 0.0982\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1129 - val_loss: 0.0970\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1128 - val_loss: 0.0966\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 3s 87us/sample - loss: 0.1126 - val_loss: 0.0941\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1125 - val_loss: 0.0951\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1123 - val_loss: 0.1000\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1121 - val_loss: 0.0937\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1119 - val_loss: 0.0937\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1118 - val_loss: 0.0918\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1116 - val_loss: 0.0941\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1114 - val_loss: 0.0960\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1113 - val_loss: 0.0955\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1111 - val_loss: 0.0919\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1111 - val_loss: 0.0963\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1108 - val_loss: 0.0944\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1107 - val_loss: 0.0938\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1106 - val_loss: 0.0947\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1104 - val_loss: 0.0946\n",
      "Epoch 50/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1102 - val_loss: 0.0938\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 4\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 9s 278us/sample - loss: 0.1748 - val_loss: 0.1236\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1390 - val_loss: 0.1063\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1264 - val_loss: 0.1046\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1224 - val_loss: 0.1062\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1204 - val_loss: 0.1050\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1189 - val_loss: 0.1060\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1180 - val_loss: 0.1081\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1170 - val_loss: 0.1003\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1163 - val_loss: 0.0994\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1156 - val_loss: 0.1014\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1150 - val_loss: 0.0976\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1147 - val_loss: 0.1006\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1143 - val_loss: 0.0967\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 3s 87us/sample - loss: 0.1140 - val_loss: 0.0995\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1135 - val_loss: 0.1003\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1132 - val_loss: 0.0922\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1130 - val_loss: 0.1001\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1127 - val_loss: 0.0958\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1123 - val_loss: 0.0970\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1121 - val_loss: 0.0986\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1117 - val_loss: 0.0957\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1115 - val_loss: 0.0964\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1113 - val_loss: 0.0948\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1112 - val_loss: 0.0947\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 3s 88us/sample - loss: 0.1107 - val_loss: 0.0952\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1106 - val_loss: 0.0885\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1105 - val_loss: 0.1002\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1102 - val_loss: 0.0882\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1100 - val_loss: 0.0938\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1099 - val_loss: 0.0891\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1095 - val_loss: 0.0969\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1093 - val_loss: 0.0892\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1091 - val_loss: 0.0946\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1089 - val_loss: 0.0881\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1087 - val_loss: 0.0857\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1087 - val_loss: 0.0876\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1083 - val_loss: 0.0930\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1082 - val_loss: 0.0888\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1080 - val_loss: 0.0865\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1077 - val_loss: 0.0850\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1075 - val_loss: 0.0887\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1073 - val_loss: 0.0908\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1072 - val_loss: 0.0867\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1069 - val_loss: 0.0870\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1067 - val_loss: 0.0886\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1065 - val_loss: 0.0892\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1064 - val_loss: 0.0899\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1062 - val_loss: 0.0847\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1061 - val_loss: 0.0867\n",
      "Epoch 50/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1059 - val_loss: 0.0877\n",
      "Epoch 51/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1058 - val_loss: 0.0843\n",
      "Epoch 52/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1055 - val_loss: 0.0833\n",
      "Epoch 53/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1054 - val_loss: 0.0867\n",
      "Epoch 54/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1052 - val_loss: 0.0820\n",
      "Epoch 55/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1052 - val_loss: 0.0815\n",
      "Epoch 56/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1051 - val_loss: 0.0836\n",
      "Epoch 57/100\n",
      "33340/33340 [==============================] - 3s 75us/sample - loss: 0.1050 - val_loss: 0.0827\n",
      "Epoch 58/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1049 - val_loss: 0.0844\n",
      "Epoch 59/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1048 - val_loss: 0.0817\n",
      "Epoch 60/100\n",
      "33340/33340 [==============================] - 3s 87us/sample - loss: 0.1048 - val_loss: 0.0851\n",
      "Epoch 61/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1044 - val_loss: 0.0852\n",
      "Epoch 62/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1045 - val_loss: 0.0854\n",
      "Epoch 63/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1043 - val_loss: 0.0835\n",
      "Epoch 64/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1042 - val_loss: 0.0845\n",
      "Epoch 65/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1041 - val_loss: 0.0835\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 8\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 9s 275us/sample - loss: 0.1640 - val_loss: 0.1081\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1290 - val_loss: 0.1072\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 75us/sample - loss: 0.1227 - val_loss: 0.1073\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1207 - val_loss: 0.1067\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1194 - val_loss: 0.1015\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1182 - val_loss: 0.1043\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1174 - val_loss: 0.1007\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1164 - val_loss: 0.0962\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1158 - val_loss: 0.0978\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1151 - val_loss: 0.0992\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1147 - val_loss: 0.0962\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1143 - val_loss: 0.1015\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1138 - val_loss: 0.0980\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1135 - val_loss: 0.0984\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 3s 87us/sample - loss: 0.1130 - val_loss: 0.0989\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1128 - val_loss: 0.0916\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1125 - val_loss: 0.0953\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1121 - val_loss: 0.0978\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1117 - val_loss: 0.0976\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1116 - val_loss: 0.0965\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1111 - val_loss: 0.0918\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1109 - val_loss: 0.0935\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1107 - val_loss: 0.0915\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1104 - val_loss: 0.0948\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1099 - val_loss: 0.0892\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1099 - val_loss: 0.0872\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1095 - val_loss: 0.0952\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1091 - val_loss: 0.0891\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1089 - val_loss: 0.0943\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1085 - val_loss: 0.0878\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1082 - val_loss: 0.0949\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1079 - val_loss: 0.0856\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1076 - val_loss: 0.0921\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1075 - val_loss: 0.0886\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1069 - val_loss: 0.0879\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1069 - val_loss: 0.0851\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1065 - val_loss: 0.0974\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1063 - val_loss: 0.0901\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1060 - val_loss: 0.0835\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1056 - val_loss: 0.0882\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1052 - val_loss: 0.0892\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1051 - val_loss: 0.0829\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1047 - val_loss: 0.0849\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1044 - val_loss: 0.0858\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1044 - val_loss: 0.0866\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1037 - val_loss: 0.0856\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1035 - val_loss: 0.0847\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1033 - val_loss: 0.0792\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1031 - val_loss: 0.0815\n",
      "Epoch 50/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1028 - val_loss: 0.0799\n",
      "Epoch 51/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1028 - val_loss: 0.0810\n",
      "Epoch 52/100\n",
      "33340/33340 [==============================] - 3s 76us/sample - loss: 0.1023 - val_loss: 0.0810\n",
      "Epoch 53/100\n",
      "33340/33340 [==============================] - 3s 75us/sample - loss: 0.1020 - val_loss: 0.0820\n",
      "Epoch 54/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1020 - val_loss: 0.0799\n",
      "Epoch 55/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1015 - val_loss: 0.0811\n",
      "Epoch 56/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1017 - val_loss: 0.0783\n",
      "Epoch 57/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1016 - val_loss: 0.0798\n",
      "Epoch 58/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1014 - val_loss: 0.0741\n",
      "Epoch 59/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1009 - val_loss: 0.0791\n",
      "Epoch 60/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1010 - val_loss: 0.0833\n",
      "Epoch 61/100\n",
      "33340/33340 [==============================] - 3s 76us/sample - loss: 0.1008 - val_loss: 0.0811\n",
      "Epoch 62/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1006 - val_loss: 0.0802\n",
      "Epoch 63/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1003 - val_loss: 0.0760\n",
      "Epoch 64/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1003 - val_loss: 0.0771\n",
      "Epoch 65/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1002 - val_loss: 0.0783\n",
      "Epoch 66/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1003 - val_loss: 0.0752\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0999 - val_loss: 0.0823\n",
      "Epoch 68/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0998 - val_loss: 0.0780\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 16\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 9s 275us/sample - loss: 0.1497 - val_loss: 0.1048\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1227 - val_loss: 0.1087\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1197 - val_loss: 0.1153\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1180 - val_loss: 0.1039\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1170 - val_loss: 0.1061\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1166 - val_loss: 0.1085\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1160 - val_loss: 0.1001\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1151 - val_loss: 0.0947\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1149 - val_loss: 0.0982\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1143 - val_loss: 0.1001\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1138 - val_loss: 0.1024\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1134 - val_loss: 0.0991\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1130 - val_loss: 0.1065\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1128 - val_loss: 0.0983\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1122 - val_loss: 0.1002\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1117 - val_loss: 0.0951\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1114 - val_loss: 0.0991\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1110 - val_loss: 0.0920\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1107 - val_loss: 0.0922\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1105 - val_loss: 0.1014\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1099 - val_loss: 0.0906\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1094 - val_loss: 0.0907\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1093 - val_loss: 0.0958\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1088 - val_loss: 0.0944\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1085 - val_loss: 0.0899\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1081 - val_loss: 0.0894\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1077 - val_loss: 0.0899\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1073 - val_loss: 0.0875\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1069 - val_loss: 0.0937\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1065 - val_loss: 0.0838\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1060 - val_loss: 0.0909\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1057 - val_loss: 0.0771\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1051 - val_loss: 0.0898\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1049 - val_loss: 0.0851\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1043 - val_loss: 0.0810\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1042 - val_loss: 0.0849\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1037 - val_loss: 0.0914\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1031 - val_loss: 0.0841\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1029 - val_loss: 0.0852\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1026 - val_loss: 0.0804\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1021 - val_loss: 0.0772\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1019 - val_loss: 0.0817\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 32\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 9s 275us/sample - loss: 0.1455 - val_loss: 0.1061\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1232 - val_loss: 0.1097\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1200 - val_loss: 0.1152\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 3s 89us/sample - loss: 0.1182 - val_loss: 0.1033\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1172 - val_loss: 0.1048\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1164 - val_loss: 0.1116\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1161 - val_loss: 0.1017\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1148 - val_loss: 0.0916\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1147 - val_loss: 0.0983\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1141 - val_loss: 0.0999\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1134 - val_loss: 0.1032\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1131 - val_loss: 0.0965\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1125 - val_loss: 0.1031\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1124 - val_loss: 0.0915\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1116 - val_loss: 0.0960\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1109 - val_loss: 0.0884\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1105 - val_loss: 0.0929\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1100 - val_loss: 0.0875\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1093 - val_loss: 0.0873\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1090 - val_loss: 0.0995\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1081 - val_loss: 0.0896\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1076 - val_loss: 0.0859\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1073 - val_loss: 0.0898\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1067 - val_loss: 0.0919\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1059 - val_loss: 0.0843\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1056 - val_loss: 0.0820\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1053 - val_loss: 0.0858\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1049 - val_loss: 0.0814\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1044 - val_loss: 0.0843\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1041 - val_loss: 0.0753\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1034 - val_loss: 0.0856\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1030 - val_loss: 0.0740\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1027 - val_loss: 0.0786\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1025 - val_loss: 0.0768\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1020 - val_loss: 0.0762\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1018 - val_loss: 0.0780\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1017 - val_loss: 0.0854\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1011 - val_loss: 0.0789\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1009 - val_loss: 0.0782\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1008 - val_loss: 0.0738\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1003 - val_loss: 0.0751\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1002 - val_loss: 0.0761\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.0998 - val_loss: 0.0708\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.0999 - val_loss: 0.0774\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 3s 87us/sample - loss: 0.0996 - val_loss: 0.0748\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 3s 88us/sample - loss: 0.0992 - val_loss: 0.0734\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.0987 - val_loss: 0.0762\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.0990 - val_loss: 0.0740\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.0986 - val_loss: 0.0702\n",
      "Epoch 50/100\n",
      "33340/33340 [==============================] - 3s 88us/sample - loss: 0.0985 - val_loss: 0.0683\n",
      "Epoch 51/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.0983 - val_loss: 0.0738\n",
      "Epoch 52/100\n",
      "33340/33340 [==============================] - 3s 87us/sample - loss: 0.0981 - val_loss: 0.0713\n",
      "Epoch 53/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0977 - val_loss: 0.0715\n",
      "Epoch 54/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.0977 - val_loss: 0.0680\n",
      "Epoch 55/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.0975 - val_loss: 0.0714\n",
      "Epoch 56/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.0978 - val_loss: 0.0728\n",
      "Epoch 57/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.0977 - val_loss: 0.0751\n",
      "Epoch 58/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.0971 - val_loss: 0.0714\n",
      "Epoch 59/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.0967 - val_loss: 0.0710\n",
      "Epoch 60/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.0966 - val_loss: 0.0724\n",
      "Epoch 61/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.0965 - val_loss: 0.0690\n",
      "Epoch 62/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.0965 - val_loss: 0.0690\n",
      "Epoch 63/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.0962 - val_loss: 0.0675\n",
      "Epoch 64/100\n",
      "33340/33340 [==============================] - 2s 75us/sample - loss: 0.0961 - val_loss: 0.0682\n",
      "Epoch 65/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.0959 - val_loss: 0.0742\n",
      "Epoch 66/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.0957 - val_loss: 0.0665\n",
      "Epoch 67/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.0957 - val_loss: 0.0713\n",
      "Epoch 68/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.0957 - val_loss: 0.0744\n",
      "Epoch 69/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.0954 - val_loss: 0.0677\n",
      "Epoch 70/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.0952 - val_loss: 0.0674\n",
      "Epoch 71/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0954 - val_loss: 0.0743\n",
      "Epoch 72/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.0951 - val_loss: 0.0700\n",
      "Epoch 73/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.0950 - val_loss: 0.0679\n",
      "Epoch 74/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.0950 - val_loss: 0.0687\n",
      "Epoch 75/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.0947 - val_loss: 0.0718\n",
      "Epoch 76/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0948 - val_loss: 0.0643\n",
      "Epoch 77/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.0948 - val_loss: 0.0741\n",
      "Epoch 78/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.0944 - val_loss: 0.0663\n",
      "Epoch 79/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0946 - val_loss: 0.0722\n",
      "Epoch 80/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.0943 - val_loss: 0.0674\n",
      "Epoch 81/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0941 - val_loss: 0.0650\n",
      "Epoch 82/100\n",
      "33340/33340 [==============================] - 2s 75us/sample - loss: 0.0942 - val_loss: 0.0668\n",
      "Epoch 83/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.0942 - val_loss: 0.0658\n",
      "Epoch 84/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.0939 - val_loss: 0.0657\n",
      "Epoch 85/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.0941 - val_loss: 0.0679\n",
      "Epoch 86/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0939 - val_loss: 0.0669\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 64\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 9s 269us/sample - loss: 0.1404 - val_loss: 0.1010\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1220 - val_loss: 0.1058\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1200 - val_loss: 0.1079\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1190 - val_loss: 0.1020\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1179 - val_loss: 0.1083\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1171 - val_loss: 0.1167\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1168 - val_loss: 0.1037\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1156 - val_loss: 0.0927\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1149 - val_loss: 0.0994\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1141 - val_loss: 0.0947\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 3s 88us/sample - loss: 0.1135 - val_loss: 0.1029\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1134 - val_loss: 0.0954\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1123 - val_loss: 0.1049\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1123 - val_loss: 0.0926\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1111 - val_loss: 0.0954\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1103 - val_loss: 0.0884\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1097 - val_loss: 0.0940\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1088 - val_loss: 0.0947\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1084 - val_loss: 0.0880\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1075 - val_loss: 0.0944\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1068 - val_loss: 0.0852\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1056 - val_loss: 0.0835\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1054 - val_loss: 0.0910\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1045 - val_loss: 0.0854\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1039 - val_loss: 0.0814\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1035 - val_loss: 0.0831\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1031 - val_loss: 0.0871\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1028 - val_loss: 0.0781\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1021 - val_loss: 0.0802\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1021 - val_loss: 0.0806\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1012 - val_loss: 0.0823\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1011 - val_loss: 0.0701\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1008 - val_loss: 0.0798\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1001 - val_loss: 0.0832\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.0999 - val_loss: 0.0769\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.0997 - val_loss: 0.0713\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0992 - val_loss: 0.0823\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.0990 - val_loss: 0.0828\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.0987 - val_loss: 0.0709\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0983 - val_loss: 0.0725\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.0981 - val_loss: 0.0781\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0978 - val_loss: 0.0759\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 128\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 10s 301us/sample - loss: 0.1384 - val_loss: 0.0974\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1223 - val_loss: 0.1064\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 3s 89us/sample - loss: 0.1208 - val_loss: 0.1073\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.1199 - val_loss: 0.1045\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1192 - val_loss: 0.1110\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1180 - val_loss: 0.1189\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1175 - val_loss: 0.1057\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1161 - val_loss: 0.0897\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1156 - val_loss: 0.0992\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1143 - val_loss: 0.0920\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.1135 - val_loss: 0.1039\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 3s 87us/sample - loss: 0.1136 - val_loss: 0.0929\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1121 - val_loss: 0.0999\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1115 - val_loss: 0.0911\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1100 - val_loss: 0.0940\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1086 - val_loss: 0.0822\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.1072 - val_loss: 0.0890\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1059 - val_loss: 0.0859\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.1052 - val_loss: 0.0827\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 3s 87us/sample - loss: 0.1039 - val_loss: 0.0827\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.1032 - val_loss: 0.0781\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 3s 85us/sample - loss: 0.1028 - val_loss: 0.0769\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1025 - val_loss: 0.0827\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.1017 - val_loss: 0.0768\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1012 - val_loss: 0.0761\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.1011 - val_loss: 0.0734\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.1005 - val_loss: 0.0803\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.1002 - val_loss: 0.0709\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0995 - val_loss: 0.0777\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 3s 86us/sample - loss: 0.0999 - val_loss: 0.0709\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.0989 - val_loss: 0.0749\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.0987 - val_loss: 0.0692\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0986 - val_loss: 0.0731\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.0979 - val_loss: 0.0766\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.0980 - val_loss: 0.0690\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.0979 - val_loss: 0.0695\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.0972 - val_loss: 0.0730\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.0969 - val_loss: 0.0769\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 3s 80us/sample - loss: 0.0969 - val_loss: 0.0695\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 3s 77us/sample - loss: 0.0961 - val_loss: 0.0678\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0960 - val_loss: 0.0710\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.0959 - val_loss: 0.0710\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.0957 - val_loss: 0.0675\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0954 - val_loss: 0.0704\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0951 - val_loss: 0.0734\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.0949 - val_loss: 0.0687\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0946 - val_loss: 0.0668\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 3s 79us/sample - loss: 0.0946 - val_loss: 0.0688\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.0944 - val_loss: 0.0676\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.0943 - val_loss: 0.0634\n",
      "Epoch 51/100\n",
      "33340/33340 [==============================] - 3s 83us/sample - loss: 0.0943 - val_loss: 0.0671\n",
      "Epoch 52/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0939 - val_loss: 0.0686\n",
      "Epoch 53/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0939 - val_loss: 0.0645\n",
      "Epoch 54/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0938 - val_loss: 0.0641\n",
      "Epoch 55/100\n",
      "33340/33340 [==============================] - 3s 84us/sample - loss: 0.0938 - val_loss: 0.0646\n",
      "Epoch 56/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0938 - val_loss: 0.0652\n",
      "Epoch 57/100\n",
      "33340/33340 [==============================] - 3s 78us/sample - loss: 0.0942 - val_loss: 0.0682\n",
      "Epoch 58/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.0935 - val_loss: 0.0674\n",
      "Epoch 59/100\n",
      "33340/33340 [==============================] - 3s 81us/sample - loss: 0.0933 - val_loss: 0.0680\n",
      "Epoch 60/100\n",
      "33340/33340 [==============================] - 3s 82us/sample - loss: 0.0934 - val_loss: 0.0645\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 2\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 10s 306us/sample - loss: 0.1718 - val_loss: 0.1099\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1320 - val_loss: 0.1040\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 3s 87us/sample - loss: 0.1263 - val_loss: 0.1027\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1236 - val_loss: 0.1045\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1217 - val_loss: 0.1031\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1203 - val_loss: 0.1038\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1189 - val_loss: 0.0991\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1177 - val_loss: 0.1011\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1165 - val_loss: 0.0982\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1156 - val_loss: 0.0993\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1149 - val_loss: 0.0993\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1140 - val_loss: 0.0968\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1134 - val_loss: 0.0996\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1129 - val_loss: 0.0990\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1123 - val_loss: 0.0973\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1119 - val_loss: 0.0968\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1116 - val_loss: 0.0944\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1112 - val_loss: 0.0993\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1108 - val_loss: 0.0945\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1106 - val_loss: 0.0991\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1102 - val_loss: 0.0933\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1099 - val_loss: 0.0979\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1097 - val_loss: 0.0963\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1093 - val_loss: 0.0955\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1093 - val_loss: 0.0938\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 3s 87us/sample - loss: 0.1089 - val_loss: 0.0938\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1086 - val_loss: 0.0923\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1084 - val_loss: 0.0991\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1082 - val_loss: 0.0954\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 3s 88us/sample - loss: 0.1080 - val_loss: 0.0921\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1077 - val_loss: 0.0921\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1075 - val_loss: 0.0959\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1073 - val_loss: 0.0937\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1072 - val_loss: 0.0898\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1069 - val_loss: 0.0912\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1067 - val_loss: 0.0897\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1065 - val_loss: 0.0906\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1062 - val_loss: 0.0874\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 3s 87us/sample - loss: 0.1062 - val_loss: 0.0900\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1060 - val_loss: 0.0869\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1058 - val_loss: 0.0911\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1055 - val_loss: 0.0930\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1054 - val_loss: 0.0864\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1051 - val_loss: 0.0922\n",
      "Epoch 45/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1051 - val_loss: 0.0835\n",
      "Epoch 46/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1049 - val_loss: 0.0841\n",
      "Epoch 47/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1047 - val_loss: 0.0837\n",
      "Epoch 48/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1047 - val_loss: 0.0873\n",
      "Epoch 49/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1045 - val_loss: 0.0846\n",
      "Epoch 50/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1044 - val_loss: 0.0845\n",
      "Epoch 51/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1041 - val_loss: 0.0843\n",
      "Epoch 52/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1040 - val_loss: 0.0870\n",
      "Epoch 53/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1040 - val_loss: 0.0859\n",
      "Epoch 54/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1037 - val_loss: 0.0788\n",
      "Epoch 55/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1037 - val_loss: 0.0844\n",
      "Epoch 56/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1036 - val_loss: 0.0877\n",
      "Epoch 57/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1034 - val_loss: 0.0844\n",
      "Epoch 58/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1033 - val_loss: 0.0825\n",
      "Epoch 59/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1031 - val_loss: 0.0826\n",
      "Epoch 60/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1030 - val_loss: 0.0854\n",
      "Epoch 61/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1028 - val_loss: 0.0838\n",
      "Epoch 62/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1027 - val_loss: 0.0799\n",
      "Epoch 63/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1027 - val_loss: 0.0799\n",
      "Epoch 64/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1025 - val_loss: 0.0838\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 4\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 10s 305us/sample - loss: 0.1533 - val_loss: 0.1089\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1265 - val_loss: 0.1061\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1217 - val_loss: 0.1040\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1191 - val_loss: 0.1011\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 3s 89us/sample - loss: 0.1173 - val_loss: 0.1015\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1158 - val_loss: 0.1079\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 3s 89us/sample - loss: 0.1146 - val_loss: 0.1000\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1136 - val_loss: 0.0939\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1128 - val_loss: 0.0955\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1120 - val_loss: 0.0931\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1115 - val_loss: 0.0977\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1108 - val_loss: 0.0985\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1104 - val_loss: 0.1021\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1101 - val_loss: 0.0972\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1095 - val_loss: 0.0896\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1094 - val_loss: 0.0919\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1090 - val_loss: 0.0901\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 3s 77us/sample - loss: 0.1087 - val_loss: 0.0955\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1082 - val_loss: 0.0948\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1081 - val_loss: 0.0918\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 73us/sample - loss: 0.1076 - val_loss: 0.0928\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 3s 76us/sample - loss: 0.1073 - val_loss: 0.0944\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1072 - val_loss: 0.0882\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1068 - val_loss: 0.0925\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1067 - val_loss: 0.0917\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1064 - val_loss: 0.0909\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1060 - val_loss: 0.0892\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1059 - val_loss: 0.0924\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1056 - val_loss: 0.0892\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1054 - val_loss: 0.0861\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1052 - val_loss: 0.0872\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1050 - val_loss: 0.0900\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1047 - val_loss: 0.0937\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 3s 76us/sample - loss: 0.1047 - val_loss: 0.0836\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1044 - val_loss: 0.0907\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1042 - val_loss: 0.0878\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1040 - val_loss: 0.0892\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1038 - val_loss: 0.0840\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1035 - val_loss: 0.0881\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1035 - val_loss: 0.0847\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 3s 87us/sample - loss: 0.1032 - val_loss: 0.0895\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1032 - val_loss: 0.0896\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1030 - val_loss: 0.0806\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1028 - val_loss: 0.0871\n",
      "Epoch 45/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1027 - val_loss: 0.0834\n",
      "Epoch 46/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1025 - val_loss: 0.0823\n",
      "Epoch 47/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1023 - val_loss: 0.0810\n",
      "Epoch 48/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1022 - val_loss: 0.0844\n",
      "Epoch 49/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1020 - val_loss: 0.0772\n",
      "Epoch 50/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1021 - val_loss: 0.0803\n",
      "Epoch 51/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1019 - val_loss: 0.0834\n",
      "Epoch 52/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1017 - val_loss: 0.0862\n",
      "Epoch 53/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1015 - val_loss: 0.0852\n",
      "Epoch 54/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1013 - val_loss: 0.0804\n",
      "Epoch 55/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1014 - val_loss: 0.0818\n",
      "Epoch 56/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1011 - val_loss: 0.0840\n",
      "Epoch 57/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1011 - val_loss: 0.0889\n",
      "Epoch 58/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1011 - val_loss: 0.0799\n",
      "Epoch 59/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1007 - val_loss: 0.0769\n",
      "Epoch 60/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1006 - val_loss: 0.0816\n",
      "Epoch 61/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1006 - val_loss: 0.0836\n",
      "Epoch 62/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1005 - val_loss: 0.0781\n",
      "Epoch 63/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1004 - val_loss: 0.0796\n",
      "Epoch 64/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1003 - val_loss: 0.0861\n",
      "Epoch 65/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1004 - val_loss: 0.0795\n",
      "Epoch 66/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1000 - val_loss: 0.0845\n",
      "Epoch 67/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1000 - val_loss: 0.0802\n",
      "Epoch 68/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.0998 - val_loss: 0.0806\n",
      "Epoch 69/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.0998 - val_loss: 0.0780\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 8\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 10s 304us/sample - loss: 0.1478 - val_loss: 0.1012\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1199 - val_loss: 0.1095\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1157 - val_loss: 0.0923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1139 - val_loss: 0.0941\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1127 - val_loss: 0.1001\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1116 - val_loss: 0.1046\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1111 - val_loss: 0.0923\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1106 - val_loss: 0.0985\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1098 - val_loss: 0.0992\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 3s 87us/sample - loss: 0.1094 - val_loss: 0.0866\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1092 - val_loss: 0.0996\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1084 - val_loss: 0.0985\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1082 - val_loss: 0.0960\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1078 - val_loss: 0.0963\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1072 - val_loss: 0.0877\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1069 - val_loss: 0.0910\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1064 - val_loss: 0.0854\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1061 - val_loss: 0.0903\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1056 - val_loss: 0.0960\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1055 - val_loss: 0.0847\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1052 - val_loss: 0.0895\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1047 - val_loss: 0.0922\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1045 - val_loss: 0.0881\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1039 - val_loss: 0.0875\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1034 - val_loss: 0.0874\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1033 - val_loss: 0.0882\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1028 - val_loss: 0.0844\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1026 - val_loss: 0.0819\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1025 - val_loss: 0.0789\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1020 - val_loss: 0.0813\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1017 - val_loss: 0.0822\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1014 - val_loss: 0.0812\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1010 - val_loss: 0.0877\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1012 - val_loss: 0.0735\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1009 - val_loss: 0.0815\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1006 - val_loss: 0.0790\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1003 - val_loss: 0.0797\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 3s 75us/sample - loss: 0.1000 - val_loss: 0.0767\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0999 - val_loss: 0.0796\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0998 - val_loss: 0.0786\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.0996 - val_loss: 0.0789\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.0993 - val_loss: 0.0824\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.0992 - val_loss: 0.0744\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.0990 - val_loss: 0.0808\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 16\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 10s 309us/sample - loss: 0.1416 - val_loss: 0.1024\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1186 - val_loss: 0.1145\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1154 - val_loss: 0.0933\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1141 - val_loss: 0.0935\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 3s 77us/sample - loss: 0.1127 - val_loss: 0.1005\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 3s 76us/sample - loss: 0.1116 - val_loss: 0.1037\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1108 - val_loss: 0.0943\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1104 - val_loss: 0.1016\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 3s 76us/sample - loss: 0.1097 - val_loss: 0.1031\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1088 - val_loss: 0.0855\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 3s 77us/sample - loss: 0.1088 - val_loss: 0.0984\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1078 - val_loss: 0.0971\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1077 - val_loss: 0.0926\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1069 - val_loss: 0.1030\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1065 - val_loss: 0.0869\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1063 - val_loss: 0.0902\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1056 - val_loss: 0.0848\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1051 - val_loss: 0.0923\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1046 - val_loss: 0.0954\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 3s 76us/sample - loss: 0.1043 - val_loss: 0.0810\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1039 - val_loss: 0.0835\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1032 - val_loss: 0.0956\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1030 - val_loss: 0.0910\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1025 - val_loss: 0.0915\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1019 - val_loss: 0.0842\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1014 - val_loss: 0.0899\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1011 - val_loss: 0.0809\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.1010 - val_loss: 0.0877\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1007 - val_loss: 0.0820\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 2s 75us/sample - loss: 0.1001 - val_loss: 0.0870\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.0997 - val_loss: 0.0781\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0996 - val_loss: 0.0791\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.0991 - val_loss: 0.0809\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0989 - val_loss: 0.0753\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.0988 - val_loss: 0.0821\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 3s 76us/sample - loss: 0.0987 - val_loss: 0.0781\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.0982 - val_loss: 0.0758\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0980 - val_loss: 0.0790\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0977 - val_loss: 0.0748\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 3s 77us/sample - loss: 0.0979 - val_loss: 0.0775\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.0976 - val_loss: 0.0772\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.0973 - val_loss: 0.0842\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 3s 76us/sample - loss: 0.0969 - val_loss: 0.0808\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.0969 - val_loss: 0.0802\n",
      "Epoch 45/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.0968 - val_loss: 0.0735\n",
      "Epoch 46/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.0969 - val_loss: 0.0809\n",
      "Epoch 47/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0966 - val_loss: 0.0744\n",
      "Epoch 48/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.0965 - val_loss: 0.0795\n",
      "Epoch 49/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0964 - val_loss: 0.0760\n",
      "Epoch 50/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0961 - val_loss: 0.0757\n",
      "Epoch 51/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.0961 - val_loss: 0.0854\n",
      "Epoch 52/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.0959 - val_loss: 0.0774\n",
      "Epoch 53/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.0959 - val_loss: 0.0765\n",
      "Epoch 54/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0957 - val_loss: 0.0798\n",
      "Epoch 55/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.0958 - val_loss: 0.0800\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 32\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 9s 280us/sample - loss: 0.1391 - val_loss: 0.0997\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1174 - val_loss: 0.1176\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1153 - val_loss: 0.0945\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1139 - val_loss: 0.1002\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1126 - val_loss: 0.0969\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1121 - val_loss: 0.1001\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1112 - val_loss: 0.0966\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1107 - val_loss: 0.1055\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1109 - val_loss: 0.1056\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 3s 89us/sample - loss: 0.1096 - val_loss: 0.0946\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1095 - val_loss: 0.0997\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1084 - val_loss: 0.0990\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 3s 77us/sample - loss: 0.1079 - val_loss: 0.0959\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 64\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 10s 315us/sample - loss: 0.1379 - val_loss: 0.1012\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 3s 77us/sample - loss: 0.1169 - val_loss: 0.1178\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1154 - val_loss: 0.0999\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1140 - val_loss: 0.1059\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1124 - val_loss: 0.0916\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1114 - val_loss: 0.1025\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1107 - val_loss: 0.0933\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1097 - val_loss: 0.1053\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1092 - val_loss: 0.1013\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1078 - val_loss: 0.0929\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1069 - val_loss: 0.0910\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1064 - val_loss: 0.0973\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1053 - val_loss: 0.0990\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1046 - val_loss: 0.0906\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1042 - val_loss: 0.0815\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1035 - val_loss: 0.0861\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 3s 77us/sample - loss: 0.1021 - val_loss: 0.0804\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.1018 - val_loss: 0.0762\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1007 - val_loss: 0.0880\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.1002 - val_loss: 0.0863\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.0995 - val_loss: 0.0814\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.0985 - val_loss: 0.0803\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0978 - val_loss: 0.0794\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.0971 - val_loss: 0.0847\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.0965 - val_loss: 0.0784\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.0958 - val_loss: 0.0790\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 3s 76us/sample - loss: 0.0957 - val_loss: 0.0763\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.0954 - val_loss: 0.0771\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 128\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 10s 306us/sample - loss: 0.1336 - val_loss: 0.0980\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1177 - val_loss: 0.1074\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1160 - val_loss: 0.1027\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1140 - val_loss: 0.1016\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.1137 - val_loss: 0.0922\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.1127 - val_loss: 0.0987\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1120 - val_loss: 0.0977\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 3s 77us/sample - loss: 0.1107 - val_loss: 0.1060\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 3s 78us/sample - loss: 0.1099 - val_loss: 0.0926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1085 - val_loss: 0.0944\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 3s 86us/sample - loss: 0.1073 - val_loss: 0.0956\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.1068 - val_loss: 0.0985\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.1049 - val_loss: 0.1007\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1037 - val_loss: 0.0887\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1026 - val_loss: 0.0765\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1016 - val_loss: 0.0747\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.0999 - val_loss: 0.0726\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 75us/sample - loss: 0.0989 - val_loss: 0.0709\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0976 - val_loss: 0.0770\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 3s 85us/sample - loss: 0.0970 - val_loss: 0.0866\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.0967 - val_loss: 0.0732\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.0959 - val_loss: 0.0762\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0952 - val_loss: 0.0782\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.0946 - val_loss: 0.0773\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 3s 87us/sample - loss: 0.0942 - val_loss: 0.0703\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 3s 81us/sample - loss: 0.0936 - val_loss: 0.0731\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0934 - val_loss: 0.0716\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0932 - val_loss: 0.0698\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0937 - val_loss: 0.0708\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.0926 - val_loss: 0.0700\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 3s 83us/sample - loss: 0.0925 - val_loss: 0.0673\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 2s 73us/sample - loss: 0.0921 - val_loss: 0.0707\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 2s 73us/sample - loss: 0.0917 - val_loss: 0.0670\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.0915 - val_loss: 0.0644\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0917 - val_loss: 0.0664\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0915 - val_loss: 0.0653\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.0910 - val_loss: 0.0694\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0909 - val_loss: 0.0675\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0907 - val_loss: 0.0667\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 3s 87us/sample - loss: 0.0907 - val_loss: 0.0680\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 3s 84us/sample - loss: 0.0906 - val_loss: 0.0651\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 3s 80us/sample - loss: 0.0904 - val_loss: 0.0715\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 3s 79us/sample - loss: 0.0901 - val_loss: 0.0667\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 3s 82us/sample - loss: 0.0899 - val_loss: 0.0653\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 2\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 10s 306us/sample - loss: 0.1675 - val_loss: 0.1274\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1376 - val_loss: 0.1102\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1283 - val_loss: 0.1068\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1244 - val_loss: 0.1075\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1222 - val_loss: 0.1077\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1211 - val_loss: 0.1062\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1200 - val_loss: 0.1061\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1191 - val_loss: 0.1069\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1185 - val_loss: 0.1048\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1179 - val_loss: 0.1062\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1174 - val_loss: 0.1029\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1171 - val_loss: 0.1041\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1166 - val_loss: 0.1045\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1163 - val_loss: 0.1046\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1160 - val_loss: 0.1048\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1157 - val_loss: 0.1064\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1154 - val_loss: 0.1056\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1151 - val_loss: 0.1034\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1149 - val_loss: 0.1032\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1146 - val_loss: 0.1036\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1144 - val_loss: 0.1039\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 4\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 9s 270us/sample - loss: 0.1800 - val_loss: 0.1311\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 3s 77us/sample - loss: 0.1338 - val_loss: 0.1056\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1247 - val_loss: 0.1047\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 3s 76us/sample - loss: 0.1215 - val_loss: 0.1081\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 3s 76us/sample - loss: 0.1197 - val_loss: 0.1072\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1186 - val_loss: 0.1038\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1177 - val_loss: 0.1055\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1170 - val_loss: 0.1034\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1165 - val_loss: 0.1028\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1159 - val_loss: 0.1056\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1155 - val_loss: 0.1020\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 3s 77us/sample - loss: 0.1151 - val_loss: 0.1010\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1147 - val_loss: 0.1014\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1143 - val_loss: 0.1043\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1141 - val_loss: 0.1013\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1137 - val_loss: 0.1031\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1134 - val_loss: 0.1023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1131 - val_loss: 0.1024\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1130 - val_loss: 0.1043\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1126 - val_loss: 0.1003\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1123 - val_loss: 0.1027\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1121 - val_loss: 0.1002\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1117 - val_loss: 0.1012\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1115 - val_loss: 0.1014\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1112 - val_loss: 0.0992\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1109 - val_loss: 0.0987\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1107 - val_loss: 0.0991\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1104 - val_loss: 0.0982\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1103 - val_loss: 0.1007\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1100 - val_loss: 0.0968\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1097 - val_loss: 0.0977\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1096 - val_loss: 0.1015\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1094 - val_loss: 0.0953\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1091 - val_loss: 0.0982\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1091 - val_loss: 0.0966\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1086 - val_loss: 0.1031\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1083 - val_loss: 0.1001\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1081 - val_loss: 0.0961\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1079 - val_loss: 0.0957\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1076 - val_loss: 0.0921\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1074 - val_loss: 0.0936\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1072 - val_loss: 0.0934\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1069 - val_loss: 0.0962\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1068 - val_loss: 0.0950\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1065 - val_loss: 0.0934\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1062 - val_loss: 0.0931\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1061 - val_loss: 0.0950\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1058 - val_loss: 0.0904\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1056 - val_loss: 0.0922\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1055 - val_loss: 0.0939\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1052 - val_loss: 0.0909\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1051 - val_loss: 0.0910\n",
      "Epoch 53/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1047 - val_loss: 0.0898\n",
      "Epoch 54/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1047 - val_loss: 0.0924\n",
      "Epoch 55/100\n",
      "33960/33960 [==============================] - 3s 77us/sample - loss: 0.1044 - val_loss: 0.0906\n",
      "Epoch 56/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1042 - val_loss: 0.0897\n",
      "Epoch 57/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1039 - val_loss: 0.0932\n",
      "Epoch 58/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1038 - val_loss: 0.0880\n",
      "Epoch 59/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1036 - val_loss: 0.0895\n",
      "Epoch 60/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1035 - val_loss: 0.0888\n",
      "Epoch 61/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1035 - val_loss: 0.0917\n",
      "Epoch 62/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1035 - val_loss: 0.0896\n",
      "Epoch 63/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1030 - val_loss: 0.0902\n",
      "Epoch 64/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1030 - val_loss: 0.0883\n",
      "Epoch 65/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1028 - val_loss: 0.0876\n",
      "Epoch 66/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1028 - val_loss: 0.0885\n",
      "Epoch 67/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1025 - val_loss: 0.0919\n",
      "Epoch 68/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1024 - val_loss: 0.0886\n",
      "Epoch 69/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1023 - val_loss: 0.0871\n",
      "Epoch 70/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1024 - val_loss: 0.0873\n",
      "Epoch 71/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1023 - val_loss: 0.0874\n",
      "Epoch 72/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1021 - val_loss: 0.0877\n",
      "Epoch 73/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1019 - val_loss: 0.0904\n",
      "Epoch 74/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1018 - val_loss: 0.0865\n",
      "Epoch 75/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1017 - val_loss: 0.0879\n",
      "Epoch 76/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1018 - val_loss: 0.0877\n",
      "Epoch 77/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1014 - val_loss: 0.0883\n",
      "Epoch 78/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1014 - val_loss: 0.0858\n",
      "Epoch 79/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1013 - val_loss: 0.0893\n",
      "Epoch 80/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1012 - val_loss: 0.0883\n",
      "Epoch 81/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1012 - val_loss: 0.0857\n",
      "Epoch 82/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1010 - val_loss: 0.0849\n",
      "Epoch 83/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1009 - val_loss: 0.0873\n",
      "Epoch 84/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1008 - val_loss: 0.0849\n",
      "Epoch 85/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1008 - val_loss: 0.0858\n",
      "Epoch 86/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1007 - val_loss: 0.0843\n",
      "Epoch 87/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1007 - val_loss: 0.0856\n",
      "Epoch 88/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1005 - val_loss: 0.0878\n",
      "Epoch 89/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1004 - val_loss: 0.0884\n",
      "Epoch 90/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1004 - val_loss: 0.0835\n",
      "Epoch 91/100\n",
      "33960/33960 [==============================] - 2s 72us/sample - loss: 0.1004 - val_loss: 0.0851\n",
      "Epoch 92/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1002 - val_loss: 0.0883\n",
      "Epoch 93/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1001 - val_loss: 0.0865\n",
      "Epoch 94/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1000 - val_loss: 0.0845\n",
      "Epoch 95/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1001 - val_loss: 0.0878\n",
      "Epoch 96/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1000 - val_loss: 0.0861\n",
      "Epoch 97/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0999 - val_loss: 0.0873\n",
      "Epoch 98/100\n",
      "33960/33960 [==============================] - 3s 76us/sample - loss: 0.0998 - val_loss: 0.0839\n",
      "Epoch 99/100\n",
      "33960/33960 [==============================] - 3s 77us/sample - loss: 0.0997 - val_loss: 0.0904\n",
      "Epoch 100/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0997 - val_loss: 0.0843\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 8\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 10s 308us/sample - loss: 0.1593 - val_loss: 0.1108\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1263 - val_loss: 0.1054\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1217 - val_loss: 0.1043\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1198 - val_loss: 0.1095\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1185 - val_loss: 0.1069\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1176 - val_loss: 0.1034\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1168 - val_loss: 0.1028\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1162 - val_loss: 0.1074\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1156 - val_loss: 0.1015\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1148 - val_loss: 0.1037\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1143 - val_loss: 0.1022\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1137 - val_loss: 0.0996\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1134 - val_loss: 0.1022\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1129 - val_loss: 0.1074\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1127 - val_loss: 0.1018\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1122 - val_loss: 0.0990\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1120 - val_loss: 0.1025\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1116 - val_loss: 0.1054\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1114 - val_loss: 0.1098\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1111 - val_loss: 0.0956\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1108 - val_loss: 0.1045\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1105 - val_loss: 0.1015\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1102 - val_loss: 0.1007\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1101 - val_loss: 0.0986\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1097 - val_loss: 0.0962\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1094 - val_loss: 0.0956\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1091 - val_loss: 0.0974\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1089 - val_loss: 0.0975\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1088 - val_loss: 0.0972\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1086 - val_loss: 0.0960\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1081 - val_loss: 0.0970\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1082 - val_loss: 0.0984\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1080 - val_loss: 0.0966\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1075 - val_loss: 0.0944\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1075 - val_loss: 0.0954\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1071 - val_loss: 0.0992\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1068 - val_loss: 0.0985\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1067 - val_loss: 0.0962\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1064 - val_loss: 0.0939\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.1061 - val_loss: 0.0901\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1061 - val_loss: 0.0987\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1058 - val_loss: 0.0889\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 3s 77us/sample - loss: 0.1056 - val_loss: 0.0938\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1053 - val_loss: 0.0938\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1052 - val_loss: 0.0943\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1048 - val_loss: 0.0917\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1046 - val_loss: 0.0975\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1046 - val_loss: 0.0880\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1043 - val_loss: 0.0915\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1042 - val_loss: 0.0909\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1038 - val_loss: 0.0891\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1036 - val_loss: 0.0913\n",
      "Epoch 53/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1034 - val_loss: 0.0869\n",
      "Epoch 54/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1036 - val_loss: 0.0914\n",
      "Epoch 55/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1031 - val_loss: 0.0912\n",
      "Epoch 56/100\n",
      "33960/33960 [==============================] - 2s 72us/sample - loss: 0.1028 - val_loss: 0.0897\n",
      "Epoch 57/100\n",
      "33960/33960 [==============================] - 2s 73us/sample - loss: 0.1026 - val_loss: 0.0928\n",
      "Epoch 58/100\n",
      "33960/33960 [==============================] - 3s 75us/sample - loss: 0.1025 - val_loss: 0.0857\n",
      "Epoch 59/100\n",
      "33960/33960 [==============================] - 3s 77us/sample - loss: 0.1020 - val_loss: 0.0863\n",
      "Epoch 60/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1020 - val_loss: 0.0889\n",
      "Epoch 61/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1019 - val_loss: 0.0921\n",
      "Epoch 62/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1019 - val_loss: 0.0870\n",
      "Epoch 63/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1015 - val_loss: 0.0880\n",
      "Epoch 64/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.1013 - val_loss: 0.0836\n",
      "Epoch 65/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.1012 - val_loss: 0.0856\n",
      "Epoch 66/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1012 - val_loss: 0.0880\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.1007 - val_loss: 0.0892\n",
      "Epoch 68/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1006 - val_loss: 0.0883\n",
      "Epoch 69/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1005 - val_loss: 0.0872\n",
      "Epoch 70/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1004 - val_loss: 0.0863\n",
      "Epoch 71/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1006 - val_loss: 0.0827\n",
      "Epoch 72/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1002 - val_loss: 0.0883\n",
      "Epoch 73/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1000 - val_loss: 0.0866\n",
      "Epoch 74/100\n",
      "33960/33960 [==============================] - 3s 74us/sample - loss: 0.0999 - val_loss: 0.0827\n",
      "Epoch 75/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0997 - val_loss: 0.0863\n",
      "Epoch 76/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1000 - val_loss: 0.0898\n",
      "Epoch 77/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0996 - val_loss: 0.0887\n",
      "Epoch 78/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0995 - val_loss: 0.0815\n",
      "Epoch 79/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0995 - val_loss: 0.0889\n",
      "Epoch 80/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0993 - val_loss: 0.0853\n",
      "Epoch 81/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0991 - val_loss: 0.0836\n",
      "Epoch 82/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0990 - val_loss: 0.0825\n",
      "Epoch 83/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0990 - val_loss: 0.0851\n",
      "Epoch 84/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0989 - val_loss: 0.0814\n",
      "Epoch 85/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0987 - val_loss: 0.0848\n",
      "Epoch 86/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0986 - val_loss: 0.0808\n",
      "Epoch 87/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0988 - val_loss: 0.0849\n",
      "Epoch 88/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0984 - val_loss: 0.0848\n",
      "Epoch 89/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0984 - val_loss: 0.0849\n",
      "Epoch 90/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0983 - val_loss: 0.0821\n",
      "Epoch 91/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0983 - val_loss: 0.0832\n",
      "Epoch 92/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0982 - val_loss: 0.0886\n",
      "Epoch 93/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0980 - val_loss: 0.0853\n",
      "Epoch 94/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.0980 - val_loss: 0.0845\n",
      "Epoch 95/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.0978 - val_loss: 0.0862\n",
      "Epoch 96/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.0979 - val_loss: 0.0842\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 16\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 10s 304us/sample - loss: 0.1487 - val_loss: 0.1051\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1228 - val_loss: 0.1108\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1199 - val_loss: 0.1065\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1183 - val_loss: 0.1126\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1172 - val_loss: 0.1082\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1165 - val_loss: 0.1092\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1160 - val_loss: 0.1054\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1153 - val_loss: 0.1079\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1148 - val_loss: 0.1026\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1143 - val_loss: 0.1024\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1139 - val_loss: 0.1041\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1133 - val_loss: 0.1023\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1131 - val_loss: 0.1017\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1124 - val_loss: 0.1095\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1121 - val_loss: 0.1037\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.1117 - val_loss: 0.1025\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1113 - val_loss: 0.1027\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1109 - val_loss: 0.1099\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1106 - val_loss: 0.1104\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1100 - val_loss: 0.0983\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1097 - val_loss: 0.1050\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.1094 - val_loss: 0.1029\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1088 - val_loss: 0.1017\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.1086 - val_loss: 0.0999\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1082 - val_loss: 0.0997\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1077 - val_loss: 0.0981\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 3s 76us/sample - loss: 0.1076 - val_loss: 0.0976\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.1070 - val_loss: 0.0997\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1067 - val_loss: 0.1003\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1063 - val_loss: 0.0936\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1060 - val_loss: 0.0962\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1060 - val_loss: 0.1058\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1056 - val_loss: 0.1048\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1051 - val_loss: 0.0924\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1051 - val_loss: 0.0934\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1046 - val_loss: 0.0971\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1040 - val_loss: 0.0940\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 3s 76us/sample - loss: 0.1039 - val_loss: 0.0943\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1036 - val_loss: 0.0905\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1033 - val_loss: 0.0959\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1031 - val_loss: 0.0965\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1027 - val_loss: 0.0911\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1024 - val_loss: 0.0918\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1021 - val_loss: 0.0919\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1020 - val_loss: 0.0908\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1018 - val_loss: 0.0937\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1014 - val_loss: 0.0923\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.1012 - val_loss: 0.0889\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1011 - val_loss: 0.0923\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1008 - val_loss: 0.0900\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1004 - val_loss: 0.0914\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1003 - val_loss: 0.0890\n",
      "Epoch 53/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.1003 - val_loss: 0.0905\n",
      "Epoch 54/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1000 - val_loss: 0.0894\n",
      "Epoch 55/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0998 - val_loss: 0.0901\n",
      "Epoch 56/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0996 - val_loss: 0.0918\n",
      "Epoch 57/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.0994 - val_loss: 0.0898\n",
      "Epoch 58/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0994 - val_loss: 0.0865\n",
      "Epoch 59/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0989 - val_loss: 0.0886\n",
      "Epoch 60/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0989 - val_loss: 0.0876\n",
      "Epoch 61/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.0989 - val_loss: 0.0875\n",
      "Epoch 62/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0990 - val_loss: 0.0867\n",
      "Epoch 63/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0984 - val_loss: 0.0912\n",
      "Epoch 64/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.0985 - val_loss: 0.0841\n",
      "Epoch 65/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.0983 - val_loss: 0.0880\n",
      "Epoch 66/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0981 - val_loss: 0.0889\n",
      "Epoch 67/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.0978 - val_loss: 0.0867\n",
      "Epoch 68/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0980 - val_loss: 0.0868\n",
      "Epoch 69/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.0980 - val_loss: 0.0848\n",
      "Epoch 70/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0978 - val_loss: 0.0867\n",
      "Epoch 71/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0976 - val_loss: 0.0861\n",
      "Epoch 72/100\n",
      "33960/33960 [==============================] - 3s 76us/sample - loss: 0.0975 - val_loss: 0.0893\n",
      "Epoch 73/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0972 - val_loss: 0.0870\n",
      "Epoch 74/100\n",
      "33960/33960 [==============================] - 2s 72us/sample - loss: 0.0972 - val_loss: 0.0831\n",
      "Epoch 75/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0971 - val_loss: 0.0858\n",
      "Epoch 76/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0971 - val_loss: 0.0867\n",
      "Epoch 77/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0969 - val_loss: 0.0924\n",
      "Epoch 78/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0969 - val_loss: 0.0919\n",
      "Epoch 79/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0967 - val_loss: 0.0936\n",
      "Epoch 80/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0964 - val_loss: 0.0838\n",
      "Epoch 81/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0964 - val_loss: 0.0905\n",
      "Epoch 82/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0967 - val_loss: 0.0864\n",
      "Epoch 83/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0963 - val_loss: 0.0861\n",
      "Epoch 84/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0966 - val_loss: 0.0862\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 32\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 9s 272us/sample - loss: 0.1464 - val_loss: 0.1042\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1218 - val_loss: 0.1091\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1192 - val_loss: 0.1063\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1178 - val_loss: 0.1080\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1168 - val_loss: 0.1032\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1162 - val_loss: 0.1112\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1154 - val_loss: 0.1018\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1147 - val_loss: 0.1018\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1142 - val_loss: 0.1055\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1137 - val_loss: 0.1048\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1135 - val_loss: 0.1039\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1125 - val_loss: 0.1101\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 3s 92us/sample - loss: 0.1125 - val_loss: 0.0992\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1120 - val_loss: 0.1033\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 3s 90us/sample - loss: 0.1113 - val_loss: 0.1045\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1111 - val_loss: 0.1022\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1104 - val_loss: 0.1040\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1095 - val_loss: 0.1067\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1090 - val_loss: 0.1015\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1086 - val_loss: 0.0953\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1080 - val_loss: 0.1059\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1074 - val_loss: 0.1057\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1069 - val_loss: 0.0972\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1066 - val_loss: 0.0930\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1060 - val_loss: 0.0948\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1057 - val_loss: 0.0970\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1050 - val_loss: 0.0953\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1044 - val_loss: 0.0967\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1039 - val_loss: 0.0916\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.1037 - val_loss: 0.0898\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1029 - val_loss: 0.0935\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1028 - val_loss: 0.0955\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1025 - val_loss: 0.0979\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1018 - val_loss: 0.0911\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33960/33960 [==============================] - 3s 77us/sample - loss: 0.1019 - val_loss: 0.0836\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1014 - val_loss: 0.0945\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1007 - val_loss: 0.0831\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1009 - val_loss: 0.0880\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1001 - val_loss: 0.0847\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1001 - val_loss: 0.0947\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.0997 - val_loss: 0.0899\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0994 - val_loss: 0.0872\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.0992 - val_loss: 0.0835\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0989 - val_loss: 0.0851\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0987 - val_loss: 0.0831\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0984 - val_loss: 0.0845\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0982 - val_loss: 0.0828\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0979 - val_loss: 0.0856\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0975 - val_loss: 0.0867\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0973 - val_loss: 0.0848\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.0968 - val_loss: 0.0918\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0968 - val_loss: 0.0854\n",
      "Epoch 53/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.0967 - val_loss: 0.0917\n",
      "Epoch 54/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.0965 - val_loss: 0.0836\n",
      "Epoch 55/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0962 - val_loss: 0.0825\n",
      "Epoch 56/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0960 - val_loss: 0.0907\n",
      "Epoch 57/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.0961 - val_loss: 0.0823\n",
      "Epoch 58/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0959 - val_loss: 0.0814\n",
      "Epoch 59/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0953 - val_loss: 0.0812\n",
      "Epoch 60/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0953 - val_loss: 0.0829\n",
      "Epoch 61/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0950 - val_loss: 0.0844\n",
      "Epoch 62/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.0953 - val_loss: 0.0811\n",
      "Epoch 63/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.0952 - val_loss: 0.0864\n",
      "Epoch 64/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0950 - val_loss: 0.0821\n",
      "Epoch 65/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0950 - val_loss: 0.0848\n",
      "Epoch 66/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0950 - val_loss: 0.0819\n",
      "Epoch 67/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0944 - val_loss: 0.0816\n",
      "Epoch 68/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0945 - val_loss: 0.0800\n",
      "Epoch 69/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0943 - val_loss: 0.0844\n",
      "Epoch 70/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0945 - val_loss: 0.0812\n",
      "Epoch 71/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.0941 - val_loss: 0.0855\n",
      "Epoch 72/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0943 - val_loss: 0.0815\n",
      "Epoch 73/100\n",
      "33960/33960 [==============================] - 3s 93us/sample - loss: 0.0942 - val_loss: 0.0831\n",
      "Epoch 74/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.0940 - val_loss: 0.0824\n",
      "Epoch 75/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0940 - val_loss: 0.0820\n",
      "Epoch 76/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0940 - val_loss: 0.0832\n",
      "Epoch 77/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0938 - val_loss: 0.0898\n",
      "Epoch 78/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.0941 - val_loss: 0.0881\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 64\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 10s 309us/sample - loss: 0.1415 - val_loss: 0.1073\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1224 - val_loss: 0.1080\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1200 - val_loss: 0.1080\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1186 - val_loss: 0.1104\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1176 - val_loss: 0.1063\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1166 - val_loss: 0.1055\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1161 - val_loss: 0.1073\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1152 - val_loss: 0.1051\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1145 - val_loss: 0.1105\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1139 - val_loss: 0.1093\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1137 - val_loss: 0.1034\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 3s 77us/sample - loss: 0.1124 - val_loss: 0.1072\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1119 - val_loss: 0.1031\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1116 - val_loss: 0.1048\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1105 - val_loss: 0.1049\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1095 - val_loss: 0.1068\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1095 - val_loss: 0.0951\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 73us/sample - loss: 0.1080 - val_loss: 0.0958\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1073 - val_loss: 0.0940\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1062 - val_loss: 0.0906\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1057 - val_loss: 0.1085\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1051 - val_loss: 0.1028\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1042 - val_loss: 0.0925\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1041 - val_loss: 0.0900\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1026 - val_loss: 0.0979\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1026 - val_loss: 0.0941\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1019 - val_loss: 0.0961\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1014 - val_loss: 0.0926\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1008 - val_loss: 0.0889\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1002 - val_loss: 0.0908\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0999 - val_loss: 0.0929\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0998 - val_loss: 0.0905\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0992 - val_loss: 0.0927\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0986 - val_loss: 0.0918\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0987 - val_loss: 0.0855\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0981 - val_loss: 0.0905\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0975 - val_loss: 0.0832\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0979 - val_loss: 0.0876\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.0972 - val_loss: 0.0856\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0967 - val_loss: 0.0929\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0965 - val_loss: 0.0889\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0964 - val_loss: 0.0861\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0957 - val_loss: 0.0852\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0956 - val_loss: 0.0847\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 3s 77us/sample - loss: 0.0957 - val_loss: 0.0835\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0956 - val_loss: 0.0834\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.0950 - val_loss: 0.0827\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0952 - val_loss: 0.0852\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0947 - val_loss: 0.0854\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0947 - val_loss: 0.0850\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0944 - val_loss: 0.0904\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0945 - val_loss: 0.0870\n",
      "Epoch 53/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.0944 - val_loss: 0.0909\n",
      "Epoch 54/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0945 - val_loss: 0.0853\n",
      "Epoch 55/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0939 - val_loss: 0.0835\n",
      "Epoch 56/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0941 - val_loss: 0.0865\n",
      "Epoch 57/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.0941 - val_loss: 0.0840\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 128\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 11s 310us/sample - loss: 0.1396 - val_loss: 0.1033\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1230 - val_loss: 0.1087\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1206 - val_loss: 0.1072\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1192 - val_loss: 0.1141\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1185 - val_loss: 0.1045\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1175 - val_loss: 0.1051\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.1166 - val_loss: 0.1108\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1156 - val_loss: 0.1089\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1147 - val_loss: 0.1099\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1137 - val_loss: 0.1058\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.1131 - val_loss: 0.1016\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1113 - val_loss: 0.1070\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.1102 - val_loss: 0.0953\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1092 - val_loss: 0.1044\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1077 - val_loss: 0.1050\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 3s 78us/sample - loss: 0.1066 - val_loss: 0.1066\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1064 - val_loss: 0.0906\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1051 - val_loss: 0.0997\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.1044 - val_loss: 0.0913\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1037 - val_loss: 0.0861\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.1030 - val_loss: 0.1019\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1027 - val_loss: 0.0879\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.1016 - val_loss: 0.0929\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.1015 - val_loss: 0.0895\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1006 - val_loss: 0.0916\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.1004 - val_loss: 0.0961\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0999 - val_loss: 0.0935\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0992 - val_loss: 0.0929\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0986 - val_loss: 0.0865\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0981 - val_loss: 0.0838\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0978 - val_loss: 0.0874\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.0978 - val_loss: 0.0875\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0968 - val_loss: 0.0939\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.0967 - val_loss: 0.1003\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 3s 81us/sample - loss: 0.0967 - val_loss: 0.0896\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0959 - val_loss: 0.0905\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0958 - val_loss: 0.0829\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0958 - val_loss: 0.0836\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0953 - val_loss: 0.0860\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0950 - val_loss: 0.0894\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0948 - val_loss: 0.0843\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.0945 - val_loss: 0.0843\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0942 - val_loss: 0.0818\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0942 - val_loss: 0.0856\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 3s 92us/sample - loss: 0.0941 - val_loss: 0.0852\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 3s 83us/sample - loss: 0.0938 - val_loss: 0.0840\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 3s 86us/sample - loss: 0.0936 - val_loss: 0.0837\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.0938 - val_loss: 0.0815\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 3s 87us/sample - loss: 0.0934 - val_loss: 0.0855\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0933 - val_loss: 0.0835\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0931 - val_loss: 0.0857\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0933 - val_loss: 0.0866\n",
      "Epoch 53/100\n",
      "33960/33960 [==============================] - 3s 88us/sample - loss: 0.0928 - val_loss: 0.0832\n",
      "Epoch 54/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0929 - val_loss: 0.0853\n",
      "Epoch 55/100\n",
      "33960/33960 [==============================] - 3s 84us/sample - loss: 0.0931 - val_loss: 0.0834\n",
      "Epoch 56/100\n",
      "33960/33960 [==============================] - 3s 89us/sample - loss: 0.0929 - val_loss: 0.0839\n",
      "Epoch 57/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.0926 - val_loss: 0.0846\n",
      "Epoch 58/100\n",
      "33960/33960 [==============================] - 3s 85us/sample - loss: 0.0926 - val_loss: 0.0824\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 2\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 9s 275us/sample - loss: 0.1751 - val_loss: 0.1307\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1336 - val_loss: 0.1097\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1244 - val_loss: 0.1050\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1212 - val_loss: 0.1030\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1192 - val_loss: 0.1052\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1178 - val_loss: 0.1027\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1165 - val_loss: 0.1081\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 3s 95us/sample - loss: 0.1156 - val_loss: 0.1012\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1147 - val_loss: 0.1001\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1139 - val_loss: 0.0993\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1133 - val_loss: 0.0996\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1126 - val_loss: 0.0991\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1121 - val_loss: 0.1006\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1117 - val_loss: 0.0983\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.1111 - val_loss: 0.0993\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1107 - val_loss: 0.0987\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1103 - val_loss: 0.1003\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1098 - val_loss: 0.0976\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1095 - val_loss: 0.0974\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1092 - val_loss: 0.0976\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 3s 92us/sample - loss: 0.1087 - val_loss: 0.1004\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1084 - val_loss: 0.0952\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 2s 73us/sample - loss: 0.1080 - val_loss: 0.0987\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 2s 73us/sample - loss: 0.1076 - val_loss: 0.0986\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1074 - val_loss: 0.0981\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.1071 - val_loss: 0.0936\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.1069 - val_loss: 0.0941\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.1066 - val_loss: 0.0937\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1063 - val_loss: 0.0932\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1060 - val_loss: 0.0948\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1058 - val_loss: 0.0927\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1056 - val_loss: 0.0930\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1054 - val_loss: 0.0947\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1052 - val_loss: 0.0940\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1051 - val_loss: 0.0926\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.1048 - val_loss: 0.0927\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1046 - val_loss: 0.0934\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1044 - val_loss: 0.0936\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1043 - val_loss: 0.0936\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1042 - val_loss: 0.0920\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1039 - val_loss: 0.0916\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1038 - val_loss: 0.0950\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 3s 92us/sample - loss: 0.1036 - val_loss: 0.0937\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 3s 92us/sample - loss: 0.1035 - val_loss: 0.0927\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1034 - val_loss: 0.0956\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1033 - val_loss: 0.0914\n",
      "Epoch 47/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1031 - val_loss: 0.0923\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.1030 - val_loss: 0.0936\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1029 - val_loss: 0.0911\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1027 - val_loss: 0.0918\n",
      "Epoch 51/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1026 - val_loss: 0.0925\n",
      "Epoch 52/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1024 - val_loss: 0.0961\n",
      "Epoch 53/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1024 - val_loss: 0.0927\n",
      "Epoch 54/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1023 - val_loss: 0.0913\n",
      "Epoch 55/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1022 - val_loss: 0.0920\n",
      "Epoch 56/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1021 - val_loss: 0.0921\n",
      "Epoch 57/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1019 - val_loss: 0.0917\n",
      "Epoch 58/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1019 - val_loss: 0.0897\n",
      "Epoch 59/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.1017 - val_loss: 0.0909\n",
      "Epoch 60/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.1017 - val_loss: 0.0917\n",
      "Epoch 61/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1016 - val_loss: 0.0909\n",
      "Epoch 62/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1015 - val_loss: 0.0915\n",
      "Epoch 63/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1014 - val_loss: 0.0919\n",
      "Epoch 64/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.1013 - val_loss: 0.0895\n",
      "Epoch 65/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1012 - val_loss: 0.0914\n",
      "Epoch 66/100\n",
      "33820/33820 [==============================] - 3s 92us/sample - loss: 0.1012 - val_loss: 0.0906\n",
      "Epoch 67/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1011 - val_loss: 0.0896\n",
      "Epoch 68/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1010 - val_loss: 0.0888\n",
      "Epoch 69/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1009 - val_loss: 0.0907\n",
      "Epoch 70/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1010 - val_loss: 0.0897\n",
      "Epoch 71/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1008 - val_loss: 0.0909\n",
      "Epoch 72/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.1007 - val_loss: 0.0890\n",
      "Epoch 73/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1007 - val_loss: 0.0902\n",
      "Epoch 74/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1006 - val_loss: 0.0901\n",
      "Epoch 75/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1004 - val_loss: 0.0921\n",
      "Epoch 76/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1005 - val_loss: 0.0888\n",
      "Epoch 77/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1004 - val_loss: 0.0901\n",
      "Epoch 78/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1004 - val_loss: 0.0903\n",
      "Epoch 79/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1002 - val_loss: 0.0901\n",
      "Epoch 80/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1002 - val_loss: 0.0889\n",
      "Epoch 81/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1000 - val_loss: 0.0913\n",
      "Epoch 82/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1000 - val_loss: 0.0907\n",
      "Epoch 83/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1000 - val_loss: 0.0883\n",
      "Epoch 84/100\n",
      "33820/33820 [==============================] - 2s 72us/sample - loss: 0.0999 - val_loss: 0.0900\n",
      "Epoch 85/100\n",
      "33820/33820 [==============================] - 2s 70us/sample - loss: 0.0999 - val_loss: 0.0895\n",
      "Epoch 86/100\n",
      "33820/33820 [==============================] - 2s 73us/sample - loss: 0.0998 - val_loss: 0.0903\n",
      "Epoch 87/100\n",
      "33820/33820 [==============================] - 2s 73us/sample - loss: 0.0997 - val_loss: 0.0896\n",
      "Epoch 88/100\n",
      "33820/33820 [==============================] - 2s 71us/sample - loss: 0.0997 - val_loss: 0.0889\n",
      "Epoch 89/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0997 - val_loss: 0.0888\n",
      "Epoch 90/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0995 - val_loss: 0.0884\n",
      "Epoch 91/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.0996 - val_loss: 0.0886\n",
      "Epoch 92/100\n",
      "33820/33820 [==============================] - 3s 92us/sample - loss: 0.0996 - val_loss: 0.0916\n",
      "Epoch 93/100\n",
      "33820/33820 [==============================] - 3s 93us/sample - loss: 0.0994 - val_loss: 0.0889\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 4\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 11s 316us/sample - loss: 0.1679 - val_loss: 0.1158\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1270 - val_loss: 0.1029\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1201 - val_loss: 0.1037\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.1176 - val_loss: 0.0992\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1156 - val_loss: 0.1075\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1141 - val_loss: 0.1007\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1129 - val_loss: 0.1035\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 3s 78us/sample - loss: 0.1121 - val_loss: 0.0998\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.1111 - val_loss: 0.0971\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1102 - val_loss: 0.0961\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1097 - val_loss: 0.0962\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1091 - val_loss: 0.0970\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1086 - val_loss: 0.1007\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1082 - val_loss: 0.0962\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1078 - val_loss: 0.0961\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1074 - val_loss: 0.0969\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1071 - val_loss: 0.0977\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1067 - val_loss: 0.0962\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 3s 92us/sample - loss: 0.1064 - val_loss: 0.0989\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1063 - val_loss: 0.0973\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 8\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 9s 266us/sample - loss: 0.1650 - val_loss: 0.1115\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1223 - val_loss: 0.1083\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1174 - val_loss: 0.1033\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1152 - val_loss: 0.1024\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1134 - val_loss: 0.1103\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1119 - val_loss: 0.1037\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1109 - val_loss: 0.1014\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1101 - val_loss: 0.0996\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1095 - val_loss: 0.0973\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1087 - val_loss: 0.0965\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1083 - val_loss: 0.0987\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1077 - val_loss: 0.0969\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1072 - val_loss: 0.0997\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 3s 79us/sample - loss: 0.1067 - val_loss: 0.1008\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1062 - val_loss: 0.0957\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 3s 92us/sample - loss: 0.1057 - val_loss: 0.0982\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1051 - val_loss: 0.0985\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1048 - val_loss: 0.0950\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 3s 80us/sample - loss: 0.1041 - val_loss: 0.0980\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1038 - val_loss: 0.0975\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1032 - val_loss: 0.0964\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1030 - val_loss: 0.0936\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1025 - val_loss: 0.0978\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1022 - val_loss: 0.0984\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1018 - val_loss: 0.0943\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1015 - val_loss: 0.0923\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1012 - val_loss: 0.0955\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1007 - val_loss: 0.0976\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1004 - val_loss: 0.0929\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.1003 - val_loss: 0.0971\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1000 - val_loss: 0.0941\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0999 - val_loss: 0.0948\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0996 - val_loss: 0.0918\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0992 - val_loss: 0.0915\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0990 - val_loss: 0.0932\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0989 - val_loss: 0.0905\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0987 - val_loss: 0.0943\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0985 - val_loss: 0.0928\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0984 - val_loss: 0.0887\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0981 - val_loss: 0.0913\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0980 - val_loss: 0.0902\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0978 - val_loss: 0.0896\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0977 - val_loss: 0.0913\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0978 - val_loss: 0.0873\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0977 - val_loss: 0.0888\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0973 - val_loss: 0.0895\n",
      "Epoch 47/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0972 - val_loss: 0.0906\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0972 - val_loss: 0.0873\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0973 - val_loss: 0.0881\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0967 - val_loss: 0.0907\n",
      "Epoch 51/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0967 - val_loss: 0.0918\n",
      "Epoch 52/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0966 - val_loss: 0.0951\n",
      "Epoch 53/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0964 - val_loss: 0.0875\n",
      "Epoch 54/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0964 - val_loss: 0.0935\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 16\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 10s 306us/sample - loss: 0.1417 - val_loss: 0.1038\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 3s 80us/sample - loss: 0.1187 - val_loss: 0.1072\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.1158 - val_loss: 0.0994\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1137 - val_loss: 0.1016\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1128 - val_loss: 0.1080\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 3s 80us/sample - loss: 0.1117 - val_loss: 0.1039\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1113 - val_loss: 0.1004\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1106 - val_loss: 0.0991\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.1098 - val_loss: 0.0994\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1093 - val_loss: 0.0963\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1086 - val_loss: 0.0963\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1077 - val_loss: 0.0991\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1070 - val_loss: 0.1038\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1067 - val_loss: 0.1003\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1058 - val_loss: 0.0958\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1049 - val_loss: 0.0990\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1042 - val_loss: 0.0977\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 3s 80us/sample - loss: 0.1035 - val_loss: 0.0932\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.1029 - val_loss: 0.0992\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1025 - val_loss: 0.1000\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1018 - val_loss: 0.0961\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1014 - val_loss: 0.0990\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1012 - val_loss: 0.0963\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 3s 92us/sample - loss: 0.1004 - val_loss: 0.0928\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1000 - val_loss: 0.0894\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 3s 98us/sample - loss: 0.0998 - val_loss: 0.0901\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0992 - val_loss: 0.0943\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0990 - val_loss: 0.0913\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0985 - val_loss: 0.0889\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.0983 - val_loss: 0.0924\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0981 - val_loss: 0.0871\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0980 - val_loss: 0.0906\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0979 - val_loss: 0.0880\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0973 - val_loss: 0.0865\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.0973 - val_loss: 0.0846\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0970 - val_loss: 0.0910\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0966 - val_loss: 0.0891\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 3s 80us/sample - loss: 0.0964 - val_loss: 0.0853\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0960 - val_loss: 0.0862\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0957 - val_loss: 0.0855\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0957 - val_loss: 0.0842\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.0955 - val_loss: 0.0818\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0952 - val_loss: 0.0881\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0952 - val_loss: 0.0828\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0948 - val_loss: 0.0836\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0947 - val_loss: 0.0842\n",
      "Epoch 47/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0946 - val_loss: 0.0861\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.0945 - val_loss: 0.0823\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.0943 - val_loss: 0.0854\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0941 - val_loss: 0.0850\n",
      "Epoch 51/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.0938 - val_loss: 0.0867\n",
      "Epoch 52/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0939 - val_loss: 0.0849\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 32\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 10s 304us/sample - loss: 0.1364 - val_loss: 0.1023\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1175 - val_loss: 0.1048\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1147 - val_loss: 0.1034\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1137 - val_loss: 0.1019\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1124 - val_loss: 0.1036\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1122 - val_loss: 0.1107\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1115 - val_loss: 0.1121\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1109 - val_loss: 0.0986\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1095 - val_loss: 0.0988\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1090 - val_loss: 0.0934\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1082 - val_loss: 0.0950\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1074 - val_loss: 0.0975\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1068 - val_loss: 0.0987\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.1065 - val_loss: 0.0932\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1056 - val_loss: 0.0968\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1047 - val_loss: 0.1009\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1039 - val_loss: 0.0914\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1030 - val_loss: 0.0948\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1028 - val_loss: 0.0961\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.1019 - val_loss: 0.0951\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1009 - val_loss: 0.0982\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.1004 - val_loss: 0.0973\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1003 - val_loss: 0.0952\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0987 - val_loss: 0.0907\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0983 - val_loss: 0.0871\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 3s 80us/sample - loss: 0.0982 - val_loss: 0.0919\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0978 - val_loss: 0.0920\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0971 - val_loss: 0.0944\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 3s 91us/sample - loss: 0.0968 - val_loss: 0.0858\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0965 - val_loss: 0.0922\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0964 - val_loss: 0.0850\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0961 - val_loss: 0.0916\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 3s 80us/sample - loss: 0.0962 - val_loss: 0.0897\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0954 - val_loss: 0.0851\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0954 - val_loss: 0.0828\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0949 - val_loss: 0.0946\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0954 - val_loss: 0.0893\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0945 - val_loss: 0.0848\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0944 - val_loss: 0.0870\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0943 - val_loss: 0.0845\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0940 - val_loss: 0.0878\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0940 - val_loss: 0.0812\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0939 - val_loss: 0.0845\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0935 - val_loss: 0.0832\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0935 - val_loss: 0.0827\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.0934 - val_loss: 0.0899\n",
      "Epoch 47/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0935 - val_loss: 0.0844\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 3s 78us/sample - loss: 0.0931 - val_loss: 0.0837\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 3s 79us/sample - loss: 0.0930 - val_loss: 0.0850\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0926 - val_loss: 0.0829\n",
      "Epoch 51/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0929 - val_loss: 0.0835\n",
      "Epoch 52/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.0923 - val_loss: 0.0854\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 64\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 9s 268us/sample - loss: 0.1336 - val_loss: 0.1087\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1170 - val_loss: 0.1065\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1156 - val_loss: 0.1063\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1145 - val_loss: 0.1020\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 72us/sample - loss: 0.1129 - val_loss: 0.1035\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1123 - val_loss: 0.1013\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 72us/sample - loss: 0.1118 - val_loss: 0.1220\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1108 - val_loss: 0.1060\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 73us/sample - loss: 0.1093 - val_loss: 0.0961\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 3s 77us/sample - loss: 0.1087 - val_loss: 0.0956\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1077 - val_loss: 0.0956\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1066 - val_loss: 0.0965\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1056 - val_loss: 0.0959\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1050 - val_loss: 0.0908\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1039 - val_loss: 0.0916\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1029 - val_loss: 0.0953\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1017 - val_loss: 0.0886\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1006 - val_loss: 0.0930\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1001 - val_loss: 0.0932\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0995 - val_loss: 0.0929\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0985 - val_loss: 0.0957\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0979 - val_loss: 0.0877\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0982 - val_loss: 0.0926\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0965 - val_loss: 0.0861\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0960 - val_loss: 0.0835\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0959 - val_loss: 0.0902\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0952 - val_loss: 0.0895\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 2s 73us/sample - loss: 0.0946 - val_loss: 0.0919\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0943 - val_loss: 0.0826\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0937 - val_loss: 0.0873\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0934 - val_loss: 0.0849\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0930 - val_loss: 0.0823\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0931 - val_loss: 0.0894\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0924 - val_loss: 0.0834\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0920 - val_loss: 0.0870\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0918 - val_loss: 0.0885\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0917 - val_loss: 0.0846\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0915 - val_loss: 0.0829\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0910 - val_loss: 0.0871\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0911 - val_loss: 0.0815\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0907 - val_loss: 0.0818\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 3s 78us/sample - loss: 0.0908 - val_loss: 0.0843\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 3s 79us/sample - loss: 0.0904 - val_loss: 0.0849\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 3s 81us/sample - loss: 0.0903 - val_loss: 0.0818\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0901 - val_loss: 0.0841\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0902 - val_loss: 0.0862\n",
      "Epoch 47/100\n",
      "33820/33820 [==============================] - 3s 78us/sample - loss: 0.0902 - val_loss: 0.0883\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0902 - val_loss: 0.0830\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0902 - val_loss: 0.0875\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0896 - val_loss: 0.0861\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 128\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 11s 323us/sample - loss: 0.1329 - val_loss: 0.1070\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1171 - val_loss: 0.1022\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1158 - val_loss: 0.1015\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1145 - val_loss: 0.1114\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1134 - val_loss: 0.1017\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1121 - val_loss: 0.1001\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.1120 - val_loss: 0.1170\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.1107 - val_loss: 0.1073\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.1094 - val_loss: 0.0992\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1089 - val_loss: 0.1023\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1073 - val_loss: 0.1045\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.1061 - val_loss: 0.1017\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.1041 - val_loss: 0.0930\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.1026 - val_loss: 0.0918\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 3s 80us/sample - loss: 0.1008 - val_loss: 0.0876\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0996 - val_loss: 0.0956\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.0983 - val_loss: 0.0908\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 3s 90us/sample - loss: 0.0977 - val_loss: 0.0987\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0973 - val_loss: 0.0923\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0965 - val_loss: 0.0880\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0961 - val_loss: 0.0929\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0954 - val_loss: 0.0858\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.0950 - val_loss: 0.0927\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0943 - val_loss: 0.0869\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0939 - val_loss: 0.0836\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0937 - val_loss: 0.0835\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0933 - val_loss: 0.0884\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0927 - val_loss: 0.0850\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0926 - val_loss: 0.0842\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0922 - val_loss: 0.0855\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0919 - val_loss: 0.0862\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 3s 80us/sample - loss: 0.0916 - val_loss: 0.0832\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0920 - val_loss: 0.0843\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0911 - val_loss: 0.0852\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 3s 82us/sample - loss: 0.0910 - val_loss: 0.0835\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0907 - val_loss: 0.0846\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0906 - val_loss: 0.0833\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0905 - val_loss: 0.0836\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0903 - val_loss: 0.0850\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0903 - val_loss: 0.0810\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0903 - val_loss: 0.0846\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0901 - val_loss: 0.0840\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0897 - val_loss: 0.0866\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0897 - val_loss: 0.0796\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0895 - val_loss: 0.0838\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 3s 79us/sample - loss: 0.0894 - val_loss: 0.0841\n",
      "Epoch 47/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0894 - val_loss: 0.0871\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0894 - val_loss: 0.0845\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0893 - val_loss: 0.0867\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0888 - val_loss: 0.0822\n",
      "Epoch 51/100\n",
      "33820/33820 [==============================] - 3s 86us/sample - loss: 0.0888 - val_loss: 0.0822\n",
      "Epoch 52/100\n",
      "33820/33820 [==============================] - 3s 89us/sample - loss: 0.0885 - val_loss: 0.0786\n",
      "Epoch 53/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.0885 - val_loss: 0.0822\n",
      "Epoch 54/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0885 - val_loss: 0.0901\n",
      "Epoch 55/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0882 - val_loss: 0.0807\n",
      "Epoch 56/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.0881 - val_loss: 0.0825\n",
      "Epoch 57/100\n",
      "33820/33820 [==============================] - 3s 85us/sample - loss: 0.0880 - val_loss: 0.0839\n",
      "Epoch 58/100\n",
      "33820/33820 [==============================] - 3s 84us/sample - loss: 0.0879 - val_loss: 0.0825\n",
      "Epoch 59/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0879 - val_loss: 0.0879\n",
      "Epoch 60/100\n",
      "33820/33820 [==============================] - 3s 87us/sample - loss: 0.0877 - val_loss: 0.0829\n",
      "Epoch 61/100\n",
      "33820/33820 [==============================] - 3s 88us/sample - loss: 0.0878 - val_loss: 0.0833\n",
      "Epoch 62/100\n",
      "33820/33820 [==============================] - 3s 83us/sample - loss: 0.0875 - val_loss: 0.0874\n"
     ]
    }
   ],
   "source": [
    "lag_vec = [7,14]\n",
    "units_vec = [2,4,8,16,32,64,128]\n",
    "results = cross_validation(lag_vec, units_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4410.45366408 4083.986412   3345.05999559 4381.99300655 4333.86394161\n",
      "   4101.7913781  3012.72578322]\n",
      "  [3806.4133163  3503.32342332 3984.76891184 3319.86057168 3175.48389696\n",
      "   2893.78738935 2862.34941859]]\n",
      "\n",
      " [[4607.3759112  4089.84809058 3716.40524351 3870.75824644 3228.01951038\n",
      "   3516.42466214 3179.46876634]\n",
      "  [3953.49546017 3859.4286121  3686.66388058 3688.91937532 4743.61813689\n",
      "   3825.83010805 3230.83420922]]\n",
      "\n",
      " [[5163.18153646 4190.65789754 4055.47670003 4168.79374471 4016.73343994\n",
      "   4147.61338481 4088.1472758 ]\n",
      "  [4430.52456502 4821.22080688 4378.41356262 4104.35447428 4074.01569702\n",
      "   4091.30554281 3943.41487935]]]\n",
      "14 128\n"
     ]
    }
   ],
   "source": [
    "MAE = results[2,:]\n",
    "print(MAE)\n",
    "MAE = numpy.sum(MAE, axis=0)\n",
    "ind = numpy.unravel_index(numpy.argmin(MAE, axis=None), MAE.shape)\n",
    "\n",
    "lag = lag_vec[ind[0]]\n",
    "units = units_vec[ind[1]]\n",
    "print(lag,units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34400 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "34400/34400 [==============================] - 9s 265us/sample - loss: 0.1317 - val_loss: 0.1496\n",
      "Epoch 2/100\n",
      "34400/34400 [==============================] - 3s 82us/sample - loss: 0.1173 - val_loss: 0.1546\n",
      "Epoch 3/100\n",
      "34400/34400 [==============================] - 3s 87us/sample - loss: 0.1149 - val_loss: 0.1587\n",
      "Epoch 4/100\n",
      "34400/34400 [==============================] - 3s 84us/sample - loss: 0.1136 - val_loss: 0.1594\n",
      "Epoch 5/100\n",
      "34400/34400 [==============================] - 3s 87us/sample - loss: 0.1130 - val_loss: 0.1578\n",
      "Epoch 6/100\n",
      "34400/34400 [==============================] - 3s 88us/sample - loss: 0.1129 - val_loss: 0.1615\n",
      "Epoch 7/100\n",
      "34400/34400 [==============================] - 3s 78us/sample - loss: 0.1111 - val_loss: 0.1626\n",
      "Epoch 8/100\n",
      "34400/34400 [==============================] - 3s 82us/sample - loss: 0.1101 - val_loss: 0.1635\n",
      "Epoch 9/100\n",
      "34400/34400 [==============================] - 3s 93us/sample - loss: 0.1092 - val_loss: 0.1666\n",
      "Epoch 10/100\n",
      "34400/34400 [==============================] - 3s 88us/sample - loss: 0.1077 - val_loss: 0.1695\n",
      "Epoch 11/100\n",
      "34400/34400 [==============================] - 3s 86us/sample - loss: 0.1067 - val_loss: 0.1722\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 11062.4033203125\n",
      "MAPE: 0.2687646795760825\n",
      "MAE: 7509.172182932208\n",
      "R2 score: 0.3588373179027742\n",
      " \n",
      " \n",
      "---------------------------------------------------\n",
      "Train on 35020 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "35020/35020 [==============================] - 11s 314us/sample - loss: 0.1332 - val_loss: 0.1263\n",
      "Epoch 2/100\n",
      "35020/35020 [==============================] - 3s 86us/sample - loss: 0.1185 - val_loss: 0.1343\n",
      "Epoch 3/100\n",
      "35020/35020 [==============================] - 3s 80us/sample - loss: 0.1164 - val_loss: 0.1358\n",
      "Epoch 4/100\n",
      "35020/35020 [==============================] - 3s 81us/sample - loss: 0.1146 - val_loss: 0.1326\n",
      "Epoch 5/100\n",
      "35020/35020 [==============================] - 3s 82us/sample - loss: 0.1136 - val_loss: 0.1388\n",
      "Epoch 6/100\n",
      "35020/35020 [==============================] - 3s 81us/sample - loss: 0.1127 - val_loss: 0.1470\n",
      "Epoch 7/100\n",
      "35020/35020 [==============================] - 3s 83us/sample - loss: 0.1120 - val_loss: 0.1374\n",
      "Epoch 8/100\n",
      "35020/35020 [==============================] - 3s 84us/sample - loss: 0.1107 - val_loss: 0.1465\n",
      "Epoch 9/100\n",
      "35020/35020 [==============================] - 3s 86us/sample - loss: 0.1096 - val_loss: 0.1439\n",
      "Epoch 10/100\n",
      "35020/35020 [==============================] - 3s 86us/sample - loss: 0.1083 - val_loss: 0.1434\n",
      "Epoch 11/100\n",
      "35020/35020 [==============================] - 3s 86us/sample - loss: 0.1073 - val_loss: 0.1495\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3UElEQVR4nO3deXzU1dX48c/JvieQBAIESNhlRwFRcMEVpC7Vqqi4t1bbutRHn2o32/662KfWWqutdcENxX1BRcUNFXFhkR1kDRAgJIQtAUK28/vjTkKALEOY78wkOe/Xa16zfGe+c4Zlztx7z71XVBVjjDHmUBGhDsAYY0x4sgRhjDGmXpYgjDHG1MsShDHGmHpZgjDGGFOvqFAHEEgZGRmak5MT6jCMMabFmDdv3jZVzazvWKtKEDk5OcydOzfUYRhjTIshIusbOmZdTMYYY+plCcIYY0y9LEEYY4ypl6djECIyDvgnEAk8rqr3HnK8H/AkcCzwK1W9r86xPKAEqAIqVXV4c2KoqKggPz+fsrKy5n2IFiIuLo7s7Gyio6NDHYoxppXwLEGISCTwMHAmkA/MEZFpqrqsztO2A7cAFzRwmrGquu1o4sjPzyc5OZmcnBxE5GhOFbZUleLiYvLz88nNzQ11OMaYVsLLLqaRwGpVXauq5cALwPl1n6Cqhao6B6jwKoiysjLS09NbbXIAEBHS09NbfSvJGBNcXiaILsDGOvfzfY/5S4EZIjJPRG5o6EkicoOIzBWRuUVFRQ095wjetmVqC5/RGBNcXiaI+r6xjmRt8dGqeiwwHvipiJxc35NU9VFVHa6qwzMz653rYYwxrdfGOfDFg56c2ssEkQ90rXM/G9js74tVdbPvuhB4Hddl1eLs3LmTf//730f8unPOOYedO3cGPiBjTOuxdiY8cz7MnQz7SwJ+ei8TxBygt4jkikgMMBGY5s8LRSRRRJJrbgNnAUs8i9RDDSWIqqqqRl83ffp00tLSPIrKGNPirXgHnrsY2nWH696D2OSAv4VnVUyqWikiPwPex5W5TlbVpSJyo+/4IyKSBcwFUoBqEbkN6A9kAK/7+tWjgOdV9T2vYvXSXXfdxZo1axg6dCjR0dEkJSXRqVMnFixYwLJly7jgggvYuHEjZWVl3HrrrdxwgxtuqVk2pLS0lPHjxzNmzBhmz55Nly5dePPNN4mPjw/xJzPGhMzCF+GNm6DzULjiFUho78nbeDoPQlWnA9MPeeyROrcLcF1Ph9oNDAl0PL9/aynLNu8O6Dn7d07hnnMHNHj83nvvZcmSJSxYsICZM2cyYcIElixZUluOOnnyZNq3b8++ffsYMWIEF110Eenp6QedY9WqVUydOpXHHnuMSy65hFdffZVJkyYF9HMYY1qIbx6D6XdAzklw2VRPWg41WtVifS3ByJEjD5qr8OCDD/L6668DsHHjRlatWnVYgsjNzWXo0KEAHHfcceTl5QUrXGNMOPn8fvjo99BnPFz8FETHefp2bSpBNPZLP1gSExNrb8+cOZMPP/yQL7/8koSEBE499dR65zLExsbW3o6MjGTfvn1BidUYEyZU4cPfwRcPwKCL4YL/QKT3qya0qQQRCsnJyZSU1F9dsGvXLtq1a0dCQgIrVqzgq6++CnJ0xpiwV10N0//HVSoNvw7O+TtEBGcZPUsQHktPT2f06NEMHDiQ+Ph4OnbsWHts3LhxPPLIIwwePJi+ffsyatSoEEZqjAk7VRVuMHrxyzD6NjjjdxDESbGieiRz18Lb8OHD9dANg5YvX84xxxwTooiCqy19VmNavYoyeOVa+G46nH4PnHS7J28jIvMaWgzVWhDGGBNu9pfA1Msg73M45z4Y+aOQhGEJwhhjwsne7W4C3OZv4fuPwpBLQxaKJQhjvFRZDi9OgrhUuPDRoPYfmxaoZCs8+30oXgWXPgv9JoQ0HEsQxnhpxq9g1fvudo9TYJhNcDQN2LHeratUWghXvAw9Tg11RLblqDGeWfA8fPMojPoJdB8N790Nu/JDHVVwrJwBaz52JZqmaUUr4cnxsG87XPVGWCQHsARhjDc2fwtv3eaWQzjz/8H5D0N1Fbz5MzfpqTXLmwXPX+K6Sh4cCp//3XWdmPptWeiSQ1UFXDMduobPwtWWIDzW3OW+AR544AH27t0b4IiM5/ZsgxevhMRMtxxCZBS0z4Wz/gBrP4F5T4Y6Qu/s2wGv3QDte8D3/wtp3eCjP8A/+ruxmNUfWquirvVfwlPfg+h4tyJr1sBQR3QQSxAeswTRxlRVutr10kKYOAUSMw4cG3696zp4/9ewIy9UEXpH1bWaSrfCRY/BkIlwzdvws7kw6iZYPxumXAQPDoHP/ga7t4Q64tBa/aFrZSV1gGvfhfSeoY7oMJYgPFZ3ue8777yTv/3tb4wYMYLBgwdzzz33ALBnzx4mTJjAkCFDGDhwIC+++CIPPvggmzdvZuzYsYwdOzbEn8L47aPfwbrP4Hv/gM7DDj4mAuc9BBLhuppa2y/pBc/Dsjdg7C+hy3EHHs/oDWf9EW5fDj+YDO1y4OM/wj8GwAtXwKoPXPdbW7LsTXh+ImT0gmvfg7SuTb8mBNpWFdO7d0HB4sCeM2sQjL+3wcN1l/ueMWMGr7zyCt988w2qynnnncdnn31GUVERnTt35p133gHcGk2pqancf//9fPLJJ2RkZDR4fhNGlrwKs/8FI34Iw66o/zlpXWHcn2HazTDnMTj+x8GN0SvFa2D6ndB9jFsSoj5RsTDwIncpXgPzn4Zvn4MVb0NqVzj2KlflldI5qKEH3bfPwbSfQfYIuPwliE8LdUQNshZEEM2YMYMZM2YwbNgwjj32WFasWMGqVasYNGgQH374Ib/4xS/4/PPPSU1NDXWo5kgVLHGtgq6j4Oy/NP7cYVdCrzPhg3vcF2VLV1UBr/7QrS564X8hIrLp16T3hDP/4FoVFz/l7n/yJ9eqmHoZrHy/dbYqvnoE3vwJ5J4CV74e1skB2loLopFf+sGgqtx99938+MeH/2qcN28e06dP5+677+ass87it7/9bQgiNM2ybwe8eAXEpsAlT0NUTOPPF4HzHoSHR8EbP4Frp/v3pRquZv4FNs+Hi5+G1Pr2/2pEVAwM+L67bF8L85+Bb6e49YdSsuHYK12r4kjPG25U3bjLJ3+Cft9zXW1RsU2/LsSsBeGxust9n3322UyePJnS0lIANm3aRGFhIZs3byYhIYFJkyZxxx13MH/+/MNea8JUdRW8+iPYtcnNfE3O8u91KZ1h/F9h41fw1X+8jdFLebPcJjbDJsGAC47uXO17uNVKf74MLnkGMvu45PPAIHj+UvjuXVcE0NKowoxfu+Qw5HKXSFtAcoC21oIIgbrLfY8fP57LL7+cE044AYCkpCSmTJnC6tWrufPOO4mIiCA6Opr//Md9Ydxwww2MHz+eTp068cknn4TyY5iGzPwLrP4AJtx/5PXrQybC8mmuDLT3We4LsSXZtwNe+7Er4R3318CdNyoG+p/vLtvXwbfPulbFyvcgubOvVXFl2A7sHqS6Ct6+zbWMRv4Yxt0btL0cAsGW+25F2tJnDQvL33ZdS8Mm+aqTmrHOUslW+Pfx7tfzdTPcnImWQNWV8y5/C66fcXDVkheqKlyCmPcUrP7IPdb7TDjuGuh9dnj+uVWWw+s3wNLX4eQ7YeyvwnItrsaW+245qcyYcFK0El6/ETof63b4au5//OSObjnnTfNg9oOBjdFLC553X3yHlrR6JTIajjkXJr0Kty6Ek++ALYvghcvhgYGubHbHeu/j8Ff5Xhfb0tfdTPrTfh2WyaEpYZh2jQlzZbvdf/6oWDfucLQbxw+8yHU1zfwL9BkHHfsHJk6v+FPS6qV23d0X7il3uYUQ5z0Fn93nLr1Od3s2p3SBhHQ3UTG+fXBbGGW7YepENzHw3H+6Vk4L1SYShKoiLTB7H4nW1FUY1qqr3RaQ29fC1dMCU10j4sYw8r6AN26EH34UlA3pm6W2pDXK/5JWr0RGueWw+02AnRvcOMX8Z2F1PXNL4tJcskhIh4QMSEw/cLsmkSSkH7gdndC8X/x7imHKhbB1CVz0OAz6wVF/zFBq9QkiLi6O4uJi0tPTW22SUFWKi4uJizvKX7KmabP+7iZ2nf0XyBkTuPMmZrjZ1y9d6aqCTv1F4M4dSDPv9ZW0PhVepadp3Vx318n/C9u+c+th7d3mNt+pvV3sbu/Ig01z3f3qBqqiouIaSCT1JZUMiG8HpQVu6YwdeTDxeehzdjD/BDzR6hNEdnY2+fn5FBUVhToUT8XFxZGdHUb/YVujVR/Ax39yXRijbgr8+fuf58792f9B33HQaUjg3+No5H3hVmYdOsnNWwhHkVHQcYB/z1WFsl0uUdQkj7qJZO92d3/PNtettrcYykvrP5dEQES0a/lNejWwPx5CqNVXMRkTEMVr4LGxkNrNVe3EJHjzPnu3w79HuV+lN8xsetJdsOzbAf8Z4+L58ecQmxTqiEKjosyXUGoSSfGB+2W73RIr4ZbYm9BYFVOrb0EYc9TK97jluyXCrdDqVXIASGjvBjanToRP/wqn/8a79/KXKrz9c9eFcv2MtpscwBUkpHZxlzbAylyNaYyqW2OpaDlc9IRbidRrfce7Gbez/uHKX0Mt2CWtJmxYgjCmMV8+BEtfg9N+40oog2XcXyCpI7x+k+vWCJVQl7SakLIEYUxD1s6ED34Lx5wHY34e3PeOT4Pz/uUqcj75U3Dfu0ZVBbz2o/AoaTUhYQnCmPrs3AAvXwsZfeCCf4dmFmzvM+DYq90eExu+Dv77z7zXdXGd+8/wKmk1QWMJwphDVexz+ydXV8Klz0FscuhiOftPbjOdN25yyzcES0soaTWeswRhTF2q8PbtsGUhXPiY2xIylGKT4fyHYPsat+prMOzbAa/d4FZpHR/AVVpNi2MJwpi65jwOC5936/z0HRfqaJwep8CIH8HX/3H7L3ipbknrhY+37ZJWYwnCmFrrZ8N7d7kF804Js6UuzvidK7F94yewv4HZvIGwcKoraT31bsi2kta2zhKEMQC7N8NLV0Nad/j+f8NvU5fYJLjgP27w/MN7vHmPuiWtwa7aMmEpzP4XGBMClfvhpavcjOmJz4XvRvLdT4RRP3HdYGtnBvbcNSWtEZFW0mpqeZogRGSciHwnIqtF5K56jvcTkS9FZL+I3FHP8UgR+VZE3vYyTtPGvfsLyJ/jylk7hPmOfKf/BtJ7udndZbsDd14raTX18CxBiEgk8DAwHugPXCYih+6Esh24BbivgdPcCiz3KkZjmPc0zHvSzRIecEGoo2ladDxc8Ajs3gQzfhWYc9aWtF5hJa3mIF62IEYCq1V1raqWAy8A59d9gqoWquocoOLQF4tINjABeNzDGE1blj8Xpt8BPcbC6b8NdTT+6zoCTrwZ5j/jliA/Gvt2upLWdjlW0moO42WC6AJsrHM/3/eYvx4A/heoDmBMxjilhW6F1uQs+MHkltfnfuovIbMfTLvZzVtojrolrRc9EdoJgSYseZkg6lubwK/NJ0Tke0Chqja5lKWI3CAic0VkbmvfFMgESFUFvHyN+2K99Dm3xHZLEx3nqppKC+G9u5t3joVT3UKEVtJqGuBlgsgHuta5nw1s9vO1o4HzRCQP1zV1mohMqe+Jqvqoqg5X1eGZmZlHE69pK2b8BtZ/Aec9CJ0Ghzqa5utyLJx0u/uiXzH9yF5bW9I62kpaTYO8TBBzgN4ikisiMcBEYJo/L1TVu1U1W1VzfK/7WFUneReqaTMWvuhmJB9/Ewy+JNTRHL2T/xc6DoS3bnW70fmjbknr962k1TTMswShqpXAz4D3cZVIL6nqUhG5UURuBBCRLBHJB24Hfi0i+SKS4lVMpo3bshDeusVNBDvr/4U6msCIinFdTfu2uwF3f9SUtH7vAUjr2uTTTdtle1KbtqFkKzxxBlRXub2ekzqEOqLA+vT/3L4RFz/deLlu3hfw1AQYermb92HavMb2pG60BeGbqGYdlKZl25UPT453G8xf8mzrSw7gxhE6DYV3bofSBoo19u2E139sJa3Gb40mCFWt4pC5C8a0KDvyfMmhCK58rfVW60RGw/cfgf0l8M7PXQlrXTUlrbs3W0mr8Zs/YxBfiMhDInKSiBxbc/E8MmOO1rbVMHm8W5Liqjeh26hQR+StDsfA2F/C8rdgyasHH6spaR1rJa3Gf1F+POdE33Xd3UoUOC3w4RgTIFuXwTPng1bDNW9D1qBQRxQcJ9wMy9+Gd/4Hcsa4iYAHlbTeHuoITQvSZIJQ1bHBCMSYgNmyEJ65ACJjXHLI7BvqiIInMsp1NT0yBt66DS591i2lYSWtphma7GISkVQRub9mtrKI/F1EUoMRnDFHLH8uPH0uxCTCtdPbVnKokdEbTvsNrHzX/VlsmmslraZZ/BmDmAyUAJf4LruBJ70MyphmyfvCdSvFt3PJIb1nqCMKnVE3QbcTYMOXbpXWgReGOiLTAvkzBtFTVS+qc//3IrLAo3iMaZ41n8DUy9yv5KvehJTOoY4otCIi4cLH3FLmtpSGaSZ/WhD7RGRMzR0RGQ3s8y4kY47Qyvfh+UuhfQ+45h1LDjXSurplzK2k1TSTPy2IG4Fn6ow77ACu9i4kY47AsmnwynXQcQBc+XrLXJnVmDDVaILw7Qo3SVWH1KyRpKoB3OfQmKOw6GU3M7jLcTDpFYiz2gljAqnRBKGqVSJynO+2JQYTPuY/6zbLyRkDl70AsUmhjsiYVsefLqZvRWQa8DKwp+ZBVX3Ns6iMacw3j7mVS3ueDpdOgZiEUEdkTKvkT4JoDxRz8MxpBSxBtDSqsH0trJ/tJpP1OBX6TQCpb/O/MDX7XzDj19D3HLj4KYiKDXVExrRa/oxBbFPVO4MUjwmk6mooXOYSwobZ7rp0qzsWEQ1zHoOswW7Lyb7jwztRqMJnf3NLWve/AC563C1QZ4zxjD9jELYwX0tRWe5aBuu/cBOkNnwJZbvcsZQukHsydD8Rup3oJpEtftntI/DCZW6p6FPvhj5nh1+iUIWP/gCz7ochl8F5D7klJYwxnvLnf9kCG4MIU+V7IX/OgRbCxjlQ6Zuikt4b+p/vFmjrdgKkdTv8i3/o5TDoYlj0oksUUy+Fzse6RNH7zPBIFKrw/i/hq3/DcdfAhH9AhJc75RpjajS5o5yI1Leshqrqdd6E1Hytfke5fTtgw9cHWgibv4XqSkDcaqXdT/S1EE448k1xqircktCf/Q12bnClo6feDb3OCF2iqK52G+DMexKOvxHG3RseScuYVqSxHeVsy9FwVlLgWgfrZ7uEsHUpoG78oMtx0P0E10LoOjJwcwAqy2Hh8/DZfbBrI2SPgFPvchVDwfxyrq6CN3/mYhnzczj9HksOxnjgqBKEiPQB/gN0VNWBIjIYOE9V/xj4UI9Oi04QqrBjHaz/8kCX0fa17lh0oksCNS2ELsdBdLy38VSWw4Ln4PO/+xLFSLfZTI+x3n9RV1W4JaqXvgZjfwUn32nJwRiPHG2C+BS4E/ivqg7zPbZEVQcGPNKj1OISRNkuWPTSgRZCyRb3eHw7N5Dc/QSXELIGh65ip3I/fDvFJYrdm6DrKJcock/x5ku7cj+8fC189w6c+QcYfWvg38MYU6uxBOHPIHWCqn4jB38ZVAYksrascj9M+QHkfwPJnV1XUU0LIaNv+AzERsXCiOth2CSY/wx8fr9bUrvbib5EcXLg3qtiH7w4CVZ/COP/BsffELhzG2OOmD8JYpuI9MRNjkNEfgBs8TSq1k7VbQmZ/43bQH7gReHfhRIVCyN/BMOudIli1v1uM5ruY1yiyBnT9Dkas78Upk6EvFlw3r/g2KsCE7cxptn8+Zn6U+C/QD8R2QTchlvh1TTXnMfh22dd3/qgH4R/cqgrOs79sr9lAYz7KxSvhqcmwFPfcxv2NEfZLphyketqu/BRSw7GhAm/q5hEJBGIUNUSb0NqvhYxBpE3y3XR9DoTJj4fPl1JzVWxD+Y9BbP+4WZp554Mp/7SjZ/4Y+92mHIhFCyGH0x2czeMMUHT2BiE399OqronnJNDc1VXKx8s28ryLUFYrHbnBnjpKrexzYWPtvzkAK6aatRNrkVx9p+hcAU8Oc4lwQ1fN/7a0iLXTbV1KVz6nCUHY8JMK/iGOjr7Kqq4/aUFPPTJam/fqHwvvHA5VFXCxKkQl+Lt+wVbTAKc8FO4dSGc9UcoWAKTz4Jnv+9meB9q9xbXNVW8Bi5/EfqOC37MxphGtfkEkRgbxWUju/HekgI27fRoJ1VVePOn7kvzoscho5c37xMOYhLgxJvhtkWuTHXLQnjiDDfGkD/PPWfnRnhyvCubnfQq9Dyt8XMaY0KiyQQhIgki8hsRecx3v7eIfM/70ILn6hNzAHhmdp43b/DFA27S1xn3QJ+zvHmPcBOT6OYw3LoIzvg9bJoPj58Gz10MT57jxh6ufANyRoc6UmNMA/xpQTwJ7AdqRh3zgbCbRX00uqTFM25gFs9/s4E9+wM8xWPVB/Dh710p6+jbAnvuliA2CcbcBrctdstl5M+B8lK4ehp0HRHq6IwxjfAnQfRU1f8DKgBUdR/Qguoy/XPd6FxKyip5dX5+4E66bTW8cj1kDXRLVLekctZAi02Ck26Hny+FW+ZD56GhjsgY0wR/EkS5iMRzYKJcT1yLolU5rns7hnZN48kv8qiuDsAChmW73T4LkVGunNW2xXRiEt1SIsaYsOdPgvgd8B7QVUSeAz4CfuFlUKFy/Zhc1m3bw8crCo/uRNXVbrG57WvhkmfcXgzGGNPCNLnUhqrOEJF5wChc19KtqrrN88hCYNzALDqlxjH5i3Wc0b9j808088+w8l04576jX4LCGGNCxJ8qpo9UtVhV31HVt1V1m4h8FIzggi06MoKrT8xh9ppilm1u5sS5pW+4TXeGXQkjfhjQ+IwxJpgaTBAiEici7YEMEWknIu19lxygc9AiDLLLRnQjPjqSyV+sO/IXFyyBN25yeydM+HvbHpQ2xrR4jbUgfgzMA/oB83235wFvAg97H1popCZE84Pjspm2YDNFJUcwFr93u5spHZcKlz7rVj81xpgWrMEEoar/VNVc4A5Vza1zGaKqDwUxxqC7dnQO5VXVTPlqvX8vqKqEl692W4Re+hwkZ3kboDHGBIE/VUy7ROSqQy/+nFxExonIdyKyWkTuqud4PxH5UkT2i8gddR6PE5FvRGShiCwVkd8fwWc6aj0ykzi9XwemfLWesoqqpl/wwW9g3Wdw7gOQfZzn8RljTDD4kyBG1LmchCt7Pa+pF4lIJK4rajzQH7hMRPof8rTtwC3AfYc8vh84TVWHAEOBcSIyyo9YA+a6MbkU7yln2sLNjT9xwfPw1b/h+Jtg6OXBCc4YY4LAnzLXm+veF5FU4Fk/zj0SWK2qa32vewE4H1hW59yFQKGITDjkPRUo9d2N9l0CMHvNfyf2TKdfVjKTZ63j4uOykfoGnPPnwVu3uT0QzmpVq48YY0yzVnPdC/T243ldgI117uf7HvOLiESKyAKgEPhAVZvYXCCwRITrxuSyoqCE2WuKD39CSQG8eIUbb7j4aTdj2hhjWhF/5kG8JSLTfJe3ge9wlUxNvrSex/xuBahqlaoOBbKBkSIysIH4bhCRuSIyt6ioyN/T++W8IZ3JSIph8qxDSl4r98OLV7qtMic+DwntA/q+xhgTDvz52Vt3fKASWK+q/qxolw90rXM/G2iiQ/9wqrpTRGYC44Al9Rx/FHgU3JajR3r+xsRFR3LF8d3550erWFtUSo/MJLe3wzv/A/nfuJZDVr15yxhjWrwmWxCq+mmdyxd+JgeAOUBvEckVkRhgIjDNnxeKSKaIpPluxwNnACv8fN+AmjSqOzGRETz5RZ57YM7j8O2zcNIdMOCCUIRkjDFB0WALQkRKqL9LSHDjyI3umamqlSLyM+B9IBKYrKpLReRG3/FHRCQLmAukANUichuu4qkT8LSvEioCeElV3z7iTxcAmcmxnDe0M6/My+cX/baR9N5d0GccjP1VKMIxxpigaTBBqGry0Z5cVacD0w957JE6twtwXU+HWgQMO9r3D5TrRufy5bxviXz1HmjfAy58FCLa/G6txphWzq/SGxEZgpsDAfCZqi7yLqTw0z8jiueSH6SyopyKS54jOi411CEZY4zn/KliuhV4DujguzwnIjc3/qpWRBXe/CndK9Zyc/lPeW9LUqgjMsaYoPCnn+R64HhV/a2q/ha3L8SPvA0rjHzxACx9DT3tt6xvP4YnDi15NcaYVsqfBCFA3QWJqmiFe1LXa9UH8OHvYcCFRJz0c64dncOCjTuZt35HqCMzxhjP+ZMgngS+FpHf+RbN+wp4wtuwwsC21fDK9W6ew/kPgwgXHZtNSlxU8/aKMMaYFsafeRD3A9fiFtbbDlyrqg94HFdole2GFy5zy2dMfB5iEgBIjI3ispHdeG9JAZt27gtxkMYY4y1/Bql7AktV9UFgIXBSzSS2Vqm6Gl67AYrXuJnSad0OOnzViTkAPD07L/ixGWNMEPnTxfQqUCUivYDHgVzgeU+jCqWZf4aV78K4eyH3pMMOd0mLZ9zALKZ+s4E9+ytDEKAxxgSHPwmiWlUrgQuBf6rqz3EznVufZW/CZ3+DYVfCyIYLta4fk0tJWSWvzPN31RFjjGl5/EkQFSJyGXAVULPcRbR3IYVIwRJ4/SbIHgkT/g717f/gc2y3dgzrlsaTX6yjujqo21QYY0zQ+JMgrgVOAP6kqutEJBeY4m1YQbZ3O7xwOcSlwKXPQlRsky+5bnQuecV7+WhFYRACNMaY4POnimkZcAewVEQGAZtU9V7PIwuWqkp4+Woo2QKXTnEbAPlh/MAsOqfGHb5XhDHGtBL+VDFNANYADwIPAatFZLzXgQVNxR53fe4/IXu43y+Liozg6hNz+HJtMUs37/IoOGOMCR1/upj+DoxV1VNV9RRgLPAPb8MKorhUuPJNGHr5Eb904ohuxEdHMnlWXuDjMsaYEPMnQRSq6uo699fi9oluPZq5dHdqQjQXD8/mrYWbKSwpC3BQxhgTWg1+M4rIhSJyIW7sYbqIXCMiVwNv4XaLM8C1o3Mpr6pmylcbQh2KMcYEVGM/nc/1XeKArcApwKlAEdDO88haiNyMRE7v14HnvlpPWUVV0y8wxpgWorEd5a4NZiAt2fVjcrn88a+ZtmAzl4zoGupwjDEmIJrcUU5E4nB7QgzAtSYAUNXrPIyrRTmhZzr9spKZ/MU6Lh6ejTQyyc4YY1oKf0ZnnwWygLOBT3F7SJd4GVRLIyJcNyaXFQUlfLG6ONThGGNMQPiTIHqp6m+APar6NDABGORtWC3PeUM6k5EUY3tFGGNaDb/WYvJd7xSRgUAqkONZRC1UXHQkk0Z15+MVhawpKg11OMYYc9T8SRCPikg74NfANGAZ8FdPo2qhrji+OzGRETxprQhjTCvgz1pMj6vqDlX9TFV7qGoHVf1vMIJraTKTYzl/aGdenbeJnXvLQx2OMcYcleZNITYNum5MLvsqqpj6zcZQh2KMMUfFEkSAHdMphRN7pvP07DwqqqpDHY4xxjSbJQgPXD8ml4LdZby7pCDUoRhjTLM1OVEOQEROxFUu1T5fVZ/xKKYWb2zfDuRmJPLErHWcO7iTTZwzxrRI/uwH8SxwHzAGGOG7+L9xQhsUESFcOzqHhRt3Mn/DjlCHY4wxzeJPC2I40F9VbfPlI3DRsdnc9/53TJ6Vx3Hd24c6HGOMOWL+jEEswS21YY5AYmwUlx3fjXeXbCF/x95Qh2OMMUfMnwSRASwTkfdFZFrNxevAWoOrT8hBRHh6dl6oQzHGmCPmTxfT77wOorXqnBbP+IFZvDBnI7ee0YekWL9qAowxJiw0+Y2lqp8GI5DW6voxuby9aAuvzN3INaNzQx2OMcb4zZ8qplEiMkdESkWkXESqRGR3MIJrDYZ1a8ewbmk8OTuPqmob5zfGtBz+jEE8BFwGrALigR/6HjN+un5MLuuL9/LxisJQh2KMMX7zaya1qq4GIlW1SlWfxO1Nbfw0bkAWnVPjeGLW2lCHYowxfvMnQewVkRhggYj8n4j8HEj0OK5WJSoygqtPzOGrtdtZunlXqMMxxhi/+JMgrvQ972fAHqArcJE/JxeRcSLynYisFpG76jneT0S+FJH9InJHnce7isgnIrJcRJaKyK3+fZzwNXFkNxJiIpk8Ky/UoRhjjF/82Q9iPSBAJ1X9vare7utyapSIRAIPA+OB/sBlItL/kKdtB27BLeVRVyXwP6p6DDAK+Gk9r21RUuOjufi4bN5auJnCkrJQh2OMMU3yp4rpXGAB8J7v/lA/J8qNBFar6lpVLQdeAM6v+wRVLVTVORzY1rTm8S2qOt93uwRYDnTx4z3D2jWjc6mormbKl+tDHYoxxjTJny6m3+G+7HcCqOoC/NuTugtQd9ecfJrxJS8iOcAw4OsGjt8gInNFZG5RUdGRnj6ocjMSOb1fB6Z8vYGyiqpQh2OMMY3yJ0FUqmpzRlbrW+P6iCYCiEgS8Cpwm6rWO/dCVR9V1eGqOjwzM7MZYQbXdWNy2b6nnDcXbAp1KMYY0yi/FusTkcuBSBHpLSL/Amb78bp83IB2jWxgs7+BiUg0Ljk8p6qv+fu6cHdCj3T6ZSXzxKx12AK5xphw5k+CuBkYAOwHpgK7gdv8eN0coLeI5PrKZCcCfi3yJ26HnSeA5ap6vz+vaSlEhOvH5LJyaymzVm8LdTjGGNMgf6qY9qrqr1R1hK8r51eq2mQZjqpW4kpj38cNMr+kqktF5EYRuRFARLJEJB+4Hfi1iOSLSAowGldee5qILPBdzjmKzxlWzhvamYykGCbPWhfqUIwxpkENLtbXVKWSqp7X1MlVdTow/ZDHHqlzuwDX9XSoWdQ/htEqxEZFMmlUdx74cBWrC0vp1SEp1CEZY8xhGlvN9QRcFdJUXAVRq/3CDoVJo7rz75lrmPzFOv78/UGhDscYYw7TWILIAs7ELdR3OfAOMFVVlwYjsNYuIymWC4d14fmvNzBn3XbGD+rE+IFZ9MtKxg3BGGNMaIk/lTQiEotLFH8D/qCq//I6sOYYPny4zp07N9Rh+G1veSUvzdnIu0sK+CZvO6pursS4gVmMH5jFoC6pliyMMZ4SkXmqOrzeY40lCF9imIBLDjm4KqTJqhqWRfwtLUHUVVSynxnLCnhvSQGz1xRTVa108e1IN35QJ4Z1TSMiwpKFMSawmpUgRORpYCDwLvCCqi7xLsTAaMkJoq4de8r5YPlW3l28hVmrt1FRpXRMiWXcAJcsRuS0J9KShTEmAJqbIKpxq7fCwTOgBVBVTQlolAHQWhJEXbvLKvh4eSHTF2/h05VF7K+sJiMphjP7Z3HOoCxG9UgnOtKvbT2MMeYwze5iamlaY4Koa8/+SmZ+V8T0JVv4ZEUhe8urSEuI5sxjOjJ+UBaje2UQGxUZ6jCNMS2IJYhWqKyiik9XFvHekgI+XLaVkv2VJMdGcfoxHRg3sBOn9s0kLtqShTGmcY0liMbKXE0Yi4uO5OwBWZw9IIv9lVXMXl3Mu0u2MGPZVt5YsJmEmEjG9u3AuIFZnNavA4mx9ldtjDky1oJoZSqqqvl67XbeXbKF95cWsK20nNioCE7uk8k5g7I4/ZiOpMRFhzpMY0yYsC6mNqqqWpmbt513l7jy2YLdZURHCqN7ZTB+YBZj+3WgQ3JcqMM0xoSQJQhDdbWyIH8n7y7ewrtLCsjfsQ+AflnJnNQ7g5N6ZzIyt72NWxjTxliCMAdRVZZt2c1nK7fx+aoi5ubtoLyqmpioCI7Pbc+YXi5hHNPJlv0wprWzBGEatbe8km/WbefzVS5hrNxaCrj1ok7qneFLGBl0SLHuKGNaG6tiMo1KiIni1L4dOLVvBwAKdpUxa7VLFp+tLOL1b93KKjXdUWN6ZzIypz3xMdYdZUxrZi0I06jqamV5we7a1sWcdQe6o0bmtPcljAyOyUqxtaKMaYGsi8kEzL7yKr7J287nK4v4fNU2vttaAkBGUgxjernWxUm9M+ho3VHGtAjWxWQCJj4mklP6ZHJKn0wAtu4uY5avdTFr9TbeWLAZgL4dfdVRfaw7ypiWyloQJmBquqNcwtjGN3nbKa+sJiYyghG57TipdyZjemXQv5N1RxkTLqyLyYTEvvIq5uRt5/NVrjtqRYHrjkqKjaJnZiI9OyTRMzOJXr7r7ukJtjKtMUFmXUwmJOJjIjm5TyYn+7qjCneX8fmqbSzM38maolJmry7mtfkH9p6KihC6pycclDR6dUiiR2YiybY8iDFBZy0IE1IlZRWsLdrD6sJS1hS5y+rCUtYX76Wy+sC/zY4psQcljZ6Z7tIxJdYm8xlzFKwFYcJWclw0Q7qmMaRr2kGPV1RVs75470FJY03RHl6bv4nS/ZW1zzu0u6omgVh3lTFHzxKECUvRkRH06uC+7OtSVQpL9rOmsJTVRaW11411V/XskEQv33VOegKp8dHW6jDGD5YgTIsiInRMiaNjShwn9so46FhNd9WBFodrdXy8ovCg7qrkuCi6tU+ge3oCXdsn0M136d4+kU5pcdbyMMbHEoRpNRrrrtqwfS9rCkvZsH1v7WVFQQkfLiukvKq69rmREULntDhf0kg8kDx8ySQ13gbLTdthCcK0etGREbXjE4eqqla27i47kDiKDySQGUsLKN5TftDzU+Oj62l5uPudUuOIstaHaUUsQZg2zbUY4umcFs+oHumHHS8pq2Dj9n2+pLHHd72PZZt3M2NpARVVB7quoiKE7HbxBycPXzLJTksgJT7Kxj5Mi2IJwphGJMdF079zNP07pxx2rKpa2bJr32Etj43b9/LO4i3s3Ftx0PPjoiPISokjKzWOrJQ4OvquO6W6MZWs1Dgyk2KtFWLChiUIY5opMkLIbpdAdrsETux5+PFd+yrY6Esam3fuo2BXGQW7y9i6u4y563dQuHv/QeMfABECmcmxLoHUJBNfIqmbVBJj7b+u8Z79KzPGI6nx0aR2SWVgl9R6j6sq2/eUU7C77EDy8F1v2VVGXvEevlpbzO6yysNemxwXdVBrJKumFVInqbRPiLE1r8xRsQRhTIiICOlJsaQnxTKgc/1JBNyOf3VbH1t2HUgkBbv3s3JrEUUl+6k+ZFGE6Eg3vjKgcwqDuqQxJDuVgdmppNiyJcZPliCMCXMJMVH0yEyiRz1VWDUqq6rZVlrTGqnpztrPxu17WbxpF9MXF9Q+t0dGIoOzUxmU7ZJG/84pJMTYV4E5nP2rMKYViIqMqO1a4pB5IAA79pSzeNMuFuXvZFH+Lr5au712744IgT4dkxnUJZXBXV3S6JuVTGyU7eHR1tlifca0UYW7y1iU70sam3axKH8X233zPmIiI+jXySWNIdlpDO6aSq/MJKuwaoVsPwhjTJNUlfwd+1i8aRcL83eyOH8Xi/N3UeJbHDE+OtKNZ2T7kkZ2KjnpiTYQ3sJZgjDGNEt1tZJXvIdF+QeSxpLNuyircOW5yXFRDOqSWps0BnVJJbtdvE0IbEFsuW9jTLNEREjtAPkFw7oAbkB8dVEpizbuYtEmN6Yxeda62lnl7RNjGJydSv9OKfTNSqZPx2R6ZCbamEYL5GmCEJFxwD+BSOBxVb33kOP9gCeBY4Ffqep9dY5NBr4HFKrqQC/jNMb4Lyoygn5ZKfTLSuGSEV0B2F9ZxXcFJSzM38Vi30D4rFXbalfRjYwQcjMS6dvRJYy+WUn06ZhM9/REIq2LKmx5liBEJBJ4GDgTyAfmiMg0VV1W52nbgVuAC+o5xVPAQ8AzXsVojAmM2KhIBmenMTg7DegOQHllNXnFe/iuoISVW0v4rqCEpZt3MX3JFmp6tmOiIuiVmVTb0qhJHF3SrJsqHHjZghgJrFbVtQAi8gJwPlCbIFS1ECgUkQmHvlhVPxORHA/jM8Z4KCYqgj6+FkNd+8qrWF1YyndbDySOr9YW8/q3BzZ8SoqNonfHpDotDnedkRRjiSOIvEwQXYCNde7nA8cH+k1E5AbgBoBu3boF+vTGmACLj4lkULYb2K5r174KVm0tcYmjwF2/v7SAF+Yc+BppnxhD7w51WxzJ9OmQTGqCzQ73gpcJor40H/CSKVV9FHgUXBVToM9vjAmO1Phohue0Z3hO+9rHVJVtpeW1LY2VvgRy6N7kWSlx9MlKpm/HJPpmpTCoSyo9MxNt3sZR8jJB5ANd69zPBjZ7+H7GmFZGRMhMjiUzOZbRdbaYVVU27yqrbWnUXD+9tpjySleCWzNvY2CXVAZnu0tuRpINih8BLxPEHKC3iOQCm4CJwOUevp8xpo0QEbqkxdMlLZ6x/TrUPl5ZVc26bXtYvGmXu+Tv4sU5G3lqdh4ACTGRDOzsurdq5m/k2mS/Bnk6UU5EzgEewJW5TlbVP4nIjQCq+oiIZAFzgRSgGigF+qvqbhGZCpwKZABbgXtU9YnG3s8myhljDlVVrawpKmWRrwR38aZdLN28m/2+lkZSbBQDOqfULmA4qEsq3dsntJmkYTOpjTGmjsqqalYVlta2MhZt2sXyLbtru6eS46IY2DnVlzRca6Nb+4RWWUFlCcIYY5pQUVXNyq0lbg0qXxfVii0ltbv+pcZHM8i3AdRgX9JoDcuK2FIbxhjThOjICAZ0TmVA51Qm+h4rr3RJY1H+LhZvct1Tj3++tnaGeLuE6IMSRq8OyXRrn0BMVOuonrIEYYwxDYiJimBg7baxbp5VWYVbVqRu99Qjn66lypc0IgSy2yWQk5FIbrrv2nfpkhbfokpvLUEYY8wRiIuOZEjXNIbU2ZiprKKKFQUlrCksJa94D+u27SGveA/z1+84aL5GdKTQtd2BpOGSSCI5GQl0To0Pu4FxSxDGGHOU4qIjGdo1jaGH7OanqhSV7idv217ytu1hXfEed71tD7PXbKtdNh1cayUnPYGc9APJIyc9kR6ZiXRIjg3JWIclCGOM8YiI0CE5jg7JcYzMbX/QsepqZWtJmWttbNtLXvEe1hbtYe22Pcz8rqh2cBzc/I3u6YnkZrgEkpORSA9fEklP9G59KksQxhgTAhERQqfUeDqlxnNiz4OPVVUrm3fuI8/X4li7zV0v31LCjKVbawfJAZJjo+iblczLN54Q8ERhCcIYY8JMZITQtX0CXdsncFLvzIOOVVRVs2nHPtZtOzDWUV5Z7UkrwhKEMca0INGREW58IiORsR6/V8uptzLGGBNUliCMMcbUyxKEMcaYelmCMMYYUy9LEMYYY+plCcIYY0y9LEEYY4yplyUIY4wx9WpVGwaJSBGwvpkvzwC2BTCclsA+c+vX1j4v2Gc+Ut1VNbO+A60qQRwNEZnb0K5KrZV95tavrX1esM8cSNbFZIwxpl6WIIwxxtTLEsQBj4Y6gBCwz9z6tbXPC/aZA8bGIIwxxtTLWhDGGGPqZQnCGGNMvdp8ghCRcSLynYisFpG7Qh2P10Skq4h8IiLLRWSpiNwa6piCRUQiReRbEXk71LEEg4ikicgrIrLC9/d9Qqhj8pqI/Nz373qJiEwVkbhQxxRoIjJZRApFZEmdx9qLyAcissp33S4Q79WmE4SIRAIPA+OB/sBlItI/tFF5rhL4H1U9BhgF/LQNfOYatwLLQx1EEP0TeE9V+wFDaOWfXUS6ALcAw1V1IBAJTAxtVJ54Chh3yGN3AR+pam/gI9/9o9amEwQwElitqmtVtRx4ATg/xDF5SlW3qOp83+0S3JdGl9BG5T0RyQYmAI+HOpZgEJEU4GTgCQBVLVfVnSENKjiigHgRiQISgM0hjifgVPUzYPshD58PPO27/TRwQSDeq60niC7Axjr382kDX5Y1RCQHGAZ8HeJQguEB4H+B6hDHESw9gCLgSV+32uMikhjqoLykqpuA+4ANwBZgl6rOCG1UQdNRVbeA+xEIdAjESdt6gpB6HmsTdb8ikgS8CtymqrtDHY+XROR7QKGqzgt1LEEUBRwL/EdVhwF7CFC3Q7jy9bufD+QCnYFEEZkU2qhatraeIPKBrnXuZ9MKm6SHEpFoXHJ4TlVfC3U8QTAaOE9E8nDdiKeJyJTQhuS5fCBfVWtah6/gEkZrdgawTlWLVLUCeA04McQxBctWEekE4LsuDMRJ23qCmAP0FpFcEYnBDWhNC3FMnhIRwfVLL1fV+0MdTzCo6t2qmq2qObi/449VtVX/slTVAmCjiPT1PXQ6sCyEIQXDBmCUiCT4/p2fTisfmK9jGnC17/bVwJuBOGlUIE7SUqlqpYj8DHgfV/EwWVWXhjgsr40GrgQWi8gC32O/VNXpoQvJeORm4Dnfj5+1wLUhjsdTqvq1iLwCzMdV631LK1x2Q0SmAqcCGSKSD9wD3Au8JCLX4xLlxQF5L1tqwxhjTH3aeheTMcaYBliCMMYYUy9LEMYYY+plCcIYY0y9LEEYY4yplyUIY5ogIlUisqDOJWAzkkUkp+6qnMaEkzY9D8IYP+1T1aGhDsKYYLMWhDHNJCJ5IvJXEfnGd+nle7y7iHwkIot81918j3cUkddFZKHvUrMMRKSIPObbx2CGiMT7nn+LiCzzneeFEH1M04ZZgjCmafGHdDFdWufYblUdCTyEWzEW3+1nVHUw8BzwoO/xB4FPVXUIbl2kmln7vYGHVXUAsBO4yPf4XcAw33lu9OajGdMwm0ltTBNEpFRVk+p5PA84TVXX+hZALFDVdBHZBnRS1Qrf41tUNUNEioBsVd1f5xw5wAe+jV4QkV8A0ar6RxF5DygF3gDeUNVSjz+qMQexFoQxR0cbuN3Qc+qzv87tKg6MDU7A7Xh4HDDPtwmOMUFjCcKYo3Npnesvfbdnc2CryyuAWb7bHwE3Qe3+2CkNnVREIoCuqvoJbqOjNOCwVowxXrJfJMY0Lb7Oyrfg9nmuKXWNFZGvcT+2LvM9dgswWUTuxO3qVrOK6q3Ao74VN6twyWJLA+8ZCUwRkVTcxlb/aCNbhpowYmMQxjSTbwxiuKpuC3UsxnjBupiMMcbUy1oQxhhj6mUtCGOMMfWyBGGMMaZeliCMMcbUyxKEMcaYelmCMMYYU6//DxyFnR+o3tROAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 9758.0283203125\n",
      "MAPE: 0.695155713094864\n",
      "MAE: 6335.961394856771\n",
      "R2 score: -0.2642537894333561\n",
      " \n",
      " \n",
      "---------------------------------------------------\n",
      "Train on 35620 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "35620/35620 [==============================] - 9s 263us/sample - loss: 0.1325 - val_loss: 0.0793\n",
      "Epoch 2/100\n",
      "35620/35620 [==============================] - 3s 84us/sample - loss: 0.1184 - val_loss: 0.0784\n",
      "Epoch 3/100\n",
      "35620/35620 [==============================] - 3s 88us/sample - loss: 0.1160 - val_loss: 0.0768\n",
      "Epoch 4/100\n",
      "35620/35620 [==============================] - 3s 88us/sample - loss: 0.1159 - val_loss: 0.0784\n",
      "Epoch 5/100\n",
      "35620/35620 [==============================] - 3s 85us/sample - loss: 0.1143 - val_loss: 0.0776\n",
      "Epoch 6/100\n",
      "35620/35620 [==============================] - 3s 82us/sample - loss: 0.1131 - val_loss: 0.0753\n",
      "Epoch 7/100\n",
      "35620/35620 [==============================] - 3s 87us/sample - loss: 0.1120 - val_loss: 0.0790\n",
      "Epoch 8/100\n",
      "35620/35620 [==============================] - 3s 85us/sample - loss: 0.1114 - val_loss: 0.0754\n",
      "Epoch 9/100\n",
      "35620/35620 [==============================] - 3s 84us/sample - loss: 0.1095 - val_loss: 0.0765\n",
      "Epoch 10/100\n",
      "35620/35620 [==============================] - 3s 82us/sample - loss: 0.1083 - val_loss: 0.0745\n",
      "Epoch 11/100\n",
      "35620/35620 [==============================] - 3s 85us/sample - loss: 0.1074 - val_loss: 0.0743\n",
      "Epoch 12/100\n",
      "35620/35620 [==============================] - 3s 86us/sample - loss: 0.1054 - val_loss: 0.0803\n",
      "Epoch 13/100\n",
      "35620/35620 [==============================] - 3s 88us/sample - loss: 0.1042 - val_loss: 0.0779\n",
      "Epoch 14/100\n",
      "35620/35620 [==============================] - 3s 83us/sample - loss: 0.1032 - val_loss: 0.0818\n",
      "Epoch 15/100\n",
      "35620/35620 [==============================] - 3s 85us/sample - loss: 0.1019 - val_loss: 0.0783\n",
      "Epoch 16/100\n",
      "35620/35620 [==============================] - 3s 85us/sample - loss: 0.1007 - val_loss: 0.0794\n",
      "Epoch 17/100\n",
      "35620/35620 [==============================] - 3s 83us/sample - loss: 0.0997 - val_loss: 0.0831\n",
      "Epoch 18/100\n",
      "35620/35620 [==============================] - 3s 83us/sample - loss: 0.0988 - val_loss: 0.0799\n",
      "Epoch 19/100\n",
      "35620/35620 [==============================] - 3s 80us/sample - loss: 0.0981 - val_loss: 0.0782\n",
      "Epoch 20/100\n",
      "35620/35620 [==============================] - 3s 81us/sample - loss: 0.0970 - val_loss: 0.0792\n",
      "Epoch 21/100\n",
      "35620/35620 [==============================] - 3s 82us/sample - loss: 0.0964 - val_loss: 0.0812\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 5727.767578125\n",
      "MAPE: 0.1424945962938534\n",
      "MAE: 3727.668521610383\n",
      "R2 score: 0.4502723321775489\n",
      " \n",
      " \n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# lag=14\n",
    "ds=['october','november','december']\n",
    "\n",
    "data_LSTM_X1,data_LSTM_X2, data_LSTM_X3, data_LSTM_Y = sequence_data_build(data, lag)\n",
    "totday=int(data_LSTM_X1.shape[0]/20)\n",
    "m1 = [totday-61, totday-31, totday, ]\n",
    "m2 = [m1[0]-31, m1[1]-30, m1[2]-31, ]\n",
    "for i in range(len(m1)):\n",
    "    trainX1, trainY, testX1, testY= train_test_build(data_LSTM_X1, data_LSTM_Y, m1[i], m2[i])\n",
    "    trainX2, trainY, testX2, testY= train_test_build(data_LSTM_X2, data_LSTM_Y, m1[i], m2[i])\n",
    "    trainX3, trainY, testX3, testY= train_test_build(data_LSTM_X3, data_LSTM_Y, m1[i], m2[i])\n",
    "    testYcopy=testY\n",
    "    saving = True\n",
    "    EarlyStop = True\n",
    "    rmse, mape, mae, r2, history = model_build(trainX1, trainX2, trainX3, testX1, testX2, testX3, trainY, testY, units, saving, ds[i], EarlyStop)\n",
    "        \n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"Epochs\")\n",
    "    pyplot.ylabel(\"Mean absolute error\")\n",
    "    pyplot.savefig(\"SMP3_\" + ds[i] + \".png\")\n",
    "    pyplot.show()\n",
    "    \n",
    "    \n",
    "    print(\"Root mean square error: {0}\".format(rmse))\n",
    "    print(\"MAPE: {0}\".format(mape))\n",
    "    print(\"MAE: {0}\".format(mae))\n",
    "    print(\"R2 score: {0}\".format(r2))\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lag vector\n",
      "[7, 14]\n",
      "LSTM units\n",
      "[2, 4, 8, 16, 32, 64, 128]\n",
      "Cross validation results\n",
      "14 128\n",
      "----------------------------\n",
      "RMSE\n",
      "[[[5862.04199219 5413.99902344 4400.63134766 5880.10498047 5795.38720703\n",
      "   5551.18164062 4261.34912109]\n",
      "  [5210.70166016 4695.95849609 5230.31591797 4377.95556641 4273.95898438\n",
      "   4014.49633789 3999.69091797]]\n",
      "\n",
      " [[6083.08837891 5336.98535156 4942.62011719 5056.89501953 4426.07519531\n",
      "   4757.82617188 4351.19091797]\n",
      "  [5194.64111328 5087.05371094 4878.89453125 4903.99511719 6229.44677734\n",
      "   5000.66796875 4470.47753906]]\n",
      "\n",
      " [[7610.06396484 6330.91357422 6255.43310547 6380.15869141 6298.94042969\n",
      "   6476.23974609 6419.67871094]\n",
      "  [6679.31738281 7153.29882812 6611.67675781 6399.61523438 6439.21289062\n",
      "   6464.48974609 6391.67382812]]]\n",
      "----------------------------\n",
      "MAE\n",
      "[[[0.26687783 0.242578   0.16269596 0.27332142 0.22887758 0.17254396\n",
      "   0.13314255]\n",
      "  [0.19681708 0.17059713 0.20549043 0.14844073 0.1544768  0.12746374\n",
      "   0.11476927]]\n",
      "\n",
      " [[0.25658765 0.25287587 0.19729199 0.19585539 0.14272249 0.15213284\n",
      "   0.13089464]\n",
      "  [0.20902346 0.19291146 0.17669685 0.20297292 0.27871841 0.20905929\n",
      "   0.14849901]]\n",
      "\n",
      " [[0.23050977 0.16865909 0.15834718 0.1630303  0.1552582  0.1647464\n",
      "   0.16164752]\n",
      "  [0.17048344 0.19545773 0.16271479 0.15386711 0.15388348 0.17110921\n",
      "   0.14541935]]]\n",
      "----------------------------\n",
      "MAPE\n",
      "[[[4410.45366408 4083.986412   3345.05999559 4381.99300655 4333.86394161\n",
      "   4101.7913781  3012.72578322]\n",
      "  [3806.4133163  3503.32342332 3984.76891184 3319.86057168 3175.48389696\n",
      "   2893.78738935 2862.34941859]]\n",
      "\n",
      " [[4607.3759112  4089.84809058 3716.40524351 3870.75824644 3228.01951038\n",
      "   3516.42466214 3179.46876634]\n",
      "  [3953.49546017 3859.4286121  3686.66388058 3688.91937532 4743.61813689\n",
      "   3825.83010805 3230.83420922]]\n",
      "\n",
      " [[5163.18153646 4190.65789754 4055.47670003 4168.79374471 4016.73343994\n",
      "   4147.61338481 4088.1472758 ]\n",
      "  [4430.52456502 4821.22080688 4378.41356262 4104.35447428 4074.01569702\n",
      "   4091.30554281 3943.41487935]]]\n",
      "----------------------------\n",
      "R2-score\n",
      "[[[0.74737319 0.78451457 0.85763225 0.74581394 0.75308555 0.77345605\n",
      "   0.86650168]\n",
      "  [0.80039381 0.83788243 0.79888827 0.85909569 0.86571043 0.88152036\n",
      "   0.88239264]]\n",
      "\n",
      " [[0.72833075 0.79088543 0.82064778 0.81225856 0.85617651 0.83380828\n",
      "   0.86100199]\n",
      "  [0.8018914  0.81001256 0.82524276 0.82344002 0.71510083 0.81641031\n",
      "   0.85327634]]\n",
      "\n",
      " [[0.65665754 0.76237962 0.7680119  0.75866858 0.76477369 0.75134524\n",
      "   0.75566962]\n",
      "  [0.73550645 0.69663627 0.7408363  0.75719439 0.75418046 0.75224672\n",
      "   0.75779664]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"lag vector\")\n",
    "print(lag_vec)\n",
    "print(\"LSTM units\")\n",
    "print(units_vec)\n",
    "print(\"Cross validation results\")\n",
    "print(lag,units)\n",
    "print(\"----------------------------\")\n",
    "print(\"RMSE\")\n",
    "print(results[0,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"MAE\")\n",
    "print(results[1,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"MAPE\")\n",
    "print(results[2,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"R2-score\")\n",
    "print(results[3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
