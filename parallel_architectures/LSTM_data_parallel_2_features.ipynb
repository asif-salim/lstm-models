{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Concatenate, Input\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,  mean_absolute_error, r2_score\n",
    " \n",
    "# tensorflow.reset_default_graph()\n",
    "tensorflow.random.set_seed(0)\n",
    "# random.seed(0)\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=14, day_offset=5):\n",
    "    dataY= []\n",
    "    dataX1=numpy.zeros([(len(dataset)-look_back),2,look_back])\n",
    "    dataX2=numpy.zeros([(len(dataset)-look_back),2,look_back])\n",
    "#     dataX3=numpy.zeros([(len(dataset)-look_back),2,look_back])\n",
    "    \n",
    "    #print(dataX.shape)\n",
    "    for i in range(look_back,len(dataset)):\n",
    "       # print(i)\n",
    "        a = numpy.zeros([4,look_back])\n",
    "        t1=dataset[(i-look_back):i, 0]\n",
    "        t1=numpy.reshape(t1,[1,look_back])\n",
    "        t4=dataset[(i-look_back):i, 48]\n",
    "        t4=numpy.reshape(t4,[1,look_back])\n",
    "        #print(t1.shape)\n",
    "        t2=dataset[i, 48-look_back:48]\n",
    "        t2=numpy.reshape(t2,[1,look_back])\n",
    "        t6=dataset[i,-(look_back+3):-3]\n",
    "        t6=numpy.reshape(t6,[1,look_back])\n",
    "#         t3=numpy.zeros([1,look_back])\n",
    "#         if i>=((day_offset+1)*7+look_back):\n",
    "#             t3[0,0:day_offset]=[dataset[j,0] for j in range(i-(((day_offset+1)*7)+look_back),i-(look_back+7),7)]\n",
    "#         t5=numpy.zeros([1,look_back])\n",
    "#         if i>=((day_offset+1)*7+look_back):\n",
    "#             t5[0,0:day_offset]=[dataset[j,48] for j in range(i-(((day_offset+1)*7)+look_back),i-(look_back+7),7)]\n",
    "            \n",
    "            \n",
    "        #print(t2.shape)\n",
    "        a[0,:] = t1\n",
    "        a[1,:] = t4\n",
    "        a[2,:] = t2\n",
    "        a[3,:] = t6\n",
    "#         a[4,:] = t3\n",
    "#         a[5,:] = t5\n",
    "        dataX1[i-look_back,:,:]=a[0:2,:]\n",
    "        dataX2[i-look_back,:,:]=a[2:4,:]\n",
    "#         dataX3[i-look_back,:,:]=a[4:,:]\n",
    "#         a = numpy.concatenate([dataset[(i-look_back-7):i-7, 0], dataset[i,-14:]],axis=1)\n",
    "        \n",
    "        #dataX.append(a)\n",
    "        dataY.append(dataset[i,-1])\n",
    "    return numpy.array(dataX1),numpy.array(dataX2),numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(datarange, categorical=[]):  \n",
    "    datarange= pd.DataFrame(datarange)\n",
    "    if not categorical:\n",
    "        meandata=datarange.mean()\n",
    "        meandata=meandata.to_numpy()\n",
    "    else:\n",
    "        meandata=datarange.mean()\n",
    "        meandata=meandata.to_numpy()\n",
    "        \n",
    "        modedata = datarange.mode()\n",
    "        modedata = modedata.to_numpy()\n",
    "        modedata = modedata[0,:]\n",
    "        \n",
    "        for i in categorical:\n",
    "                meandata[i-1] = modedata[i]\n",
    "                \n",
    "    datetime_series = pd.to_datetime(datarange['fltdat'])\n",
    "    miss_idx=pd.date_range(start = '01-01-2015', end = '31-12-2019' ).difference(datetime_series)\n",
    "    datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
    "    datarange=datarange.set_index(datetime_index)\n",
    "\n",
    "    datarange.drop('fltdat',axis=1,inplace=True)\n",
    "    newidx = pd.date_range('01-01-2015', '31-12-2019')\n",
    "    datarange = datarange.reindex(newidx, fill_value=0)\n",
    "    \n",
    "    meandata=meandata.reshape(1,meandata.shape[0])\n",
    "    dat = numpy.tile(meandata, [miss_idx.shape[0],1])\n",
    "    datarange.loc[miss_idx]=dat\n",
    "    return datarange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_test, y_pred):\n",
    "        import numpy as np\n",
    "        t = np.array(y_test)\n",
    "        p = np.array(y_pred)\n",
    "        mae = list()\n",
    "        mape = list()\n",
    "        for i in range(len(t)):\n",
    "            if (t[i] == 0):\n",
    "                mae.append(abs(p[i]))\n",
    "            else:\n",
    "                mae.append(float(abs(t[i] - p[i])))\n",
    "                mape.append(float(abs((t[i] - p[i])/t[i])))\n",
    "        return np.mean(mae) , np.mean(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fltdat', 'paxcntfc', 'fltnum', 'legorg', 'legdst', 'acrtypcod',\n",
      "       'keyidr', 'totpaylodwgt', 'totpaylodvol', 'totpaylodpos', 'totsetfc',\n",
      "       'totpaxwgt', 'dp_51', 'dp_50', 'dp_49', 'dp_48', 'dp_47', 'dp_46',\n",
      "       'dp_45', 'dp_44', 'dp_43', 'dp_42', 'dp_41', 'dp_40', 'dp_39', 'dp_38',\n",
      "       'dp_37', 'dp_36', 'dp_35', 'dp_34', 'dp_33', 'dp_32', 'dp_31', 'dp_30',\n",
      "       'dp_29', 'dp_28', 'dp_27', 'dp_26', 'dp_25', 'dp_24', 'dp_23', 'dp_22',\n",
      "       'dp_21', 'dp_20', 'dp_19', 'dp_18', 'dp_17', 'dp_16', 'dp_15', 'dp_14',\n",
      "       'dp_13', 'dp_12', 'dp_11', 'dp_10', 'dp_9', 'dp_8', 'dp_7', 'dp_6',\n",
      "       'dp_5', 'dp_4', 'dp_3', 'dp_2', 'dp_1'],\n",
      "      dtype='object')\n",
      "Index(['fltdat', 'paxcntfc', 'acrtypcod', 'totpaylodwgt', 'totpaxwgt', 'dp_51',\n",
      "       'dp_50', 'dp_49', 'dp_48', 'dp_47', 'dp_46', 'dp_45', 'dp_44', 'dp_43',\n",
      "       'dp_42', 'dp_41', 'dp_40', 'dp_39', 'dp_38', 'dp_37', 'dp_36', 'dp_35',\n",
      "       'dp_34', 'dp_33', 'dp_32', 'dp_31', 'dp_30', 'dp_29', 'dp_28', 'dp_27',\n",
      "       'dp_26', 'dp_25', 'dp_24', 'dp_23', 'dp_22', 'dp_21', 'dp_20', 'dp_19',\n",
      "       'dp_18', 'dp_17', 'dp_16', 'dp_15', 'dp_14', 'dp_13', 'dp_12', 'dp_11',\n",
      "       'dp_10', 'dp_9', 'dp_8', 'paxcnty', 'dcp_51', 'dcp_50', 'dcp_49',\n",
      "       'dcp_48', 'dcp_47', 'dcp_46', 'dcp_45', 'dcp_44', 'dcp_43', 'dcp_42',\n",
      "       'dcp_41', 'dcp_40', 'dcp_39', 'dcp_38', 'dcp_37', 'dcp_36', 'dcp_35',\n",
      "       'dcp_34', 'dcp_33', 'dcp_3', 'dcp_31', 'dcp_30', 'dcp_29', 'dcp_28',\n",
      "       'dcp_27', 'dcp_26', 'dcp_25', 'dcp_24', 'dcp_23', 'dcp_22', 'dcp_21',\n",
      "       'dcp_20', 'dcp_19', 'dcp_18', 'dcp_17', 'dcp_16', 'dcp_15', 'dcp_14',\n",
      "       'dcp_13', 'dcp_12', 'dcp_11', 'dcp_10', 'dcp_9', 'dcp_8'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('data/rmscapfc.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "print(dataframe.columns)\n",
    "dataframe.drop(dataframe.columns[[2,3,4,6,8,9,10,56,57,58,59,60,61,62] ], axis=1, inplace=True)\n",
    "\n",
    "dataframe2 = read_csv('data/rmscapy.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "# print(dataframe2.columns)\n",
    "f_column = dataframe2[[\"paxcnty\", \"dcp_51\", \"dcp_50\", \"dcp_49\", \"dcp_48\", \"dcp_47\", \"dcp_46\",\n",
    "       \"dcp_45\", \"dcp_44\", \"dcp_43\", \"dcp_42\", \"dcp_41\", \"dcp_40\", \"dcp_39\",\n",
    "       \"dcp_38\", \"dcp_37\", \"dcp_36\", \"dcp_35\", \"dcp_34\", \"dcp_33\", \"dcp_3\",\n",
    "       \"dcp_31\", \"dcp_30\", \"dcp_29\", \"dcp_28\", \"dcp_27\", \"dcp_26\", \"dcp_25\",\n",
    "       \"dcp_24\", \"dcp_23\", \"dcp_22\", \"dcp_21\", \"dcp_20\", \"dcp_19\", \"dcp_18\",\n",
    "       \"dcp_17\", \"dcp_16\", \"dcp_15\", \"dcp_14\", \"dcp_13\", \"dcp_12\", \"dcp_11\",\n",
    "       \"dcp_10\", \"dcp_9\", \"dcp_8\"]]\n",
    " \n",
    "\n",
    "dataframe = pd.concat([dataframe,f_column], axis = 1)\n",
    "# print(dataframe.columns)\n",
    "dataframe3 = read_csv('data/uldfc.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "\n",
    "dataframe3.drop(dataframe3.columns[[1,2,3] ], axis=1, inplace=True)\n",
    "dataframe4 = read_csv('data/uldy.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "dataframe4.drop(dataframe4.columns[[1,2,3] ], axis=1, inplace=True)\n",
    "f_column = dataframe4[[\"county\"]]\n",
    "dataframe3 = pd.concat([dataframe3,f_column], axis = 1)\n",
    "print(dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM=[0,1825,1825,1817,1816,1812,1821,1825,1824,1819,1825,1825,1824,1826,1819,1825,1822,1823,1813, 1826, 1820]\n",
    "NUMuld=[0,1817,1574,1808,1802,1807,1808,1730,1817,1385,1820,1816,1606,1819,1810,1817,421,1813,434,1532,1814]\n",
    "cat_inp=[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iist\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  import sys\n",
      "C:\\Users\\iist\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# dataset, datasetY = numpy.empty([1805,3,look_back]), []\n",
    "for i in range(0,len(NUM)-1):\n",
    "#     print(i)\n",
    "    datasub = dataframe.iloc[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "#     datasub=datasetall[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "    datasub = missing_values(datasub, cat_inp)\n",
    "    datasub = datasub.values\n",
    "    datasub = datasub.astype('float32')\n",
    "    \n",
    "    datasubuld = dataframe3.iloc[sum(NUMuld[0:i+1]):sum(NUMuld[0:i+2]),:]\n",
    "#     datasub=datasetall[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "    datasubuld = missing_values(datasubuld)\n",
    "    datasubuld = datasubuld.values\n",
    "    datasubuld = datasubuld.astype('float32')\n",
    "    \n",
    "    datasub = numpy.concatenate([datasub, datasubuld], axis=1)\n",
    "    if i==0:\n",
    "        data = datasub\n",
    "    else:\n",
    "        data = numpy.concatenate([data, datasub], axis =0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36520, 96)\n"
     ]
    }
   ],
   "source": [
    "out = data[:,2] - data[:,3] - (data[:,-1]*data[:,-2]*114)\n",
    "out = out.reshape(out.shape[0],1)\n",
    "data = numpy.concatenate([data,out], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_data_build(data, look_back):\n",
    "    for i in range(0,20):\n",
    "        datasub = data[i*1826:((i+1)*1826),:]\n",
    "        X1, X2, Y = create_dataset(datasub, look_back)\n",
    "        Y = Y.reshape(Y.shape[0],1)\n",
    "        if i==0: \n",
    "            data_LSTM_X1, data_LSTM_X2 = X1, X2\n",
    "            data_LSTM_Y = Y\n",
    "        else:\n",
    "            data_LSTM_X1,data_LSTM_X2 = numpy.concatenate([data_LSTM_X1, X1],axis=0), numpy.concatenate([data_LSTM_X2, X2],axis=0)  \n",
    "            data_LSTM_Y = numpy.concatenate([data_LSTM_Y, Y],axis=0)\n",
    "    return data_LSTM_X1,data_LSTM_X2,data_LSTM_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_build(X, Y, m1, m2):\n",
    "    spliter = int(X.shape[0]/20)\n",
    "    tot_len = m1\n",
    "    train_len = m2\n",
    "    for i in range(0,20):\n",
    "        Xsub = X[i*spliter:((i+1)*spliter),:]\n",
    "        Ysub = Y[i*spliter:((i+1)*spliter)]\n",
    "        if i==0:\n",
    "            trainX = Xsub[0:m2,:]\n",
    "            testX = Xsub[m2:m1,:]\n",
    "            trainY = Ysub[0:m2]\n",
    "            testY = Ysub[m2:m1]\n",
    "        else:\n",
    "            trainX = numpy.concatenate([trainX, Xsub[0:m2,:]], axis=0)\n",
    "            testX = numpy.concatenate([testX, Xsub[m2:m1,:]], axis=0)\n",
    "            trainY = numpy.concatenate([trainY, Ysub[0:m2]], axis=0)\n",
    "            testY = numpy.concatenate([testY, Ysub[m2:m1]], axis=0)\n",
    "        \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(trainX1, trainX2, testX1, testX2, trainY, testY, units, saving =False, month=None, EarlyStop = False):\n",
    "    \n",
    "    if EarlyStop:\n",
    "        callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                                            mode = 'min', restore_best_weights=True)\n",
    "    \n",
    "    inp1 = Input(shape=(2, trainX1.shape[2]))\n",
    "    inp2 = Input(shape=(2, trainX2.shape[2]))\n",
    "#     inp3 = Input(shape=(2, trainX3.shape[2]))\n",
    "\n",
    "    LS1 = LSTM(units, input_shape=(2, trainX1.shape[2]))\n",
    "    out1 = LS1(inp1)\n",
    "    LS2 = LSTM(units, input_shape=(2, trainX2.shape[2]))\n",
    "    out2 = LS2(inp2)\n",
    "#     LS3 = LSTM(units, input_shape=(2, trainX3.shape[2]))\n",
    "#     out3 = LS3(inp3)\n",
    "\n",
    "    mrg = Concatenate(axis=1)([out1,out2])\n",
    "    op = Dense(1)(mrg)\n",
    "\n",
    "    model = Model(inputs=[inp1, inp2], outputs=op)\n",
    "#     model.add(Dense(1))\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    if EarlyStop:\n",
    "        history =model.fit([trainX1, trainX2], trainY, epochs=100, batch_size=150, \n",
    "                           validation_data=([testX1,testX2], testY),verbose=1,callbacks=[callback])\n",
    "    else:\n",
    "        history =model.fit([trainX1, trainX2], trainY, epochs=100, batch_size=150, \n",
    "                           validation_data=([testX1,testX2], testY),verbose=1)\n",
    "                \n",
    "    testPredict = model.predict([testX1,testX2])\n",
    "                \n",
    "    sh = testPredict.shape\n",
    "    inv_yhat = testPredict.reshape(sh[0]*sh[1],1)\n",
    "    inv_yhat = numpy.concatenate([ data[0:inv_yhat.shape[0],0:95], inv_yhat], axis=1)\n",
    "\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "    testY = testY.reshape(sh[0]*sh[1],1)\n",
    " \n",
    "    inv_y = numpy.concatenate([data[0:testY.shape[0],0:95], testY], axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-1]\n",
    "             \n",
    "    rmse = numpy.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    mae, mape = mean_absolute_percentage_error(inv_y, inv_yhat) \n",
    "    r2 = r2_score(inv_y, inv_yhat)\n",
    "    \n",
    "    res=[]\n",
    "    if saving:\n",
    "        inv_y = inv_y.reshape(inv_y.shape[0],1) \n",
    "        inv_yhat = inv_yhat.reshape(inv_yhat.shape[0],1) \n",
    "        res = numpy.concatenate([inv_y, inv_yhat], axis=1)\n",
    "        df = pd.DataFrame(res)\n",
    "        res = df.to_csv(\"pm2f_\" + month + \".csv\", index = False)\n",
    "    return rmse, mape,mae,r2, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(lag_vec = [7,14,21,28], units_vec = [2,4,8,16,32,64,128]):\n",
    "    results = numpy.zeros([4,3,len(lag_vec),len(units_vec)])\n",
    "    \n",
    "    for folds in range(0,3):\n",
    "        for i in range(0,len(lag_vec)):\n",
    "            data_LSTM_X1,data_LSTM_X2,data_LSTM_Y = sequence_data_build(data, lag_vec[i])\n",
    "            totday = int(data_LSTM_X1.shape[0]/20)\n",
    "            m1 = [totday-152, totday-121, totday-91, ]\n",
    "            m2 = [m1[0]-31, m1[1]-31, m1[2]-30, ]\n",
    "            lag = lag_vec[i]\n",
    "            trainX1, trainY, testX1, testY= train_test_build(data_LSTM_X1, data_LSTM_Y, m1[folds], m2[folds])\n",
    "            trainX2, trainY, testX2, testY= train_test_build(data_LSTM_X2, data_LSTM_Y, m1[folds], m2[folds])\n",
    "#             trainX3, trainY, testX3, testY= train_test_build(data_LSTM_X3, data_LSTM_Y, m1[folds], m2[folds])\n",
    "            testYcopy=testY\n",
    "            for j in range(0, len(units_vec)):\n",
    "                print(\" \")\n",
    "                print(\" \")\n",
    "                print(\" \")\n",
    "                print(\"------------------------------------------------\")\n",
    "                print(\"fold: {0}, lag: {1}, units: {2}\".format(folds, lag_vec[i], units_vec[j]))\n",
    "                units = units_vec[j]\n",
    "                rmse, mape, mae, r2, his = model_build(trainX1, trainX2, testX1, testX2, trainY, testY, units, EarlyStop=True) \n",
    "                results[0,folds,i,j] = rmse \n",
    "                results[2,folds,i,j], results[1,folds,i,j] = mae, mape\n",
    "                results[3,folds,i,j] = r2\n",
    "    return results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 2\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 7s 218us/sample - loss: 0.2108 - val_loss: 0.1549\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1777 - val_loss: 0.1413\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1633 - val_loss: 0.1296\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1480 - val_loss: 0.1159\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1379 - val_loss: 0.1060\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1329 - val_loss: 0.1027\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1292 - val_loss: 0.1010\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1263 - val_loss: 0.0997\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1243 - val_loss: 0.0972\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1230 - val_loss: 0.0992\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1219 - val_loss: 0.1011\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1211 - val_loss: 0.0974\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1206 - val_loss: 0.0986\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1201 - val_loss: 0.0966\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1196 - val_loss: 0.0965\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1192 - val_loss: 0.0964\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1190 - val_loss: 0.0996\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1186 - val_loss: 0.0954\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1184 - val_loss: 0.0943\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1182 - val_loss: 0.0967\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1178 - val_loss: 0.0962\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1176 - val_loss: 0.0959\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1173 - val_loss: 0.0952\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1170 - val_loss: 0.0942\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1169 - val_loss: 0.0937\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1166 - val_loss: 0.0939\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 2s 76us/sample - loss: 0.1164 - val_loss: 0.0919\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1162 - val_loss: 0.0923\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1158 - val_loss: 0.0937\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1156 - val_loss: 0.0914\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1153 - val_loss: 0.0926\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 2s 62us/sample - loss: 0.1150 - val_loss: 0.0935\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 2s 62us/sample - loss: 0.1148 - val_loss: 0.0907\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1145 - val_loss: 0.0894\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1143 - val_loss: 0.0893\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1141 - val_loss: 0.0896\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1138 - val_loss: 0.0891\n",
      "Epoch 38/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1137 - val_loss: 0.0924\n",
      "Epoch 39/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1134 - val_loss: 0.0893\n",
      "Epoch 40/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1132 - val_loss: 0.0871\n",
      "Epoch 41/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1131 - val_loss: 0.0899\n",
      "Epoch 42/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1129 - val_loss: 0.0917\n",
      "Epoch 43/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1127 - val_loss: 0.0897\n",
      "Epoch 44/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1125 - val_loss: 0.0876\n",
      "Epoch 45/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1124 - val_loss: 0.0908\n",
      "Epoch 46/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1123 - val_loss: 0.0910\n",
      "Epoch 47/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1122 - val_loss: 0.0869\n",
      "Epoch 48/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1120 - val_loss: 0.0867\n",
      "Epoch 49/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1119 - val_loss: 0.0856\n",
      "Epoch 50/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1117 - val_loss: 0.0885\n",
      "Epoch 51/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1116 - val_loss: 0.0867\n",
      "Epoch 52/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1115 - val_loss: 0.0852\n",
      "Epoch 53/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1114 - val_loss: 0.0849\n",
      "Epoch 54/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1113 - val_loss: 0.0886\n",
      "Epoch 55/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1111 - val_loss: 0.0865\n",
      "Epoch 56/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1110 - val_loss: 0.0869\n",
      "Epoch 57/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1109 - val_loss: 0.0862\n",
      "Epoch 58/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1108 - val_loss: 0.0855\n",
      "Epoch 59/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1107 - val_loss: 0.0859\n",
      "Epoch 60/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1106 - val_loss: 0.0861\n",
      "Epoch 61/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1105 - val_loss: 0.0853\n",
      "Epoch 62/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1105 - val_loss: 0.0854\n",
      "Epoch 63/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1104 - val_loss: 0.0843\n",
      "Epoch 64/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1102 - val_loss: 0.0829\n",
      "Epoch 65/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1102 - val_loss: 0.0841\n",
      "Epoch 66/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1100 - val_loss: 0.0840\n",
      "Epoch 67/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1099 - val_loss: 0.0842\n",
      "Epoch 68/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1099 - val_loss: 0.0854\n",
      "Epoch 69/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1098 - val_loss: 0.0841\n",
      "Epoch 70/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1097 - val_loss: 0.0832\n",
      "Epoch 71/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1097 - val_loss: 0.0840\n",
      "Epoch 72/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1096 - val_loss: 0.0822\n",
      "Epoch 73/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1095 - val_loss: 0.0827\n",
      "Epoch 74/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1094 - val_loss: 0.0819\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1093 - val_loss: 0.0823\n",
      "Epoch 76/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1093 - val_loss: 0.0835\n",
      "Epoch 77/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1092 - val_loss: 0.0837\n",
      "Epoch 78/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1091 - val_loss: 0.0818\n",
      "Epoch 79/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1090 - val_loss: 0.0819\n",
      "Epoch 80/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1090 - val_loss: 0.0824\n",
      "Epoch 81/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1089 - val_loss: 0.0813\n",
      "Epoch 82/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1088 - val_loss: 0.0826\n",
      "Epoch 83/100\n",
      "32720/32720 [==============================] - 3s 77us/sample - loss: 0.1087 - val_loss: 0.0821\n",
      "Epoch 84/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1087 - val_loss: 0.0828\n",
      "Epoch 85/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1086 - val_loss: 0.0844\n",
      "Epoch 86/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1086 - val_loss: 0.0807\n",
      "Epoch 87/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1085 - val_loss: 0.0797\n",
      "Epoch 88/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1084 - val_loss: 0.0816\n",
      "Epoch 89/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1084 - val_loss: 0.0810\n",
      "Epoch 90/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1083 - val_loss: 0.0821\n",
      "Epoch 91/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1082 - val_loss: 0.0815\n",
      "Epoch 92/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1082 - val_loss: 0.0790\n",
      "Epoch 93/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1081 - val_loss: 0.0801\n",
      "Epoch 94/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1080 - val_loss: 0.0805\n",
      "Epoch 95/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1080 - val_loss: 0.0784\n",
      "Epoch 96/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1079 - val_loss: 0.0815\n",
      "Epoch 97/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1078 - val_loss: 0.0806\n",
      "Epoch 98/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1078 - val_loss: 0.0794\n",
      "Epoch 99/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1077 - val_loss: 0.0767\n",
      "Epoch 100/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1077 - val_loss: 0.0783\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 4\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 7s 212us/sample - loss: 0.1806 - val_loss: 0.1177\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1391 - val_loss: 0.1055\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1299 - val_loss: 0.1018\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1260 - val_loss: 0.1039\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1239 - val_loss: 0.0989\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1226 - val_loss: 0.0984\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1217 - val_loss: 0.0984\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1210 - val_loss: 0.0992\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1204 - val_loss: 0.0990\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 61us/sample - loss: 0.1198 - val_loss: 0.0964\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1194 - val_loss: 0.0998\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1190 - val_loss: 0.0988\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1187 - val_loss: 0.0977\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1183 - val_loss: 0.0968\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1179 - val_loss: 0.0929\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1177 - val_loss: 0.0951\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1174 - val_loss: 0.0960\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1171 - val_loss: 0.0958\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1168 - val_loss: 0.0937\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1166 - val_loss: 0.0936\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1163 - val_loss: 0.0940\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1161 - val_loss: 0.0924\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1158 - val_loss: 0.0942\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1156 - val_loss: 0.0919\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1154 - val_loss: 0.0932\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1152 - val_loss: 0.0932\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1148 - val_loss: 0.0904\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1148 - val_loss: 0.0969\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1145 - val_loss: 0.0951\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1144 - val_loss: 0.0935\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1141 - val_loss: 0.0912\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1140 - val_loss: 0.0929\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1137 - val_loss: 0.0949\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1136 - val_loss: 0.0885\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1134 - val_loss: 0.0939\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1131 - val_loss: 0.0901\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1129 - val_loss: 0.0908\n",
      "Epoch 38/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1128 - val_loss: 0.0905\n",
      "Epoch 39/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1125 - val_loss: 0.0881\n",
      "Epoch 40/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1123 - val_loss: 0.0873\n",
      "Epoch 41/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1122 - val_loss: 0.0916\n",
      "Epoch 42/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1120 - val_loss: 0.0869\n",
      "Epoch 43/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1119 - val_loss: 0.0885\n",
      "Epoch 44/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1117 - val_loss: 0.0870\n",
      "Epoch 45/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1115 - val_loss: 0.0920\n",
      "Epoch 46/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1115 - val_loss: 0.0893\n",
      "Epoch 47/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1112 - val_loss: 0.0904\n",
      "Epoch 48/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1112 - val_loss: 0.0864\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1109 - val_loss: 0.0863\n",
      "Epoch 50/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1108 - val_loss: 0.0922\n",
      "Epoch 51/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1108 - val_loss: 0.0907\n",
      "Epoch 52/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1105 - val_loss: 0.0881\n",
      "Epoch 53/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1104 - val_loss: 0.0842\n",
      "Epoch 54/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1104 - val_loss: 0.0861\n",
      "Epoch 55/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1102 - val_loss: 0.0865\n",
      "Epoch 56/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1100 - val_loss: 0.0856\n",
      "Epoch 57/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1100 - val_loss: 0.0839\n",
      "Epoch 58/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1097 - val_loss: 0.0857\n",
      "Epoch 59/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1096 - val_loss: 0.0862\n",
      "Epoch 60/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1095 - val_loss: 0.0830\n",
      "Epoch 61/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1096 - val_loss: 0.0866\n",
      "Epoch 62/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1093 - val_loss: 0.0841\n",
      "Epoch 63/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1092 - val_loss: 0.0864\n",
      "Epoch 64/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1090 - val_loss: 0.0857\n",
      "Epoch 65/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1089 - val_loss: 0.0838\n",
      "Epoch 66/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1089 - val_loss: 0.0854\n",
      "Epoch 67/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1089 - val_loss: 0.0827\n",
      "Epoch 68/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1086 - val_loss: 0.0863\n",
      "Epoch 69/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1086 - val_loss: 0.0796\n",
      "Epoch 70/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1084 - val_loss: 0.0830\n",
      "Epoch 71/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1086 - val_loss: 0.0853\n",
      "Epoch 72/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1084 - val_loss: 0.0808\n",
      "Epoch 73/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1083 - val_loss: 0.0809\n",
      "Epoch 74/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1080 - val_loss: 0.0845\n",
      "Epoch 75/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1081 - val_loss: 0.0848\n",
      "Epoch 76/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1080 - val_loss: 0.0818\n",
      "Epoch 77/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1079 - val_loss: 0.0868\n",
      "Epoch 78/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1077 - val_loss: 0.0774\n",
      "Epoch 79/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1078 - val_loss: 0.0825\n",
      "Epoch 80/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1077 - val_loss: 0.0841\n",
      "Epoch 81/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1074 - val_loss: 0.0846\n",
      "Epoch 82/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1075 - val_loss: 0.0831\n",
      "Epoch 83/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1073 - val_loss: 0.0783\n",
      "Epoch 84/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1073 - val_loss: 0.0786\n",
      "Epoch 85/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1072 - val_loss: 0.0825\n",
      "Epoch 86/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1070 - val_loss: 0.0811\n",
      "Epoch 87/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1071 - val_loss: 0.0802\n",
      "Epoch 88/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1070 - val_loss: 0.0789\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 8\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 7s 216us/sample - loss: 0.1612 - val_loss: 0.1098\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1277 - val_loss: 0.1012\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1236 - val_loss: 0.0967\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1220 - val_loss: 0.0991\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1208 - val_loss: 0.0982\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1199 - val_loss: 0.0958\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1193 - val_loss: 0.0998\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 74us/sample - loss: 0.1189 - val_loss: 0.0988\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1183 - val_loss: 0.0991\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.1179 - val_loss: 0.0956\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.1175 - val_loss: 0.0999\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 60us/sample - loss: 0.1169 - val_loss: 0.0996\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 61us/sample - loss: 0.1165 - val_loss: 0.0978\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.1163 - val_loss: 0.0969\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 62us/sample - loss: 0.1159 - val_loss: 0.0928\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1157 - val_loss: 0.0943\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1153 - val_loss: 0.0951\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1150 - val_loss: 0.0921\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1146 - val_loss: 0.0940\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1146 - val_loss: 0.0915\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1140 - val_loss: 0.0936\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1138 - val_loss: 0.0889\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1135 - val_loss: 0.0920\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 73us/sample - loss: 0.1132 - val_loss: 0.0893\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1132 - val_loss: 0.0909\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1129 - val_loss: 0.0901\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1124 - val_loss: 0.0871\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1123 - val_loss: 0.0922\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1120 - val_loss: 0.0920\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1118 - val_loss: 0.0912\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1115 - val_loss: 0.0874\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 2s 73us/sample - loss: 0.1114 - val_loss: 0.0884\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1111 - val_loss: 0.0900\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1108 - val_loss: 0.0853\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1107 - val_loss: 0.0875\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1104 - val_loss: 0.0836\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1101 - val_loss: 0.0847\n",
      "Epoch 38/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1100 - val_loss: 0.0860\n",
      "Epoch 39/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1097 - val_loss: 0.0822\n",
      "Epoch 40/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1095 - val_loss: 0.0828\n",
      "Epoch 41/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1095 - val_loss: 0.0876\n",
      "Epoch 42/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1091 - val_loss: 0.0826\n",
      "Epoch 43/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1089 - val_loss: 0.0831\n",
      "Epoch 44/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1087 - val_loss: 0.0800\n",
      "Epoch 45/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1085 - val_loss: 0.0881\n",
      "Epoch 46/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1086 - val_loss: 0.0816\n",
      "Epoch 47/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1082 - val_loss: 0.0854\n",
      "Epoch 48/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1081 - val_loss: 0.0808\n",
      "Epoch 49/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1079 - val_loss: 0.0790\n",
      "Epoch 50/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1077 - val_loss: 0.0853\n",
      "Epoch 51/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1076 - val_loss: 0.0843\n",
      "Epoch 52/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1073 - val_loss: 0.0825\n",
      "Epoch 53/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1073 - val_loss: 0.0799\n",
      "Epoch 54/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1071 - val_loss: 0.0813\n",
      "Epoch 55/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1068 - val_loss: 0.0798\n",
      "Epoch 56/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1068 - val_loss: 0.0764\n",
      "Epoch 57/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1067 - val_loss: 0.0779\n",
      "Epoch 58/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1064 - val_loss: 0.0770\n",
      "Epoch 59/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1064 - val_loss: 0.0783\n",
      "Epoch 60/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1062 - val_loss: 0.0773\n",
      "Epoch 61/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1063 - val_loss: 0.0781\n",
      "Epoch 62/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1061 - val_loss: 0.0766\n",
      "Epoch 63/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1059 - val_loss: 0.0801\n",
      "Epoch 64/100\n",
      "32720/32720 [==============================] - 2s 73us/sample - loss: 0.1057 - val_loss: 0.0788\n",
      "Epoch 65/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1055 - val_loss: 0.0755\n",
      "Epoch 66/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1055 - val_loss: 0.0781\n",
      "Epoch 67/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1054 - val_loss: 0.0751\n",
      "Epoch 68/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1052 - val_loss: 0.0787\n",
      "Epoch 69/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1051 - val_loss: 0.0725\n",
      "Epoch 70/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1050 - val_loss: 0.0744\n",
      "Epoch 71/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1050 - val_loss: 0.0790\n",
      "Epoch 72/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1049 - val_loss: 0.0741\n",
      "Epoch 73/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1049 - val_loss: 0.0733\n",
      "Epoch 74/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1045 - val_loss: 0.0774\n",
      "Epoch 75/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1045 - val_loss: 0.0792\n",
      "Epoch 76/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1043 - val_loss: 0.0719\n",
      "Epoch 77/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1043 - val_loss: 0.0787\n",
      "Epoch 78/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1041 - val_loss: 0.0716\n",
      "Epoch 79/100\n",
      "32720/32720 [==============================] - 2s 73us/sample - loss: 0.1041 - val_loss: 0.0735\n",
      "Epoch 80/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1039 - val_loss: 0.0758\n",
      "Epoch 81/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1038 - val_loss: 0.0777\n",
      "Epoch 82/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1037 - val_loss: 0.0779\n",
      "Epoch 83/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1036 - val_loss: 0.0713\n",
      "Epoch 84/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1034 - val_loss: 0.0724\n",
      "Epoch 85/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1033 - val_loss: 0.0770\n",
      "Epoch 86/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1032 - val_loss: 0.0748\n",
      "Epoch 87/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1031 - val_loss: 0.0705\n",
      "Epoch 88/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1029 - val_loss: 0.0727\n",
      "Epoch 89/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1029 - val_loss: 0.0701\n",
      "Epoch 90/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1028 - val_loss: 0.0744\n",
      "Epoch 91/100\n",
      "32720/32720 [==============================] - 2s 73us/sample - loss: 0.1027 - val_loss: 0.0742\n",
      "Epoch 92/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1025 - val_loss: 0.0703\n",
      "Epoch 93/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1026 - val_loss: 0.0709\n",
      "Epoch 94/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1023 - val_loss: 0.0697\n",
      "Epoch 95/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1022 - val_loss: 0.0692\n",
      "Epoch 96/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1021 - val_loss: 0.0723\n",
      "Epoch 97/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1020 - val_loss: 0.0762\n",
      "Epoch 98/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1019 - val_loss: 0.0725\n",
      "Epoch 99/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1018 - val_loss: 0.0737\n",
      "Epoch 100/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1018 - val_loss: 0.0716\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 16\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 7s 209us/sample - loss: 0.1543 - val_loss: 0.1096\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1265 - val_loss: 0.1006\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1227 - val_loss: 0.0982\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1213 - val_loss: 0.0977\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1200 - val_loss: 0.1015\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1192 - val_loss: 0.0995\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 62us/sample - loss: 0.1187 - val_loss: 0.1042\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.1181 - val_loss: 0.0924\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1177 - val_loss: 0.1007\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1174 - val_loss: 0.0974\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1170 - val_loss: 0.0954\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1163 - val_loss: 0.0965\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1158 - val_loss: 0.0907\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1155 - val_loss: 0.0951\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1153 - val_loss: 0.0929\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1147 - val_loss: 0.0944\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1145 - val_loss: 0.0938\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1144 - val_loss: 0.0931\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1139 - val_loss: 0.0877\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1136 - val_loss: 0.0879\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1132 - val_loss: 0.0929\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1128 - val_loss: 0.0890\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1126 - val_loss: 0.0898\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1123 - val_loss: 0.0863\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1121 - val_loss: 0.0872\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1119 - val_loss: 0.0853\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1113 - val_loss: 0.0848\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1110 - val_loss: 0.0937\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1110 - val_loss: 0.0914\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1104 - val_loss: 0.0914\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1100 - val_loss: 0.0890\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1097 - val_loss: 0.0879\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1094 - val_loss: 0.0852\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1088 - val_loss: 0.0808\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1087 - val_loss: 0.0850\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1084 - val_loss: 0.0774\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1079 - val_loss: 0.0869\n",
      "Epoch 38/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1076 - val_loss: 0.0831\n",
      "Epoch 39/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1073 - val_loss: 0.0811\n",
      "Epoch 40/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1068 - val_loss: 0.0783\n",
      "Epoch 41/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1066 - val_loss: 0.0845\n",
      "Epoch 42/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1062 - val_loss: 0.0762\n",
      "Epoch 43/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1060 - val_loss: 0.0788\n",
      "Epoch 44/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1059 - val_loss: 0.0777\n",
      "Epoch 45/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1055 - val_loss: 0.0816\n",
      "Epoch 46/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1051 - val_loss: 0.0775\n",
      "Epoch 47/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1050 - val_loss: 0.0786\n",
      "Epoch 48/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1048 - val_loss: 0.0745\n",
      "Epoch 49/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1043 - val_loss: 0.0745\n",
      "Epoch 50/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1041 - val_loss: 0.0802\n",
      "Epoch 51/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1041 - val_loss: 0.0836\n",
      "Epoch 52/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1037 - val_loss: 0.0801\n",
      "Epoch 53/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1038 - val_loss: 0.0752\n",
      "Epoch 54/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1034 - val_loss: 0.0748\n",
      "Epoch 55/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1033 - val_loss: 0.0725\n",
      "Epoch 56/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1031 - val_loss: 0.0696\n",
      "Epoch 57/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1032 - val_loss: 0.0761\n",
      "Epoch 58/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1028 - val_loss: 0.0734\n",
      "Epoch 59/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1027 - val_loss: 0.0756\n",
      "Epoch 60/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1023 - val_loss: 0.0712\n",
      "Epoch 61/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1023 - val_loss: 0.0724\n",
      "Epoch 62/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1022 - val_loss: 0.0739\n",
      "Epoch 63/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1019 - val_loss: 0.0707\n",
      "Epoch 64/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1019 - val_loss: 0.0749\n",
      "Epoch 65/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1016 - val_loss: 0.0709\n",
      "Epoch 66/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1017 - val_loss: 0.0706\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 32\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 7s 207us/sample - loss: 0.1509 - val_loss: 0.1066\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1261 - val_loss: 0.1006\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1227 - val_loss: 0.1001\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1212 - val_loss: 0.1009\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1200 - val_loss: 0.1056\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1192 - val_loss: 0.0976\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1187 - val_loss: 0.1054\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1183 - val_loss: 0.0913\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1178 - val_loss: 0.1015\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1172 - val_loss: 0.0971\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1167 - val_loss: 0.0968\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1164 - val_loss: 0.0943\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1156 - val_loss: 0.0915\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1151 - val_loss: 0.0972\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1148 - val_loss: 0.0932\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1140 - val_loss: 0.0968\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1138 - val_loss: 0.0941\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1132 - val_loss: 0.0942\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 64\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 7s 203us/sample - loss: 0.1439 - val_loss: 0.1020\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1246 - val_loss: 0.1086\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1219 - val_loss: 0.0993\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1216 - val_loss: 0.1035\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 61us/sample - loss: 0.1201 - val_loss: 0.1048\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.1193 - val_loss: 0.0945\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.1188 - val_loss: 0.1038\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1187 - val_loss: 0.0893\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1182 - val_loss: 0.1049\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1176 - val_loss: 0.0976\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1171 - val_loss: 0.0983\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1169 - val_loss: 0.0934\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1159 - val_loss: 0.0907\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 3s 77us/sample - loss: 0.1155 - val_loss: 0.0944\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1150 - val_loss: 0.0934\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1144 - val_loss: 0.0993\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1147 - val_loss: 0.0986\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1140 - val_loss: 0.0892\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1135 - val_loss: 0.0827\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1125 - val_loss: 0.0851\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1122 - val_loss: 0.0895\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1114 - val_loss: 0.0851\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1107 - val_loss: 0.0835\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1104 - val_loss: 0.0784\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1093 - val_loss: 0.0805\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1096 - val_loss: 0.0800\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 2s 71us/sample - loss: 0.1083 - val_loss: 0.0787\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1078 - val_loss: 0.0863\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1069 - val_loss: 0.0781\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1065 - val_loss: 0.0811\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1058 - val_loss: 0.0743\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1053 - val_loss: 0.0769\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1048 - val_loss: 0.0691\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1040 - val_loss: 0.0711\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1037 - val_loss: 0.0750\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1033 - val_loss: 0.0703\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1030 - val_loss: 0.0734\n",
      "Epoch 38/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1022 - val_loss: 0.0722\n",
      "Epoch 39/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1022 - val_loss: 0.0696\n",
      "Epoch 40/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1020 - val_loss: 0.0667\n",
      "Epoch 41/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1019 - val_loss: 0.0779\n",
      "Epoch 42/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1016 - val_loss: 0.0665\n",
      "Epoch 43/100\n",
      "32720/32720 [==============================] - 2s 62us/sample - loss: 0.1012 - val_loss: 0.0676\n",
      "Epoch 44/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1009 - val_loss: 0.0681\n",
      "Epoch 45/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1008 - val_loss: 0.0715\n",
      "Epoch 46/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1006 - val_loss: 0.0708\n",
      "Epoch 47/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1004 - val_loss: 0.0659\n",
      "Epoch 48/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.1001 - val_loss: 0.0683\n",
      "Epoch 49/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1001 - val_loss: 0.0649\n",
      "Epoch 50/100\n",
      "32720/32720 [==============================] - 2s 62us/sample - loss: 0.0994 - val_loss: 0.0678\n",
      "Epoch 51/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.0994 - val_loss: 0.0660\n",
      "Epoch 52/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.0993 - val_loss: 0.0662\n",
      "Epoch 53/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0992 - val_loss: 0.0678\n",
      "Epoch 54/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0989 - val_loss: 0.0655\n",
      "Epoch 55/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0990 - val_loss: 0.0689\n",
      "Epoch 56/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0986 - val_loss: 0.0642\n",
      "Epoch 57/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0984 - val_loss: 0.0667\n",
      "Epoch 58/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0986 - val_loss: 0.0668\n",
      "Epoch 59/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0984 - val_loss: 0.0700\n",
      "Epoch 60/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0981 - val_loss: 0.0656\n",
      "Epoch 61/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0979 - val_loss: 0.0680\n",
      "Epoch 62/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.0982 - val_loss: 0.0709\n",
      "Epoch 63/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.0975 - val_loss: 0.0617\n",
      "Epoch 64/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0975 - val_loss: 0.0676\n",
      "Epoch 65/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.0973 - val_loss: 0.0631\n",
      "Epoch 66/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.0974 - val_loss: 0.0639\n",
      "Epoch 67/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0972 - val_loss: 0.0701\n",
      "Epoch 68/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0976 - val_loss: 0.0722\n",
      "Epoch 69/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.0972 - val_loss: 0.0656\n",
      "Epoch 70/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0969 - val_loss: 0.0613\n",
      "Epoch 71/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.0971 - val_loss: 0.0655\n",
      "Epoch 72/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0968 - val_loss: 0.0703\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 2s 61us/sample - loss: 0.0967 - val_loss: 0.0657\n",
      "Epoch 74/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0966 - val_loss: 0.0625\n",
      "Epoch 75/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.0965 - val_loss: 0.0635\n",
      "Epoch 76/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.0968 - val_loss: 0.0641\n",
      "Epoch 77/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.0963 - val_loss: 0.0649\n",
      "Epoch 78/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0964 - val_loss: 0.0619\n",
      "Epoch 79/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0963 - val_loss: 0.0619\n",
      "Epoch 80/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.0964 - val_loss: 0.0612\n",
      "Epoch 81/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0963 - val_loss: 0.0636\n",
      "Epoch 82/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.0962 - val_loss: 0.0635\n",
      "Epoch 83/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0962 - val_loss: 0.0649\n",
      "Epoch 84/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0959 - val_loss: 0.0617\n",
      "Epoch 85/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0960 - val_loss: 0.0704\n",
      "Epoch 86/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.0959 - val_loss: 0.0626\n",
      "Epoch 87/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.0958 - val_loss: 0.0616\n",
      "Epoch 88/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.0957 - val_loss: 0.0610\n",
      "Epoch 89/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0957 - val_loss: 0.0624\n",
      "Epoch 90/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0956 - val_loss: 0.0638\n",
      "Epoch 91/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0955 - val_loss: 0.0652\n",
      "Epoch 92/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0956 - val_loss: 0.0611\n",
      "Epoch 93/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0955 - val_loss: 0.0631\n",
      "Epoch 94/100\n",
      "32720/32720 [==============================] - 2s 60us/sample - loss: 0.0957 - val_loss: 0.0684\n",
      "Epoch 95/100\n",
      "32720/32720 [==============================] - 2s 62us/sample - loss: 0.0954 - val_loss: 0.0616\n",
      "Epoch 96/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.0954 - val_loss: 0.0658\n",
      "Epoch 97/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0954 - val_loss: 0.0627\n",
      "Epoch 98/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.0953 - val_loss: 0.0632\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 128\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 7s 208us/sample - loss: 0.1428 - val_loss: 0.0989\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1244 - val_loss: 0.1047\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1226 - val_loss: 0.1031\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1218 - val_loss: 0.1093\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1207 - val_loss: 0.1161\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1199 - val_loss: 0.0974\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1190 - val_loss: 0.1033\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1180 - val_loss: 0.0864\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.1185 - val_loss: 0.1055\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1171 - val_loss: 0.0963\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1164 - val_loss: 0.0957\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1155 - val_loss: 0.0915\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1142 - val_loss: 0.0870\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1134 - val_loss: 0.0933\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1121 - val_loss: 0.0875\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1114 - val_loss: 0.0871\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1108 - val_loss: 0.0942\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1098 - val_loss: 0.0801\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1092 - val_loss: 0.0756\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1082 - val_loss: 0.0759\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1082 - val_loss: 0.0818\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1071 - val_loss: 0.0788\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1065 - val_loss: 0.0769\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1061 - val_loss: 0.0711\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1055 - val_loss: 0.0674\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.1056 - val_loss: 0.0782\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1052 - val_loss: 0.0712\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.1044 - val_loss: 0.0790\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1039 - val_loss: 0.0722\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1032 - val_loss: 0.0682\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1030 - val_loss: 0.0712\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1028 - val_loss: 0.0684\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.1025 - val_loss: 0.0658\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.1018 - val_loss: 0.0651\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 2s 70us/sample - loss: 0.1016 - val_loss: 0.0681\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.1009 - val_loss: 0.0666\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.1005 - val_loss: 0.0710\n",
      "Epoch 38/100\n",
      "32720/32720 [==============================] - 2s 63us/sample - loss: 0.0998 - val_loss: 0.0686\n",
      "Epoch 39/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0999 - val_loss: 0.0674\n",
      "Epoch 40/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.0995 - val_loss: 0.0656\n",
      "Epoch 41/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.0993 - val_loss: 0.0675\n",
      "Epoch 42/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.0991 - val_loss: 0.0682\n",
      "Epoch 43/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0990 - val_loss: 0.0658\n",
      "Epoch 44/100\n",
      "32720/32720 [==============================] - 2s 73us/sample - loss: 0.0986 - val_loss: 0.0648\n",
      "Epoch 45/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0983 - val_loss: 0.0667\n",
      "Epoch 46/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0982 - val_loss: 0.0697\n",
      "Epoch 47/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0979 - val_loss: 0.0673\n",
      "Epoch 48/100\n",
      "32720/32720 [==============================] - 2s 68us/sample - loss: 0.0977 - val_loss: 0.0644\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0977 - val_loss: 0.0620\n",
      "Epoch 50/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0975 - val_loss: 0.0708\n",
      "Epoch 51/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0974 - val_loss: 0.0641\n",
      "Epoch 52/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.0972 - val_loss: 0.0686\n",
      "Epoch 53/100\n",
      "32720/32720 [==============================] - 2s 69us/sample - loss: 0.0971 - val_loss: 0.0624\n",
      "Epoch 54/100\n",
      "32720/32720 [==============================] - 2s 66us/sample - loss: 0.0973 - val_loss: 0.0657\n",
      "Epoch 55/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0970 - val_loss: 0.0635\n",
      "Epoch 56/100\n",
      "32720/32720 [==============================] - 2s 67us/sample - loss: 0.0968 - val_loss: 0.0621\n",
      "Epoch 57/100\n",
      "32720/32720 [==============================] - 2s 72us/sample - loss: 0.0968 - val_loss: 0.0685\n",
      "Epoch 58/100\n",
      "32720/32720 [==============================] - 2s 65us/sample - loss: 0.0966 - val_loss: 0.0660\n",
      "Epoch 59/100\n",
      "32720/32720 [==============================] - 2s 64us/sample - loss: 0.0967 - val_loss: 0.0656\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 2\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 7s 208us/sample - loss: 0.1789 - val_loss: 0.1322\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1468 - val_loss: 0.1181\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1378 - val_loss: 0.1091\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1310 - val_loss: 0.0983\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1240 - val_loss: 0.0945\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1202 - val_loss: 0.0921\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1181 - val_loss: 0.0935\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 74us/sample - loss: 0.1166 - val_loss: 0.0985\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1157 - val_loss: 0.0929\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1146 - val_loss: 0.0921\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1139 - val_loss: 0.0925\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1134 - val_loss: 0.0948\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1130 - val_loss: 0.0917\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1125 - val_loss: 0.0911\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1121 - val_loss: 0.0927\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1117 - val_loss: 0.0907\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1115 - val_loss: 0.0914\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1113 - val_loss: 0.0923\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1109 - val_loss: 0.0925\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1107 - val_loss: 0.0912\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1104 - val_loss: 0.0894\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1102 - val_loss: 0.0876\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1100 - val_loss: 0.0922\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1099 - val_loss: 0.0905\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1095 - val_loss: 0.0895\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1093 - val_loss: 0.0912\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1091 - val_loss: 0.0900\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1090 - val_loss: 0.0895\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.1087 - val_loss: 0.0896\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1086 - val_loss: 0.0906\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1085 - val_loss: 0.0862\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1083 - val_loss: 0.0855\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1081 - val_loss: 0.0864\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.1079 - val_loss: 0.0862\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 2s 60us/sample - loss: 0.1077 - val_loss: 0.0862\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1075 - val_loss: 0.0855\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1073 - val_loss: 0.0838\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1072 - val_loss: 0.0863\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1070 - val_loss: 0.0869\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1068 - val_loss: 0.0847\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1067 - val_loss: 0.0837\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1066 - val_loss: 0.0825\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1064 - val_loss: 0.0868\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1063 - val_loss: 0.0871\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1061 - val_loss: 0.0826\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 2s 63us/sample - loss: 0.1060 - val_loss: 0.0816\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1059 - val_loss: 0.0823\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1057 - val_loss: 0.0824\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1056 - val_loss: 0.0796\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1055 - val_loss: 0.0842\n",
      "Epoch 51/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1054 - val_loss: 0.0855\n",
      "Epoch 52/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1052 - val_loss: 0.0808\n",
      "Epoch 53/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1051 - val_loss: 0.0811\n",
      "Epoch 54/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1049 - val_loss: 0.0772\n",
      "Epoch 55/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1048 - val_loss: 0.0804\n",
      "Epoch 56/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1047 - val_loss: 0.0797\n",
      "Epoch 57/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1046 - val_loss: 0.0788\n",
      "Epoch 58/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1044 - val_loss: 0.0794\n",
      "Epoch 59/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1043 - val_loss: 0.0793\n",
      "Epoch 60/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1041 - val_loss: 0.0770\n",
      "Epoch 61/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1041 - val_loss: 0.0788\n",
      "Epoch 62/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1040 - val_loss: 0.0825\n",
      "Epoch 63/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1038 - val_loss: 0.0785\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1037 - val_loss: 0.0772\n",
      "Epoch 65/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1036 - val_loss: 0.0792\n",
      "Epoch 66/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1034 - val_loss: 0.0756\n",
      "Epoch 67/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1034 - val_loss: 0.0787\n",
      "Epoch 68/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1033 - val_loss: 0.0768\n",
      "Epoch 69/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1032 - val_loss: 0.0766\n",
      "Epoch 70/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1031 - val_loss: 0.0774\n",
      "Epoch 71/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1029 - val_loss: 0.0771\n",
      "Epoch 72/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1029 - val_loss: 0.0765\n",
      "Epoch 73/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1028 - val_loss: 0.0742\n",
      "Epoch 74/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1027 - val_loss: 0.0778\n",
      "Epoch 75/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1026 - val_loss: 0.0740\n",
      "Epoch 76/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1025 - val_loss: 0.0770\n",
      "Epoch 77/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1024 - val_loss: 0.0764\n",
      "Epoch 78/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1023 - val_loss: 0.0757\n",
      "Epoch 79/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1022 - val_loss: 0.0738\n",
      "Epoch 80/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1022 - val_loss: 0.0754\n",
      "Epoch 81/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1020 - val_loss: 0.0749\n",
      "Epoch 82/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1020 - val_loss: 0.0743\n",
      "Epoch 83/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1019 - val_loss: 0.0758\n",
      "Epoch 84/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1019 - val_loss: 0.0746\n",
      "Epoch 85/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1018 - val_loss: 0.0738\n",
      "Epoch 86/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1018 - val_loss: 0.0748\n",
      "Epoch 87/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1016 - val_loss: 0.0746\n",
      "Epoch 88/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1015 - val_loss: 0.0716\n",
      "Epoch 89/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1015 - val_loss: 0.0749\n",
      "Epoch 90/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1014 - val_loss: 0.0742\n",
      "Epoch 91/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1013 - val_loss: 0.0717\n",
      "Epoch 92/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1013 - val_loss: 0.0729\n",
      "Epoch 93/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1012 - val_loss: 0.0738\n",
      "Epoch 94/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1012 - val_loss: 0.0753\n",
      "Epoch 95/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1011 - val_loss: 0.0717\n",
      "Epoch 96/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1011 - val_loss: 0.0742\n",
      "Epoch 97/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1011 - val_loss: 0.0722\n",
      "Epoch 98/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1010 - val_loss: 0.0747\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 4\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 7s 209us/sample - loss: 0.1582 - val_loss: 0.1072\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1273 - val_loss: 0.1018\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1210 - val_loss: 0.1011\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1181 - val_loss: 0.0923\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1164 - val_loss: 0.0949\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1152 - val_loss: 0.0922\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.1142 - val_loss: 0.0955\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1135 - val_loss: 0.0975\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1132 - val_loss: 0.0965\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1125 - val_loss: 0.0940\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1119 - val_loss: 0.0916\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1115 - val_loss: 0.0928\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1114 - val_loss: 0.0889\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1110 - val_loss: 0.0928\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1106 - val_loss: 0.0928\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1102 - val_loss: 0.0920\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1100 - val_loss: 0.0931\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1097 - val_loss: 0.0890\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1095 - val_loss: 0.0921\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1093 - val_loss: 0.0899\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1089 - val_loss: 0.0865\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1088 - val_loss: 0.0856\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1086 - val_loss: 0.0915\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1084 - val_loss: 0.0911\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1081 - val_loss: 0.0879\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1079 - val_loss: 0.0894\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1077 - val_loss: 0.0933\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.1077 - val_loss: 0.0885\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.1075 - val_loss: 0.0874\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1072 - val_loss: 0.0863\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1072 - val_loss: 0.0848\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1069 - val_loss: 0.0848\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1068 - val_loss: 0.0847\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1067 - val_loss: 0.0875\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1064 - val_loss: 0.0860\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1062 - val_loss: 0.0879\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1060 - val_loss: 0.0866\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1060 - val_loss: 0.0833\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1059 - val_loss: 0.0845\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1057 - val_loss: 0.0849\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1055 - val_loss: 0.0847\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1055 - val_loss: 0.0826\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1054 - val_loss: 0.0857\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.1052 - val_loss: 0.0872\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1050 - val_loss: 0.0829\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1048 - val_loss: 0.0873\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1049 - val_loss: 0.0844\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1048 - val_loss: 0.0849\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1045 - val_loss: 0.0799\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1044 - val_loss: 0.0837\n",
      "Epoch 51/100\n",
      "32580/32580 [==============================] - 2s 73us/sample - loss: 0.1042 - val_loss: 0.0840\n",
      "Epoch 52/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1043 - val_loss: 0.0794\n",
      "Epoch 53/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1041 - val_loss: 0.0822\n",
      "Epoch 54/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1040 - val_loss: 0.0791\n",
      "Epoch 55/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1039 - val_loss: 0.0826\n",
      "Epoch 56/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1038 - val_loss: 0.0838\n",
      "Epoch 57/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1038 - val_loss: 0.0827\n",
      "Epoch 58/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1036 - val_loss: 0.0809\n",
      "Epoch 59/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1034 - val_loss: 0.0843\n",
      "Epoch 60/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1034 - val_loss: 0.0821\n",
      "Epoch 61/100\n",
      "32580/32580 [==============================] - 2s 74us/sample - loss: 0.1034 - val_loss: 0.0811\n",
      "Epoch 62/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1034 - val_loss: 0.0846\n",
      "Epoch 63/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1032 - val_loss: 0.0816\n",
      "Epoch 64/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1031 - val_loss: 0.0800\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 8\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 7s 204us/sample - loss: 0.1494 - val_loss: 0.1001\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1221 - val_loss: 0.0923\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1177 - val_loss: 0.1020\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1155 - val_loss: 0.0908\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1142 - val_loss: 0.0962\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1134 - val_loss: 0.0913\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1125 - val_loss: 0.0974\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1120 - val_loss: 0.0981\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1114 - val_loss: 0.0972\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1108 - val_loss: 0.0929\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1101 - val_loss: 0.0945\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1099 - val_loss: 0.0897\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1096 - val_loss: 0.0889\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1094 - val_loss: 0.0906\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1088 - val_loss: 0.0872\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1084 - val_loss: 0.0871\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1080 - val_loss: 0.0899\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1078 - val_loss: 0.0819\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1075 - val_loss: 0.0890\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1071 - val_loss: 0.0848\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1069 - val_loss: 0.0852\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1066 - val_loss: 0.0867\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1064 - val_loss: 0.0865\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1060 - val_loss: 0.0842\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1058 - val_loss: 0.0883\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1054 - val_loss: 0.0870\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1053 - val_loss: 0.0873\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 2s 60us/sample - loss: 0.1053 - val_loss: 0.0887\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 16\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 7s 221us/sample - loss: 0.1469 - val_loss: 0.1051\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1199 - val_loss: 0.0950\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1167 - val_loss: 0.1078\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1152 - val_loss: 0.0928\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1140 - val_loss: 0.0946\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.1130 - val_loss: 0.0925\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1124 - val_loss: 0.0946\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1118 - val_loss: 0.0939\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1110 - val_loss: 0.0987\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1106 - val_loss: 0.0887\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1100 - val_loss: 0.0914\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1096 - val_loss: 0.0851\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1090 - val_loss: 0.0885\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1084 - val_loss: 0.0924\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1079 - val_loss: 0.0818\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1074 - val_loss: 0.0835\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1066 - val_loss: 0.0873\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1063 - val_loss: 0.0778\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1060 - val_loss: 0.0864\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 2s 63us/sample - loss: 0.1055 - val_loss: 0.0821\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1052 - val_loss: 0.0833\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1048 - val_loss: 0.0834\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1045 - val_loss: 0.0878\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1042 - val_loss: 0.0799\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1038 - val_loss: 0.0850\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1033 - val_loss: 0.0804\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1031 - val_loss: 0.0816\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1036 - val_loss: 0.0854\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 32\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 7s 220us/sample - loss: 0.1429 - val_loss: 0.1008\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1189 - val_loss: 0.0979\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1167 - val_loss: 0.1073\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1152 - val_loss: 0.0934\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 74us/sample - loss: 0.1141 - val_loss: 0.0964\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1134 - val_loss: 0.0943\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1128 - val_loss: 0.0938\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1122 - val_loss: 0.0978\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1116 - val_loss: 0.0922\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 77us/sample - loss: 0.1112 - val_loss: 0.0878\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1108 - val_loss: 0.0950\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1097 - val_loss: 0.0891\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 73us/sample - loss: 0.1094 - val_loss: 0.0919\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1084 - val_loss: 0.0891\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1076 - val_loss: 0.0818\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1067 - val_loss: 0.0877\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1059 - val_loss: 0.0862\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 73us/sample - loss: 0.1052 - val_loss: 0.0807\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1046 - val_loss: 0.0794\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1040 - val_loss: 0.0811\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1033 - val_loss: 0.0762\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1030 - val_loss: 0.0769\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1023 - val_loss: 0.0838\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1017 - val_loss: 0.0774\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1017 - val_loss: 0.0809\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1009 - val_loss: 0.0791\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1002 - val_loss: 0.0766\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1002 - val_loss: 0.0793\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1000 - val_loss: 0.0741\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0995 - val_loss: 0.0701\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0990 - val_loss: 0.0748\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.0988 - val_loss: 0.0693\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0984 - val_loss: 0.0749\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0989 - val_loss: 0.0720\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0982 - val_loss: 0.0696\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.0980 - val_loss: 0.0720\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0979 - val_loss: 0.0767\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0984 - val_loss: 0.0732\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.0975 - val_loss: 0.0757\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0976 - val_loss: 0.0681\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0973 - val_loss: 0.0713\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0969 - val_loss: 0.0779\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.0967 - val_loss: 0.0788\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0965 - val_loss: 0.0653\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0966 - val_loss: 0.0672\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0961 - val_loss: 0.0699\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0960 - val_loss: 0.0719\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0959 - val_loss: 0.0739\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.0958 - val_loss: 0.0724\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0956 - val_loss: 0.0689\n",
      "Epoch 51/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.0952 - val_loss: 0.0658\n",
      "Epoch 52/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0954 - val_loss: 0.0718\n",
      "Epoch 53/100\n",
      "32580/32580 [==============================] - 2s 63us/sample - loss: 0.0952 - val_loss: 0.0641\n",
      "Epoch 54/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0949 - val_loss: 0.0632\n",
      "Epoch 55/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0955 - val_loss: 0.0688\n",
      "Epoch 56/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0948 - val_loss: 0.0645\n",
      "Epoch 57/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0945 - val_loss: 0.0634\n",
      "Epoch 58/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0947 - val_loss: 0.0643\n",
      "Epoch 59/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0944 - val_loss: 0.0701\n",
      "Epoch 60/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.0942 - val_loss: 0.0721\n",
      "Epoch 61/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0943 - val_loss: 0.0702\n",
      "Epoch 62/100\n",
      "32580/32580 [==============================] - 2s 60us/sample - loss: 0.0945 - val_loss: 0.0683\n",
      "Epoch 63/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0941 - val_loss: 0.0648\n",
      "Epoch 64/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0936 - val_loss: 0.0647\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 64\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 8s 232us/sample - loss: 0.1394 - val_loss: 0.0957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1181 - val_loss: 0.0969\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.1161 - val_loss: 0.1060\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1156 - val_loss: 0.1012\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1142 - val_loss: 0.0972\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1131 - val_loss: 0.0868\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1128 - val_loss: 0.0955\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1118 - val_loss: 0.0997\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1112 - val_loss: 0.0915\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1107 - val_loss: 0.0826\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1096 - val_loss: 0.0900\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1087 - val_loss: 0.0880\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1087 - val_loss: 0.0891\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.1068 - val_loss: 0.0870\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1062 - val_loss: 0.0790\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1048 - val_loss: 0.0832\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1038 - val_loss: 0.0855\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.1029 - val_loss: 0.0751\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1024 - val_loss: 0.0747\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1019 - val_loss: 0.0762\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.1008 - val_loss: 0.0728\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1004 - val_loss: 0.0768\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1000 - val_loss: 0.0736\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0991 - val_loss: 0.0771\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0991 - val_loss: 0.0748\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0984 - val_loss: 0.0718\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0978 - val_loss: 0.0766\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0980 - val_loss: 0.0735\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.0975 - val_loss: 0.0659\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0970 - val_loss: 0.0635\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.0966 - val_loss: 0.0671\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0964 - val_loss: 0.0686\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0960 - val_loss: 0.0661\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0959 - val_loss: 0.0664\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0956 - val_loss: 0.0647\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0952 - val_loss: 0.0691\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0948 - val_loss: 0.0653\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0947 - val_loss: 0.0647\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0946 - val_loss: 0.0617\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0946 - val_loss: 0.0622\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0943 - val_loss: 0.0628\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0938 - val_loss: 0.0647\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0937 - val_loss: 0.0732\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0939 - val_loss: 0.0606\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0935 - val_loss: 0.0654\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0932 - val_loss: 0.0624\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0931 - val_loss: 0.0666\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 2s 73us/sample - loss: 0.0931 - val_loss: 0.0673\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0931 - val_loss: 0.0644\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0928 - val_loss: 0.0636\n",
      "Epoch 51/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0926 - val_loss: 0.0619\n",
      "Epoch 52/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.0927 - val_loss: 0.0692\n",
      "Epoch 53/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0929 - val_loss: 0.0614\n",
      "Epoch 54/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0925 - val_loss: 0.0602\n",
      "Epoch 55/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0926 - val_loss: 0.0639\n",
      "Epoch 56/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0923 - val_loss: 0.0590\n",
      "Epoch 57/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0919 - val_loss: 0.0598\n",
      "Epoch 58/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0921 - val_loss: 0.0629\n",
      "Epoch 59/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.0921 - val_loss: 0.0634\n",
      "Epoch 60/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.0918 - val_loss: 0.0640\n",
      "Epoch 61/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0920 - val_loss: 0.0637\n",
      "Epoch 62/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0919 - val_loss: 0.0629\n",
      "Epoch 63/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0919 - val_loss: 0.0663\n",
      "Epoch 64/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.0913 - val_loss: 0.0629\n",
      "Epoch 65/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0911 - val_loss: 0.0577\n",
      "Epoch 66/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0911 - val_loss: 0.0605\n",
      "Epoch 67/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0914 - val_loss: 0.0607\n",
      "Epoch 68/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0910 - val_loss: 0.0659\n",
      "Epoch 69/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0908 - val_loss: 0.0610\n",
      "Epoch 70/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0908 - val_loss: 0.0656\n",
      "Epoch 71/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0908 - val_loss: 0.0608\n",
      "Epoch 72/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0909 - val_loss: 0.0597\n",
      "Epoch 73/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0905 - val_loss: 0.0596\n",
      "Epoch 74/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0905 - val_loss: 0.0592\n",
      "Epoch 75/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0904 - val_loss: 0.0595\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 128\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32580/32580 [==============================] - 7s 226us/sample - loss: 0.1354 - val_loss: 0.0936\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1187 - val_loss: 0.1024\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 64us/sample - loss: 0.1160 - val_loss: 0.1102\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 61us/sample - loss: 0.1158 - val_loss: 0.0923\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.1149 - val_loss: 0.0971\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 62us/sample - loss: 0.1138 - val_loss: 0.0926\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1132 - val_loss: 0.0967\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 73us/sample - loss: 0.1126 - val_loss: 0.0943\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 65us/sample - loss: 0.1115 - val_loss: 0.0902\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1113 - val_loss: 0.0872\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.1100 - val_loss: 0.0939\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1093 - val_loss: 0.0829\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.1095 - val_loss: 0.0916\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.1068 - val_loss: 0.0806\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.1057 - val_loss: 0.0801\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 75us/sample - loss: 0.1045 - val_loss: 0.0771\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1029 - val_loss: 0.0849\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1016 - val_loss: 0.0689\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.1009 - val_loss: 0.0699\n",
      "Epoch 20/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0999 - val_loss: 0.0696\n",
      "Epoch 21/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0984 - val_loss: 0.0657\n",
      "Epoch 22/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0975 - val_loss: 0.0731\n",
      "Epoch 23/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0969 - val_loss: 0.0693\n",
      "Epoch 24/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0961 - val_loss: 0.0673\n",
      "Epoch 25/100\n",
      "32580/32580 [==============================] - 2s 74us/sample - loss: 0.0956 - val_loss: 0.0607\n",
      "Epoch 26/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0950 - val_loss: 0.0657\n",
      "Epoch 27/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0948 - val_loss: 0.0763\n",
      "Epoch 28/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0945 - val_loss: 0.0655\n",
      "Epoch 29/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0943 - val_loss: 0.0647\n",
      "Epoch 30/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0940 - val_loss: 0.0586\n",
      "Epoch 31/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0938 - val_loss: 0.0623\n",
      "Epoch 32/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0938 - val_loss: 0.0633\n",
      "Epoch 33/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0934 - val_loss: 0.0605\n",
      "Epoch 34/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0931 - val_loss: 0.0586\n",
      "Epoch 35/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0930 - val_loss: 0.0600\n",
      "Epoch 36/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0929 - val_loss: 0.0623\n",
      "Epoch 37/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0926 - val_loss: 0.0599\n",
      "Epoch 38/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0924 - val_loss: 0.0586\n",
      "Epoch 39/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0927 - val_loss: 0.0620\n",
      "Epoch 40/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0924 - val_loss: 0.0615\n",
      "Epoch 41/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0921 - val_loss: 0.0589\n",
      "Epoch 42/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0920 - val_loss: 0.0613\n",
      "Epoch 43/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0918 - val_loss: 0.0642\n",
      "Epoch 44/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0918 - val_loss: 0.0579\n",
      "Epoch 45/100\n",
      "32580/32580 [==============================] - 2s 71us/sample - loss: 0.0917 - val_loss: 0.0605\n",
      "Epoch 46/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0914 - val_loss: 0.0638\n",
      "Epoch 47/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0913 - val_loss: 0.0613\n",
      "Epoch 48/100\n",
      "32580/32580 [==============================] - 2s 68us/sample - loss: 0.0913 - val_loss: 0.0639\n",
      "Epoch 49/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0910 - val_loss: 0.0610\n",
      "Epoch 50/100\n",
      "32580/32580 [==============================] - 2s 70us/sample - loss: 0.0909 - val_loss: 0.0614\n",
      "Epoch 51/100\n",
      "32580/32580 [==============================] - 2s 66us/sample - loss: 0.0907 - val_loss: 0.0626\n",
      "Epoch 52/100\n",
      "32580/32580 [==============================] - 2s 67us/sample - loss: 0.0908 - val_loss: 0.0627\n",
      "Epoch 53/100\n",
      "32580/32580 [==============================] - 2s 69us/sample - loss: 0.0909 - val_loss: 0.0580\n",
      "Epoch 54/100\n",
      "32580/32580 [==============================] - 2s 72us/sample - loss: 0.0909 - val_loss: 0.0584\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 2\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 7s 221us/sample - loss: 0.1709 - val_loss: 0.1171\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1392 - val_loss: 0.1098\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1301 - val_loss: 0.1088\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1264 - val_loss: 0.1070\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1245 - val_loss: 0.1090\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1231 - val_loss: 0.1057\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1222 - val_loss: 0.1057\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1213 - val_loss: 0.1046\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1207 - val_loss: 0.1062\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1202 - val_loss: 0.1057\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1198 - val_loss: 0.1031\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1193 - val_loss: 0.1040\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1190 - val_loss: 0.1033\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1188 - val_loss: 0.1062\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1185 - val_loss: 0.1046\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1182 - val_loss: 0.1027\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1180 - val_loss: 0.1055\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1177 - val_loss: 0.1005\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1175 - val_loss: 0.1048\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1173 - val_loss: 0.1031\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1171 - val_loss: 0.1045\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1169 - val_loss: 0.1029\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1168 - val_loss: 0.1039\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1166 - val_loss: 0.1031\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1164 - val_loss: 0.1030\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1162 - val_loss: 0.1000\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1161 - val_loss: 0.1018\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1159 - val_loss: 0.0987\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1157 - val_loss: 0.1013\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1155 - val_loss: 0.0995\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1154 - val_loss: 0.1037\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.1153 - val_loss: 0.1028\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.1150 - val_loss: 0.1002\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 60us/sample - loss: 0.1149 - val_loss: 0.1016\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.1147 - val_loss: 0.0982\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 2s 59us/sample - loss: 0.1146 - val_loss: 0.0992\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 2s 61us/sample - loss: 0.1144 - val_loss: 0.1028\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1142 - val_loss: 0.0986\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1140 - val_loss: 0.0987\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1138 - val_loss: 0.0972\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1137 - val_loss: 0.0992\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1135 - val_loss: 0.1009\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1133 - val_loss: 0.1003\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1132 - val_loss: 0.0983\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1131 - val_loss: 0.1009\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1128 - val_loss: 0.0991\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1128 - val_loss: 0.0986\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1126 - val_loss: 0.1003\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1124 - val_loss: 0.0983\n",
      "Epoch 50/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1123 - val_loss: 0.0997\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 4\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 7s 201us/sample - loss: 0.1706 - val_loss: 0.1186\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 62us/sample - loss: 0.1351 - val_loss: 0.1066\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1277 - val_loss: 0.1052\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1248 - val_loss: 0.1071\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1232 - val_loss: 0.1054\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1219 - val_loss: 0.1053\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1211 - val_loss: 0.1071\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1202 - val_loss: 0.1013\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1196 - val_loss: 0.1042\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1191 - val_loss: 0.1031\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1187 - val_loss: 0.1004\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1182 - val_loss: 0.1030\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1178 - val_loss: 0.1001\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1176 - val_loss: 0.1035\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1172 - val_loss: 0.1050\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1169 - val_loss: 0.0984\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1167 - val_loss: 0.1024\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1165 - val_loss: 0.0993\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1160 - val_loss: 0.1006\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1159 - val_loss: 0.1013\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1156 - val_loss: 0.1017\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1154 - val_loss: 0.0993\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1153 - val_loss: 0.0989\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1150 - val_loss: 0.1000\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1147 - val_loss: 0.0970\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1146 - val_loss: 0.0965\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1144 - val_loss: 0.1034\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1142 - val_loss: 0.0985\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1141 - val_loss: 0.1001\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 60us/sample - loss: 0.1139 - val_loss: 0.0975\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1136 - val_loss: 0.1051\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1134 - val_loss: 0.0948\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1132 - val_loss: 0.0991\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1131 - val_loss: 0.0967\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1128 - val_loss: 0.0937\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1128 - val_loss: 0.0937\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1126 - val_loss: 0.1002\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1122 - val_loss: 0.1000\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1121 - val_loss: 0.0936\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1118 - val_loss: 0.0933\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1116 - val_loss: 0.0943\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1114 - val_loss: 0.0923\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1113 - val_loss: 0.0936\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1111 - val_loss: 0.0951\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1108 - val_loss: 0.0949\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1107 - val_loss: 0.0915\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1105 - val_loss: 0.0945\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1102 - val_loss: 0.0946\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1101 - val_loss: 0.0930\n",
      "Epoch 50/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1099 - val_loss: 0.0930\n",
      "Epoch 51/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1098 - val_loss: 0.0951\n",
      "Epoch 52/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1095 - val_loss: 0.0891\n",
      "Epoch 53/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1094 - val_loss: 0.0928\n",
      "Epoch 54/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1094 - val_loss: 0.0905\n",
      "Epoch 55/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1090 - val_loss: 0.0901\n",
      "Epoch 56/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1090 - val_loss: 0.0923\n",
      "Epoch 57/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1088 - val_loss: 0.0893\n",
      "Epoch 58/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1087 - val_loss: 0.0899\n",
      "Epoch 59/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1085 - val_loss: 0.0898\n",
      "Epoch 60/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1083 - val_loss: 0.0914\n",
      "Epoch 61/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1081 - val_loss: 0.0917\n",
      "Epoch 62/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1081 - val_loss: 0.0927\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 8\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 7s 205us/sample - loss: 0.1628 - val_loss: 0.1082\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 62us/sample - loss: 0.1277 - val_loss: 0.1081\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1230 - val_loss: 0.1065\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1209 - val_loss: 0.1059\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1197 - val_loss: 0.1027\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1190 - val_loss: 0.1058\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1185 - val_loss: 0.1028\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1178 - val_loss: 0.1016\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1172 - val_loss: 0.1023\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1168 - val_loss: 0.1013\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1166 - val_loss: 0.0983\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1161 - val_loss: 0.1039\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1158 - val_loss: 0.1009\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1156 - val_loss: 0.1034\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1152 - val_loss: 0.1022\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1148 - val_loss: 0.0958\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1147 - val_loss: 0.1014\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1144 - val_loss: 0.0982\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1141 - val_loss: 0.0990\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1138 - val_loss: 0.1021\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1135 - val_loss: 0.0974\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1134 - val_loss: 0.0990\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1132 - val_loss: 0.0963\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1130 - val_loss: 0.1017\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1127 - val_loss: 0.0953\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1126 - val_loss: 0.0953\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1123 - val_loss: 0.1023\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1120 - val_loss: 0.0994\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1119 - val_loss: 0.0997\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1117 - val_loss: 0.0971\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1114 - val_loss: 0.1032\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1113 - val_loss: 0.0936\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1110 - val_loss: 0.0957\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1110 - val_loss: 0.0948\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1108 - val_loss: 0.0919\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1106 - val_loss: 0.0931\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1104 - val_loss: 0.1023\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1103 - val_loss: 0.0974\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1101 - val_loss: 0.0909\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1100 - val_loss: 0.0950\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1098 - val_loss: 0.0928\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1096 - val_loss: 0.0937\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1095 - val_loss: 0.0924\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1093 - val_loss: 0.0935\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1093 - val_loss: 0.0946\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1090 - val_loss: 0.0920\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1089 - val_loss: 0.0931\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1088 - val_loss: 0.0951\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1086 - val_loss: 0.0925\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 16\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 8s 228us/sample - loss: 0.1581 - val_loss: 0.1054\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1252 - val_loss: 0.1106\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1215 - val_loss: 0.1131\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1199 - val_loss: 0.1039\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1188 - val_loss: 0.1038\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1181 - val_loss: 0.1090\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1177 - val_loss: 0.0984\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1170 - val_loss: 0.0974\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1166 - val_loss: 0.1035\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1160 - val_loss: 0.1025\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1156 - val_loss: 0.1031\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1153 - val_loss: 0.1064\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1151 - val_loss: 0.1060\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1148 - val_loss: 0.1047\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1144 - val_loss: 0.1019\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1141 - val_loss: 0.0991\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1138 - val_loss: 0.0991\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1135 - val_loss: 0.0960\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1133 - val_loss: 0.0927\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1130 - val_loss: 0.1042\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1126 - val_loss: 0.0926\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1124 - val_loss: 0.0950\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1122 - val_loss: 0.0964\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1117 - val_loss: 0.0974\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1114 - val_loss: 0.0946\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1112 - val_loss: 0.0922\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1108 - val_loss: 0.0958\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1106 - val_loss: 0.1003\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1102 - val_loss: 0.0996\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1100 - val_loss: 0.0917\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1096 - val_loss: 0.0966\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1093 - val_loss: 0.0854\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1090 - val_loss: 0.0923\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1089 - val_loss: 0.0899\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1084 - val_loss: 0.0862\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1083 - val_loss: 0.0901\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1080 - val_loss: 0.1011\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1076 - val_loss: 0.0925\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1074 - val_loss: 0.0864\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 2s 62us/sample - loss: 0.1072 - val_loss: 0.0893\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1067 - val_loss: 0.0869\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1065 - val_loss: 0.0864\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 32\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 8s 228us/sample - loss: 0.1460 - val_loss: 0.1079\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1238 - val_loss: 0.1107\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1210 - val_loss: 0.1172\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1201 - val_loss: 0.1049\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1193 - val_loss: 0.1088\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1192 - val_loss: 0.1144\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1187 - val_loss: 0.1023\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1180 - val_loss: 0.0988\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1176 - val_loss: 0.1035\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1172 - val_loss: 0.1021\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1164 - val_loss: 0.1099\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1162 - val_loss: 0.1031\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1156 - val_loss: 0.1130\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1157 - val_loss: 0.1009\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1149 - val_loss: 0.1038\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1144 - val_loss: 0.0995\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1142 - val_loss: 0.0997\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1141 - val_loss: 0.0947\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1137 - val_loss: 0.0936\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1134 - val_loss: 0.1096\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1128 - val_loss: 0.0921\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1125 - val_loss: 0.0918\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1123 - val_loss: 0.0971\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1116 - val_loss: 0.0991\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1110 - val_loss: 0.0954\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 62us/sample - loss: 0.1107 - val_loss: 0.0927\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1101 - val_loss: 0.0971\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1097 - val_loss: 0.0959\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1092 - val_loss: 0.0952\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 62us/sample - loss: 0.1087 - val_loss: 0.0873\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1076 - val_loss: 0.0951\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1069 - val_loss: 0.0823\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1062 - val_loss: 0.0906\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1059 - val_loss: 0.0852\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1052 - val_loss: 0.0831\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1047 - val_loss: 0.0832\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1042 - val_loss: 0.0946\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1036 - val_loss: 0.0921\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1033 - val_loss: 0.0860\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1029 - val_loss: 0.0787\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1029 - val_loss: 0.0850\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1023 - val_loss: 0.0845\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1020 - val_loss: 0.0831\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1023 - val_loss: 0.0812\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1018 - val_loss: 0.0857\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1014 - val_loss: 0.0800\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1011 - val_loss: 0.0837\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1009 - val_loss: 0.0818\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1008 - val_loss: 0.0766\n",
      "Epoch 50/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1005 - val_loss: 0.0754\n",
      "Epoch 51/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1004 - val_loss: 0.0781\n",
      "Epoch 52/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1002 - val_loss: 0.0792\n",
      "Epoch 53/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.0999 - val_loss: 0.0751\n",
      "Epoch 54/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1000 - val_loss: 0.0755\n",
      "Epoch 55/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.0998 - val_loss: 0.0759\n",
      "Epoch 56/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.0997 - val_loss: 0.0775\n",
      "Epoch 57/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.0997 - val_loss: 0.0765\n",
      "Epoch 58/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.0993 - val_loss: 0.0794\n",
      "Epoch 59/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.0993 - val_loss: 0.0785\n",
      "Epoch 60/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.0989 - val_loss: 0.0793\n",
      "Epoch 61/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.0991 - val_loss: 0.0753\n",
      "Epoch 62/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.0989 - val_loss: 0.0749\n",
      "Epoch 63/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.0986 - val_loss: 0.0766\n",
      "Epoch 64/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.0984 - val_loss: 0.0740\n",
      "Epoch 65/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.0985 - val_loss: 0.0741\n",
      "Epoch 66/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.0984 - val_loss: 0.0725\n",
      "Epoch 67/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.0984 - val_loss: 0.0807\n",
      "Epoch 68/100\n",
      "33340/33340 [==============================] - 2s 60us/sample - loss: 0.0985 - val_loss: 0.0752\n",
      "Epoch 69/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.0979 - val_loss: 0.0763\n",
      "Epoch 70/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.0979 - val_loss: 0.0777\n",
      "Epoch 71/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.0981 - val_loss: 0.0803\n",
      "Epoch 72/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.0978 - val_loss: 0.0792\n",
      "Epoch 73/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.0977 - val_loss: 0.0739\n",
      "Epoch 74/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.0979 - val_loss: 0.0732\n",
      "Epoch 75/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.0976 - val_loss: 0.0764\n",
      "Epoch 76/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.0975 - val_loss: 0.0726\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 64\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 7s 201us/sample - loss: 0.1429 - val_loss: 0.1024\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1242 - val_loss: 0.1064\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1213 - val_loss: 0.1078\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 60us/sample - loss: 0.1205 - val_loss: 0.1018\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.1193 - val_loss: 0.1109\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1185 - val_loss: 0.1162\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.1185 - val_loss: 0.1015\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.1180 - val_loss: 0.0961\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 60us/sample - loss: 0.1174 - val_loss: 0.1005\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 60us/sample - loss: 0.1166 - val_loss: 0.0990\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1160 - val_loss: 0.1041\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1159 - val_loss: 0.1000\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1150 - val_loss: 0.1087\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 74us/sample - loss: 0.1153 - val_loss: 0.0964\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1141 - val_loss: 0.1003\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1132 - val_loss: 0.0961\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1127 - val_loss: 0.0979\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1122 - val_loss: 0.0944\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1117 - val_loss: 0.0876\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1109 - val_loss: 0.1006\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1102 - val_loss: 0.0918\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1095 - val_loss: 0.0890\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1089 - val_loss: 0.0915\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1079 - val_loss: 0.0869\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1068 - val_loss: 0.0851\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1067 - val_loss: 0.0825\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1058 - val_loss: 0.0930\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1056 - val_loss: 0.0865\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1047 - val_loss: 0.0852\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1044 - val_loss: 0.0819\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1038 - val_loss: 0.0834\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1034 - val_loss: 0.0773\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1029 - val_loss: 0.0835\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1026 - val_loss: 0.0823\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 2s 62us/sample - loss: 0.1024 - val_loss: 0.0802\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1020 - val_loss: 0.0738\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1017 - val_loss: 0.0845\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1014 - val_loss: 0.0871\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1017 - val_loss: 0.0754\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1011 - val_loss: 0.0755\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1009 - val_loss: 0.0809\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1005 - val_loss: 0.0804\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1002 - val_loss: 0.0771\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.0999 - val_loss: 0.0795\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.0998 - val_loss: 0.0775\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.0998 - val_loss: 0.0739\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 128\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 7s 202us/sample - loss: 0.1414 - val_loss: 0.1013\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1244 - val_loss: 0.1076\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1219 - val_loss: 0.1107\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1210 - val_loss: 0.1042\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1193 - val_loss: 0.1125\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 63us/sample - loss: 0.1182 - val_loss: 0.1198\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1183 - val_loss: 0.1000\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1176 - val_loss: 0.0958\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1167 - val_loss: 0.1026\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 65us/sample - loss: 0.1158 - val_loss: 0.0975\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1152 - val_loss: 0.1039\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1148 - val_loss: 0.0945\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1138 - val_loss: 0.1019\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1136 - val_loss: 0.0949\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1121 - val_loss: 0.0946\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1108 - val_loss: 0.0905\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 64us/sample - loss: 0.1099 - val_loss: 0.0942\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1085 - val_loss: 0.0899\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1079 - val_loss: 0.0843\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1065 - val_loss: 0.0891\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.1057 - val_loss: 0.0843\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1054 - val_loss: 0.0823\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1047 - val_loss: 0.0858\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.1037 - val_loss: 0.0797\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1033 - val_loss: 0.0777\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1031 - val_loss: 0.0739\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.1024 - val_loss: 0.0838\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.1022 - val_loss: 0.0781\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 2s 73us/sample - loss: 0.1016 - val_loss: 0.0833\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.1015 - val_loss: 0.0765\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.1006 - val_loss: 0.0803\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1004 - val_loss: 0.0748\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.1003 - val_loss: 0.0777\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 72us/sample - loss: 0.0998 - val_loss: 0.0743\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.0997 - val_loss: 0.0723\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.0997 - val_loss: 0.0710\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 2s 71us/sample - loss: 0.0991 - val_loss: 0.0778\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.0987 - val_loss: 0.0792\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.0985 - val_loss: 0.0730\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 2s 68us/sample - loss: 0.0979 - val_loss: 0.0735\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.0979 - val_loss: 0.0748\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 2s 69us/sample - loss: 0.0976 - val_loss: 0.0749\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 2s 70us/sample - loss: 0.0975 - val_loss: 0.0723\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.0973 - val_loss: 0.0725\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 2s 66us/sample - loss: 0.0972 - val_loss: 0.0742\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 2s 67us/sample - loss: 0.0969 - val_loss: 0.0773\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 2\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 7s 226us/sample - loss: 0.1738 - val_loss: 0.1210\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1383 - val_loss: 0.1074\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1288 - val_loss: 0.1001\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1243 - val_loss: 0.0992\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 73us/sample - loss: 0.1215 - val_loss: 0.1029\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1195 - val_loss: 0.1037\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1180 - val_loss: 0.0995\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1168 - val_loss: 0.0987\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1159 - val_loss: 0.0990\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1150 - val_loss: 0.0968\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1145 - val_loss: 0.1026\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1138 - val_loss: 0.0983\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1134 - val_loss: 0.1011\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1130 - val_loss: 0.0983\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1127 - val_loss: 0.1013\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1124 - val_loss: 0.0989\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1120 - val_loss: 0.0958\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1117 - val_loss: 0.1026\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1113 - val_loss: 0.0979\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1113 - val_loss: 0.0987\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1108 - val_loss: 0.0971\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1106 - val_loss: 0.0991\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1105 - val_loss: 0.0967\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1101 - val_loss: 0.0938\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1100 - val_loss: 0.0971\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1098 - val_loss: 0.0971\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1095 - val_loss: 0.0961\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1093 - val_loss: 0.1035\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1092 - val_loss: 0.0973\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1089 - val_loss: 0.0978\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1088 - val_loss: 0.0952\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1086 - val_loss: 0.1011\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1084 - val_loss: 0.0972\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1083 - val_loss: 0.0925\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1080 - val_loss: 0.0952\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1079 - val_loss: 0.0942\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1077 - val_loss: 0.0957\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1075 - val_loss: 0.0939\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1073 - val_loss: 0.0947\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1072 - val_loss: 0.0912\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1070 - val_loss: 0.0954\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1068 - val_loss: 0.0975\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1067 - val_loss: 0.0949\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1065 - val_loss: 0.0969\n",
      "Epoch 45/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1063 - val_loss: 0.0894\n",
      "Epoch 46/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1061 - val_loss: 0.0907\n",
      "Epoch 47/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1059 - val_loss: 0.0933\n",
      "Epoch 48/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1058 - val_loss: 0.0928\n",
      "Epoch 49/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1058 - val_loss: 0.0929\n",
      "Epoch 50/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1055 - val_loss: 0.0899\n",
      "Epoch 51/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1053 - val_loss: 0.0910\n",
      "Epoch 52/100\n",
      "33200/33200 [==============================] - 3s 76us/sample - loss: 0.1051 - val_loss: 0.0921\n",
      "Epoch 53/100\n",
      "33200/33200 [==============================] - 3s 75us/sample - loss: 0.1050 - val_loss: 0.0922\n",
      "Epoch 54/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1048 - val_loss: 0.0893\n",
      "Epoch 55/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1047 - val_loss: 0.0898\n",
      "Epoch 56/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1046 - val_loss: 0.0927\n",
      "Epoch 57/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1043 - val_loss: 0.0927\n",
      "Epoch 58/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1042 - val_loss: 0.0883\n",
      "Epoch 59/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1040 - val_loss: 0.0872\n",
      "Epoch 60/100\n",
      "33200/33200 [==============================] - 2s 73us/sample - loss: 0.1039 - val_loss: 0.0921\n",
      "Epoch 61/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1037 - val_loss: 0.0879\n",
      "Epoch 62/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1035 - val_loss: 0.0873\n",
      "Epoch 63/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1034 - val_loss: 0.0878\n",
      "Epoch 64/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1033 - val_loss: 0.0903\n",
      "Epoch 65/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1031 - val_loss: 0.0898\n",
      "Epoch 66/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1029 - val_loss: 0.0888\n",
      "Epoch 67/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1028 - val_loss: 0.0878\n",
      "Epoch 68/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1026 - val_loss: 0.0858\n",
      "Epoch 69/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1025 - val_loss: 0.0846\n",
      "Epoch 70/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1024 - val_loss: 0.0879\n",
      "Epoch 71/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1022 - val_loss: 0.0849\n",
      "Epoch 72/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1021 - val_loss: 0.0869\n",
      "Epoch 73/100\n",
      "33200/33200 [==============================] - 2s 75us/sample - loss: 0.1020 - val_loss: 0.0861\n",
      "Epoch 74/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1019 - val_loss: 0.0882\n",
      "Epoch 75/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1018 - val_loss: 0.0862\n",
      "Epoch 76/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1016 - val_loss: 0.0852\n",
      "Epoch 77/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1015 - val_loss: 0.0858\n",
      "Epoch 78/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1014 - val_loss: 0.0837\n",
      "Epoch 79/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1013 - val_loss: 0.0845\n",
      "Epoch 80/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1012 - val_loss: 0.0818\n",
      "Epoch 81/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1011 - val_loss: 0.0858\n",
      "Epoch 82/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1009 - val_loss: 0.0845\n",
      "Epoch 83/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1009 - val_loss: 0.0829\n",
      "Epoch 84/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1008 - val_loss: 0.0845\n",
      "Epoch 85/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1007 - val_loss: 0.0847\n",
      "Epoch 86/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1007 - val_loss: 0.0850\n",
      "Epoch 87/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1005 - val_loss: 0.0822\n",
      "Epoch 88/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1005 - val_loss: 0.0823\n",
      "Epoch 89/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1004 - val_loss: 0.0857\n",
      "Epoch 90/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1004 - val_loss: 0.0861\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 4\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 7s 200us/sample - loss: 0.1731 - val_loss: 0.1086\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1295 - val_loss: 0.1032\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1221 - val_loss: 0.1017\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1192 - val_loss: 0.1025\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1175 - val_loss: 0.1009\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1165 - val_loss: 0.1074\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1154 - val_loss: 0.1042\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1143 - val_loss: 0.0985\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1135 - val_loss: 0.0976\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1127 - val_loss: 0.0959\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1120 - val_loss: 0.1036\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1113 - val_loss: 0.1003\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1106 - val_loss: 0.1014\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1103 - val_loss: 0.1000\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1096 - val_loss: 0.0952\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1092 - val_loss: 0.0945\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1090 - val_loss: 0.0899\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1084 - val_loss: 0.1000\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1079 - val_loss: 0.0955\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1077 - val_loss: 0.0944\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1071 - val_loss: 0.0946\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1066 - val_loss: 0.0954\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1064 - val_loss: 0.0918\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1060 - val_loss: 0.0928\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1057 - val_loss: 0.0925\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1055 - val_loss: 0.0898\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1051 - val_loss: 0.0892\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1049 - val_loss: 0.0969\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1046 - val_loss: 0.0886\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1044 - val_loss: 0.0883\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1042 - val_loss: 0.0856\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1040 - val_loss: 0.0910\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1039 - val_loss: 0.0898\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1038 - val_loss: 0.0845\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1035 - val_loss: 0.0896\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1035 - val_loss: 0.0844\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1033 - val_loss: 0.0869\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1031 - val_loss: 0.0854\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1029 - val_loss: 0.0881\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1028 - val_loss: 0.0815\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1027 - val_loss: 0.0875\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1025 - val_loss: 0.0914\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1026 - val_loss: 0.0826\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1024 - val_loss: 0.0870\n",
      "Epoch 45/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1022 - val_loss: 0.0835\n",
      "Epoch 46/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1022 - val_loss: 0.0820\n",
      "Epoch 47/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1020 - val_loss: 0.0831\n",
      "Epoch 48/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1019 - val_loss: 0.0873\n",
      "Epoch 49/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1019 - val_loss: 0.0809\n",
      "Epoch 50/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1018 - val_loss: 0.0842\n",
      "Epoch 51/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1016 - val_loss: 0.0862\n",
      "Epoch 52/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1016 - val_loss: 0.0867\n",
      "Epoch 53/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1014 - val_loss: 0.0851\n",
      "Epoch 54/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1014 - val_loss: 0.0822\n",
      "Epoch 55/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1014 - val_loss: 0.0816\n",
      "Epoch 56/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1014 - val_loss: 0.0862\n",
      "Epoch 57/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1011 - val_loss: 0.0890\n",
      "Epoch 58/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1012 - val_loss: 0.0815\n",
      "Epoch 59/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1009 - val_loss: 0.0813\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 8\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 7s 204us/sample - loss: 0.1566 - val_loss: 0.1038\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1225 - val_loss: 0.1118\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1174 - val_loss: 0.0962\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1151 - val_loss: 0.0947\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1137 - val_loss: 0.1063\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1123 - val_loss: 0.1043\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1116 - val_loss: 0.0970\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1113 - val_loss: 0.0945\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1107 - val_loss: 0.1032\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1100 - val_loss: 0.0903\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1100 - val_loss: 0.0990\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1091 - val_loss: 0.1018\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 73us/sample - loss: 0.1089 - val_loss: 0.1000\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1085 - val_loss: 0.1002\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1080 - val_loss: 0.0902\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1079 - val_loss: 0.0903\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1076 - val_loss: 0.0893\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1071 - val_loss: 0.0980\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1067 - val_loss: 0.1015\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1068 - val_loss: 0.0926\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1062 - val_loss: 0.0924\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1059 - val_loss: 0.1012\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1059 - val_loss: 0.0944\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1055 - val_loss: 0.0925\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1050 - val_loss: 0.0930\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1048 - val_loss: 0.0915\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1046 - val_loss: 0.0881\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 2s 75us/sample - loss: 0.1044 - val_loss: 0.0878\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1043 - val_loss: 0.0878\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 2s 72us/sample - loss: 0.1039 - val_loss: 0.0904\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 2s 71us/sample - loss: 0.1038 - val_loss: 0.0874\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1034 - val_loss: 0.0878\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1034 - val_loss: 0.0933\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1033 - val_loss: 0.0783\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1033 - val_loss: 0.0916\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1027 - val_loss: 0.0888\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1026 - val_loss: 0.0896\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1025 - val_loss: 0.0825\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 2s 73us/sample - loss: 0.1021 - val_loss: 0.0898\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.1021 - val_loss: 0.0851\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1017 - val_loss: 0.0897\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1018 - val_loss: 0.0859\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.1014 - val_loss: 0.0842\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 2s 74us/sample - loss: 0.1012 - val_loss: 0.0874\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 16\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 8s 229us/sample - loss: 0.1480 - val_loss: 0.1035\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1208 - val_loss: 0.1144\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1164 - val_loss: 0.0940\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1145 - val_loss: 0.0936\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1131 - val_loss: 0.1023\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1120 - val_loss: 0.1029\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1112 - val_loss: 0.0949\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1109 - val_loss: 0.0999\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1099 - val_loss: 0.1005\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.1091 - val_loss: 0.0873\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1090 - val_loss: 0.0996\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1079 - val_loss: 0.1005\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1078 - val_loss: 0.0951\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.1071 - val_loss: 0.1027\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1065 - val_loss: 0.0870\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1062 - val_loss: 0.0908\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1056 - val_loss: 0.0870\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1051 - val_loss: 0.0928\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.1045 - val_loss: 0.0962\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1045 - val_loss: 0.0840\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1040 - val_loss: 0.0893\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1035 - val_loss: 0.0946\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1034 - val_loss: 0.0886\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1029 - val_loss: 0.0970\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1024 - val_loss: 0.0933\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1021 - val_loss: 0.0941\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1019 - val_loss: 0.0885\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.1018 - val_loss: 0.0917\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1021 - val_loss: 0.0887\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1014 - val_loss: 0.0906\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 32\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 7s 207us/sample - loss: 0.1395 - val_loss: 0.0997\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1181 - val_loss: 0.1070\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.1158 - val_loss: 0.0957\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.1150 - val_loss: 0.0971\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.1135 - val_loss: 0.0992\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1132 - val_loss: 0.1056\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1124 - val_loss: 0.0976\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1118 - val_loss: 0.1160\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.1118 - val_loss: 0.1083\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.1108 - val_loss: 0.0915\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1102 - val_loss: 0.0988\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.1091 - val_loss: 0.1072\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1088 - val_loss: 0.0983\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1079 - val_loss: 0.1026\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1076 - val_loss: 0.0910\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1070 - val_loss: 0.0953\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1062 - val_loss: 0.0897\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1057 - val_loss: 0.0920\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1047 - val_loss: 0.0971\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1043 - val_loss: 0.0909\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1040 - val_loss: 0.0894\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.1029 - val_loss: 0.0924\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1026 - val_loss: 0.0836\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.1019 - val_loss: 0.0923\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1013 - val_loss: 0.0877\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.1005 - val_loss: 0.0844\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.1002 - val_loss: 0.0812\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.1000 - val_loss: 0.0845\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.1000 - val_loss: 0.0843\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0992 - val_loss: 0.0864\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0988 - val_loss: 0.0739\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0989 - val_loss: 0.0725\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.0982 - val_loss: 0.0788\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.0978 - val_loss: 0.0753\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0979 - val_loss: 0.0793\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0982 - val_loss: 0.0788\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0971 - val_loss: 0.0782\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0972 - val_loss: 0.0772\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0967 - val_loss: 0.0727\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0971 - val_loss: 0.0769\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0968 - val_loss: 0.0745\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0962 - val_loss: 0.0823\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 64\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 7s 213us/sample - loss: 0.1401 - val_loss: 0.1003\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1183 - val_loss: 0.1131\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1165 - val_loss: 0.1049\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1152 - val_loss: 0.1057\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1139 - val_loss: 0.0979\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1133 - val_loss: 0.1033\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1130 - val_loss: 0.1007\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1121 - val_loss: 0.1052\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1115 - val_loss: 0.0978\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1105 - val_loss: 0.0997\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1099 - val_loss: 0.1021\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.1089 - val_loss: 0.1059\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1081 - val_loss: 0.1027\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1071 - val_loss: 0.0970\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.1067 - val_loss: 0.0846\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1058 - val_loss: 0.0918\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.1047 - val_loss: 0.0894\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1040 - val_loss: 0.0812\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.1032 - val_loss: 0.0867\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1022 - val_loss: 0.0867\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1015 - val_loss: 0.0821\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.1002 - val_loss: 0.0879\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0995 - val_loss: 0.0824\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0986 - val_loss: 0.0867\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0977 - val_loss: 0.0765\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.0967 - val_loss: 0.0758\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0968 - val_loss: 0.0744\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0959 - val_loss: 0.0750\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0966 - val_loss: 0.0785\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0956 - val_loss: 0.0737\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0950 - val_loss: 0.0730\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0948 - val_loss: 0.0779\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0944 - val_loss: 0.0718\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0944 - val_loss: 0.0693\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.0944 - val_loss: 0.0701\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 2s 68us/sample - loss: 0.0940 - val_loss: 0.0723\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0935 - val_loss: 0.0709\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.0936 - val_loss: 0.0681\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0932 - val_loss: 0.0705\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0936 - val_loss: 0.0667\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0929 - val_loss: 0.0688\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.0927 - val_loss: 0.0677\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0929 - val_loss: 0.0699\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0925 - val_loss: 0.0674\n",
      "Epoch 45/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0922 - val_loss: 0.0706\n",
      "Epoch 46/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0923 - val_loss: 0.0742\n",
      "Epoch 47/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0925 - val_loss: 0.0651\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.0920 - val_loss: 0.0647\n",
      "Epoch 49/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0920 - val_loss: 0.0725\n",
      "Epoch 50/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0921 - val_loss: 0.0652\n",
      "Epoch 51/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0916 - val_loss: 0.0732\n",
      "Epoch 52/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0916 - val_loss: 0.0674\n",
      "Epoch 53/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0914 - val_loss: 0.0705\n",
      "Epoch 54/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.0915 - val_loss: 0.0732\n",
      "Epoch 55/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.0914 - val_loss: 0.0708\n",
      "Epoch 56/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.0910 - val_loss: 0.0671\n",
      "Epoch 57/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.0911 - val_loss: 0.0670\n",
      "Epoch 58/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0912 - val_loss: 0.0712\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 128\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 8s 232us/sample - loss: 0.1350 - val_loss: 0.1023\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.1185 - val_loss: 0.1057\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1165 - val_loss: 0.1069\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.1147 - val_loss: 0.1031\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1139 - val_loss: 0.0934\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1131 - val_loss: 0.1014\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1124 - val_loss: 0.1014\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1114 - val_loss: 0.1103\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.1106 - val_loss: 0.0949\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1093 - val_loss: 0.0959\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1085 - val_loss: 0.0962\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.1078 - val_loss: 0.1005\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1066 - val_loss: 0.1019\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.1057 - val_loss: 0.0901\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1052 - val_loss: 0.0824\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1038 - val_loss: 0.0867\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1027 - val_loss: 0.0852\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.1016 - val_loss: 0.0797\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1005 - val_loss: 0.0868\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.1000 - val_loss: 0.0771\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0995 - val_loss: 0.0776\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0986 - val_loss: 0.0833\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0977 - val_loss: 0.0853\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0973 - val_loss: 0.0862\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0966 - val_loss: 0.0755\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0960 - val_loss: 0.0723\n",
      "Epoch 27/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0955 - val_loss: 0.0759\n",
      "Epoch 28/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0952 - val_loss: 0.0731\n",
      "Epoch 29/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0958 - val_loss: 0.0764\n",
      "Epoch 30/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.0946 - val_loss: 0.0694\n",
      "Epoch 31/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.0938 - val_loss: 0.0693\n",
      "Epoch 32/100\n",
      "33200/33200 [==============================] - 2s 69us/sample - loss: 0.0937 - val_loss: 0.0732\n",
      "Epoch 33/100\n",
      "33200/33200 [==============================] - 2s 70us/sample - loss: 0.0934 - val_loss: 0.0678\n",
      "Epoch 34/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0931 - val_loss: 0.0670\n",
      "Epoch 35/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0930 - val_loss: 0.0717\n",
      "Epoch 36/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0925 - val_loss: 0.0684\n",
      "Epoch 37/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0920 - val_loss: 0.0687\n",
      "Epoch 38/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.0921 - val_loss: 0.0695\n",
      "Epoch 39/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0917 - val_loss: 0.0705\n",
      "Epoch 40/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0915 - val_loss: 0.0667\n",
      "Epoch 41/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0915 - val_loss: 0.0673\n",
      "Epoch 42/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0913 - val_loss: 0.0701\n",
      "Epoch 43/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0913 - val_loss: 0.0675\n",
      "Epoch 44/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0908 - val_loss: 0.0666\n",
      "Epoch 45/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0905 - val_loss: 0.0669\n",
      "Epoch 46/100\n",
      "33200/33200 [==============================] - 2s 63us/sample - loss: 0.0906 - val_loss: 0.0702\n",
      "Epoch 47/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0904 - val_loss: 0.0631\n",
      "Epoch 48/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.0904 - val_loss: 0.0647\n",
      "Epoch 49/100\n",
      "33200/33200 [==============================] - 2s 66us/sample - loss: 0.0902 - val_loss: 0.0662\n",
      "Epoch 50/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0901 - val_loss: 0.0641\n",
      "Epoch 51/100\n",
      "33200/33200 [==============================] - 2s 67us/sample - loss: 0.0897 - val_loss: 0.0682\n",
      "Epoch 52/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0897 - val_loss: 0.0698\n",
      "Epoch 53/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0894 - val_loss: 0.0696\n",
      "Epoch 54/100\n",
      "33200/33200 [==============================] - 2s 64us/sample - loss: 0.0893 - val_loss: 0.0670\n",
      "Epoch 55/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0894 - val_loss: 0.0668\n",
      "Epoch 56/100\n",
      "33200/33200 [==============================] - 2s 62us/sample - loss: 0.0893 - val_loss: 0.0638\n",
      "Epoch 57/100\n",
      "33200/33200 [==============================] - 2s 65us/sample - loss: 0.0891 - val_loss: 0.0650\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 2\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 7s 202us/sample - loss: 0.1869 - val_loss: 0.1590\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1483 - val_loss: 0.1221\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1349 - val_loss: 0.1145\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1286 - val_loss: 0.1123\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1251 - val_loss: 0.1115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 3s 74us/sample - loss: 0.1233 - val_loss: 0.1106\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1219 - val_loss: 0.1098\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1211 - val_loss: 0.1112\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1205 - val_loss: 0.1096\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1199 - val_loss: 0.1099\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1194 - val_loss: 0.1087\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1191 - val_loss: 0.1090\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1187 - val_loss: 0.1091\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1184 - val_loss: 0.1094\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1182 - val_loss: 0.1097\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1180 - val_loss: 0.1119\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1177 - val_loss: 0.1108\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1175 - val_loss: 0.1090\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1174 - val_loss: 0.1090\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1171 - val_loss: 0.1089\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1169 - val_loss: 0.1100\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 4\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 8s 243us/sample - loss: 0.1747 - val_loss: 0.1455\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1388 - val_loss: 0.1179\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1281 - val_loss: 0.1141\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1243 - val_loss: 0.1140\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1223 - val_loss: 0.1138\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1209 - val_loss: 0.1116\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1200 - val_loss: 0.1115\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1191 - val_loss: 0.1112\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1185 - val_loss: 0.1108\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1178 - val_loss: 0.1118\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1172 - val_loss: 0.1095\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1169 - val_loss: 0.1085\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1165 - val_loss: 0.1094\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1162 - val_loss: 0.1104\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1159 - val_loss: 0.1090\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1155 - val_loss: 0.1113\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1152 - val_loss: 0.1098\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1150 - val_loss: 0.1078\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1149 - val_loss: 0.1091\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1146 - val_loss: 0.1077\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1144 - val_loss: 0.1095\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1141 - val_loss: 0.1076\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1138 - val_loss: 0.1085\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1137 - val_loss: 0.1062\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1134 - val_loss: 0.1070\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.1132 - val_loss: 0.1055\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.1130 - val_loss: 0.1063\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1128 - val_loss: 0.1060\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1127 - val_loss: 0.1088\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.1123 - val_loss: 0.1054\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.1120 - val_loss: 0.1054\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1118 - val_loss: 0.1075\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1117 - val_loss: 0.1040\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1115 - val_loss: 0.1039\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1114 - val_loss: 0.1029\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1110 - val_loss: 0.1073\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1108 - val_loss: 0.1057\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1105 - val_loss: 0.1036\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1102 - val_loss: 0.1042\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1101 - val_loss: 0.1019\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.1098 - val_loss: 0.1022\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1096 - val_loss: 0.1040\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1094 - val_loss: 0.1028\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1092 - val_loss: 0.1027\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1089 - val_loss: 0.1012\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1087 - val_loss: 0.1017\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1085 - val_loss: 0.1018\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1083 - val_loss: 0.1007\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1080 - val_loss: 0.1006\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1079 - val_loss: 0.1026\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1077 - val_loss: 0.1016\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1076 - val_loss: 0.0990\n",
      "Epoch 53/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1072 - val_loss: 0.1006\n",
      "Epoch 54/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1071 - val_loss: 0.1014\n",
      "Epoch 55/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1068 - val_loss: 0.1007\n",
      "Epoch 56/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1067 - val_loss: 0.0992\n",
      "Epoch 57/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1064 - val_loss: 0.1013\n",
      "Epoch 58/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1063 - val_loss: 0.0978\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1061 - val_loss: 0.0986\n",
      "Epoch 60/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1059 - val_loss: 0.0977\n",
      "Epoch 61/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1059 - val_loss: 0.0997\n",
      "Epoch 62/100\n",
      "33960/33960 [==============================] - 2s 73us/sample - loss: 0.1057 - val_loss: 0.0988\n",
      "Epoch 63/100\n",
      "33960/33960 [==============================] - 3s 75us/sample - loss: 0.1053 - val_loss: 0.0984\n",
      "Epoch 64/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1052 - val_loss: 0.0983\n",
      "Epoch 65/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1051 - val_loss: 0.0979\n",
      "Epoch 66/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1049 - val_loss: 0.0990\n",
      "Epoch 67/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1048 - val_loss: 0.0991\n",
      "Epoch 68/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1045 - val_loss: 0.0985\n",
      "Epoch 69/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1045 - val_loss: 0.0984\n",
      "Epoch 70/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1044 - val_loss: 0.0975\n",
      "Epoch 71/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1043 - val_loss: 0.0986\n",
      "Epoch 72/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1041 - val_loss: 0.0979\n",
      "Epoch 73/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1039 - val_loss: 0.0972\n",
      "Epoch 74/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1038 - val_loss: 0.0982\n",
      "Epoch 75/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1037 - val_loss: 0.0981\n",
      "Epoch 76/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1036 - val_loss: 0.0969\n",
      "Epoch 77/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1034 - val_loss: 0.0977\n",
      "Epoch 78/100\n",
      "33960/33960 [==============================] - 2s 72us/sample - loss: 0.1034 - val_loss: 0.0965\n",
      "Epoch 79/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1032 - val_loss: 0.0986\n",
      "Epoch 80/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1031 - val_loss: 0.0969\n",
      "Epoch 81/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1031 - val_loss: 0.0972\n",
      "Epoch 82/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1029 - val_loss: 0.0976\n",
      "Epoch 83/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1029 - val_loss: 0.0976\n",
      "Epoch 84/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1027 - val_loss: 0.0971\n",
      "Epoch 85/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1026 - val_loss: 0.0966\n",
      "Epoch 86/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1026 - val_loss: 0.0963\n",
      "Epoch 87/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1025 - val_loss: 0.0971\n",
      "Epoch 88/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1024 - val_loss: 0.0971\n",
      "Epoch 89/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1023 - val_loss: 0.0968\n",
      "Epoch 90/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1023 - val_loss: 0.0957\n",
      "Epoch 91/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1021 - val_loss: 0.0961\n",
      "Epoch 92/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1020 - val_loss: 0.0969\n",
      "Epoch 93/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1019 - val_loss: 0.0970\n",
      "Epoch 94/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1018 - val_loss: 0.0971\n",
      "Epoch 95/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1018 - val_loss: 0.0968\n",
      "Epoch 96/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1018 - val_loss: 0.0982\n",
      "Epoch 97/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1017 - val_loss: 0.0974\n",
      "Epoch 98/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1016 - val_loss: 0.0963\n",
      "Epoch 99/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1015 - val_loss: 0.0980\n",
      "Epoch 100/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1014 - val_loss: 0.0967\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 8\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 7s 200us/sample - loss: 0.1589 - val_loss: 0.1170\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1280 - val_loss: 0.1099\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1236 - val_loss: 0.1094\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1214 - val_loss: 0.1111\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1198 - val_loss: 0.1116\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1186 - val_loss: 0.1105\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1180 - val_loss: 0.1084\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1173 - val_loss: 0.1108\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1167 - val_loss: 0.1097\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1161 - val_loss: 0.1114\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1156 - val_loss: 0.1083\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1152 - val_loss: 0.1073\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1151 - val_loss: 0.1079\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1146 - val_loss: 0.1111\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1144 - val_loss: 0.1102\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1143 - val_loss: 0.1088\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1138 - val_loss: 0.1096\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1135 - val_loss: 0.1102\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1134 - val_loss: 0.1108\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1131 - val_loss: 0.1045\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1129 - val_loss: 0.1109\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 2s 72us/sample - loss: 0.1126 - val_loss: 0.1106\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1123 - val_loss: 0.1056\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1122 - val_loss: 0.1068\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1118 - val_loss: 0.1073\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1115 - val_loss: 0.1077\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1112 - val_loss: 0.1034\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1110 - val_loss: 0.1037\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 2s 71us/sample - loss: 0.1108 - val_loss: 0.1068\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1106 - val_loss: 0.1035\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1100 - val_loss: 0.1042\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1101 - val_loss: 0.1089\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1097 - val_loss: 0.1059\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1094 - val_loss: 0.1017\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1094 - val_loss: 0.1021\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1091 - val_loss: 0.1119\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1087 - val_loss: 0.1053\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1085 - val_loss: 0.1048\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1083 - val_loss: 0.1045\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1081 - val_loss: 0.1006\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1079 - val_loss: 0.1054\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1077 - val_loss: 0.0991\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1077 - val_loss: 0.1012\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1074 - val_loss: 0.1057\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1074 - val_loss: 0.1009\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1070 - val_loss: 0.1009\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1071 - val_loss: 0.1028\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1069 - val_loss: 0.0995\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1066 - val_loss: 0.0993\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1069 - val_loss: 0.0999\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1064 - val_loss: 0.1000\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 3s 74us/sample - loss: 0.1064 - val_loss: 0.1006\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 16\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 7s 206us/sample - loss: 0.1538 - val_loss: 0.1136\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1262 - val_loss: 0.1100\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1224 - val_loss: 0.1106\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1205 - val_loss: 0.1160\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1194 - val_loss: 0.1129\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1184 - val_loss: 0.1150\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1178 - val_loss: 0.1103\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1174 - val_loss: 0.1136\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1166 - val_loss: 0.1093\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1162 - val_loss: 0.1126\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1158 - val_loss: 0.1120\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1153 - val_loss: 0.1097\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1152 - val_loss: 0.1114\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1145 - val_loss: 0.1135\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1142 - val_loss: 0.1101\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1137 - val_loss: 0.1064\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1135 - val_loss: 0.1114\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1128 - val_loss: 0.1136\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1126 - val_loss: 0.1173\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1119 - val_loss: 0.1052\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1117 - val_loss: 0.1115\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1110 - val_loss: 0.1103\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1108 - val_loss: 0.1088\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1104 - val_loss: 0.1044\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1100 - val_loss: 0.1068\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1095 - val_loss: 0.1089\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1092 - val_loss: 0.1045\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1087 - val_loss: 0.1033\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1085 - val_loss: 0.1049\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1082 - val_loss: 0.1002\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1075 - val_loss: 0.1017\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1075 - val_loss: 0.1079\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1072 - val_loss: 0.1099\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1067 - val_loss: 0.0984\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1068 - val_loss: 0.0993\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1061 - val_loss: 0.1062\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1058 - val_loss: 0.1046\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1055 - val_loss: 0.1012\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 2s 69us/sample - loss: 0.1050 - val_loss: 0.1002\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1049 - val_loss: 0.1017\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1048 - val_loss: 0.1043\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1043 - val_loss: 0.0955\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1041 - val_loss: 0.0972\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1040 - val_loss: 0.1040\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1036 - val_loss: 0.0992\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1033 - val_loss: 0.0968\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1032 - val_loss: 0.1002\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1029 - val_loss: 0.0959\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1028 - val_loss: 0.0977\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1027 - val_loss: 0.0966\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1022 - val_loss: 0.0971\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.1021 - val_loss: 0.0979\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 32\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 8s 241us/sample - loss: 0.1470 - val_loss: 0.1124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1240 - val_loss: 0.1113\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1212 - val_loss: 0.1122\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1203 - val_loss: 0.1136\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1195 - val_loss: 0.1118\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1189 - val_loss: 0.1182\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.1186 - val_loss: 0.1094\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1182 - val_loss: 0.1094\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1177 - val_loss: 0.1116\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1173 - val_loss: 0.1115\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 67us/sample - loss: 0.1171 - val_loss: 0.1130\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1164 - val_loss: 0.1136\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1167 - val_loss: 0.1099\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1158 - val_loss: 0.1138\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1154 - val_loss: 0.1125\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1153 - val_loss: 0.1072\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1150 - val_loss: 0.1100\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1145 - val_loss: 0.1173\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1146 - val_loss: 0.1117\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1137 - val_loss: 0.1085\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1136 - val_loss: 0.1117\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1130 - val_loss: 0.1127\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1129 - val_loss: 0.1061\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1125 - val_loss: 0.1030\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 2s 68us/sample - loss: 0.1120 - val_loss: 0.1067\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1115 - val_loss: 0.1061\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1110 - val_loss: 0.1056\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1105 - val_loss: 0.1057\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1103 - val_loss: 0.1078\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.1100 - val_loss: 0.1005\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1093 - val_loss: 0.1009\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1091 - val_loss: 0.1084\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1086 - val_loss: 0.1110\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1080 - val_loss: 0.0974\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1079 - val_loss: 0.0956\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1071 - val_loss: 0.1034\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1065 - val_loss: 0.0969\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1062 - val_loss: 0.0974\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1057 - val_loss: 0.0956\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1053 - val_loss: 0.0997\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1048 - val_loss: 0.0983\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1041 - val_loss: 0.0915\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.1035 - val_loss: 0.0938\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1033 - val_loss: 0.0971\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1027 - val_loss: 0.0943\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1025 - val_loss: 0.0941\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1020 - val_loss: 0.0972\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1019 - val_loss: 0.0973\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1016 - val_loss: 0.0974\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1011 - val_loss: 0.0958\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1007 - val_loss: 0.1002\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1007 - val_loss: 0.0941\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 64\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 7s 209us/sample - loss: 0.1444 - val_loss: 0.1164\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1243 - val_loss: 0.1129\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1212 - val_loss: 0.1134\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1201 - val_loss: 0.1120\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1192 - val_loss: 0.1117\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1184 - val_loss: 0.1190\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1178 - val_loss: 0.1095\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.1175 - val_loss: 0.1090\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.1169 - val_loss: 0.1122\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.1164 - val_loss: 0.1157\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1161 - val_loss: 0.1088\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1153 - val_loss: 0.1162\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1153 - val_loss: 0.1104\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1147 - val_loss: 0.1153\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1142 - val_loss: 0.1115\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1136 - val_loss: 0.1087\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1131 - val_loss: 0.1085\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1122 - val_loss: 0.1164\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1115 - val_loss: 0.1075\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1099 - val_loss: 0.1014\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1093 - val_loss: 0.1121\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1086 - val_loss: 0.1040\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1074 - val_loss: 0.0979\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1071 - val_loss: 0.1029\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1055 - val_loss: 0.1023\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1052 - val_loss: 0.1033\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1044 - val_loss: 0.1030\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1041 - val_loss: 0.1005\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1031 - val_loss: 0.0981\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1027 - val_loss: 0.0982\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1025 - val_loss: 0.1029\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1019 - val_loss: 0.0954\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.1014 - val_loss: 0.1015\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1009 - val_loss: 0.0989\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1011 - val_loss: 0.0923\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1005 - val_loss: 0.0968\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.0999 - val_loss: 0.0923\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1001 - val_loss: 0.0936\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.0994 - val_loss: 0.0946\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.0992 - val_loss: 0.0986\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.0989 - val_loss: 0.0959\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.0987 - val_loss: 0.0888\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.0983 - val_loss: 0.0901\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.0982 - val_loss: 0.0916\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.0979 - val_loss: 0.0904\n",
      "Epoch 46/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.0981 - val_loss: 0.0917\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.0977 - val_loss: 0.0923\n",
      "Epoch 48/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0977 - val_loss: 0.0905\n",
      "Epoch 49/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0974 - val_loss: 0.0895\n",
      "Epoch 50/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.0972 - val_loss: 0.0906\n",
      "Epoch 51/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.0969 - val_loss: 0.0949\n",
      "Epoch 52/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.0971 - val_loss: 0.0902\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 128\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 9s 263us/sample - loss: 0.1422 - val_loss: 0.1130\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1250 - val_loss: 0.1105\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1222 - val_loss: 0.1133\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1209 - val_loss: 0.1142\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1199 - val_loss: 0.1115\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1190 - val_loss: 0.1185\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1188 - val_loss: 0.1184\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1188 - val_loss: 0.1090\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1174 - val_loss: 0.1127\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1165 - val_loss: 0.1146\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1160 - val_loss: 0.1083\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.1149 - val_loss: 0.1118\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1145 - val_loss: 0.1094\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.1140 - val_loss: 0.1081\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1128 - val_loss: 0.1116\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1117 - val_loss: 0.1049\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1113 - val_loss: 0.1030\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1101 - val_loss: 0.1032\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1085 - val_loss: 0.0961\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1071 - val_loss: 0.0941\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.1061 - val_loss: 0.1011\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.1058 - val_loss: 0.0997\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.1046 - val_loss: 0.0942\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.1047 - val_loss: 0.0932\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.1034 - val_loss: 0.0993\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 2s 70us/sample - loss: 0.1028 - val_loss: 0.1095\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 3s 82us/sample - loss: 0.1023 - val_loss: 0.0950\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 3s 79us/sample - loss: 0.1020 - val_loss: 0.0991\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 3s 80us/sample - loss: 0.1014 - val_loss: 0.0903\n",
      "Epoch 30/100\n",
      "33960/33960 [==============================] - 3s 75us/sample - loss: 0.1010 - val_loss: 0.0907\n",
      "Epoch 31/100\n",
      "33960/33960 [==============================] - 3s 74us/sample - loss: 0.1003 - val_loss: 0.0939\n",
      "Epoch 32/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.1003 - val_loss: 0.0893\n",
      "Epoch 33/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0999 - val_loss: 0.0920\n",
      "Epoch 34/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0995 - val_loss: 0.1038\n",
      "Epoch 35/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.0993 - val_loss: 0.0900\n",
      "Epoch 36/100\n",
      "33960/33960 [==============================] - 2s 63us/sample - loss: 0.0988 - val_loss: 0.0986\n",
      "Epoch 37/100\n",
      "33960/33960 [==============================] - 2s 66us/sample - loss: 0.0984 - val_loss: 0.0871\n",
      "Epoch 38/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0987 - val_loss: 0.0897\n",
      "Epoch 39/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.0981 - val_loss: 0.0908\n",
      "Epoch 40/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.0976 - val_loss: 0.0919\n",
      "Epoch 41/100\n",
      "33960/33960 [==============================] - 2s 65us/sample - loss: 0.0977 - val_loss: 0.0936\n",
      "Epoch 42/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.0972 - val_loss: 0.0887\n",
      "Epoch 43/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.0968 - val_loss: 0.0888\n",
      "Epoch 44/100\n",
      "33960/33960 [==============================] - 2s 62us/sample - loss: 0.0968 - val_loss: 0.0915\n",
      "Epoch 45/100\n",
      "33960/33960 [==============================] - 2s 64us/sample - loss: 0.0965 - val_loss: 0.0897\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0964 - val_loss: 0.0925\n",
      "Epoch 47/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.0962 - val_loss: 0.0904\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 2\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 7s 210us/sample - loss: 0.1947 - val_loss: 0.1703\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1546 - val_loss: 0.1420\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1391 - val_loss: 0.1257\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1296 - val_loss: 0.1144\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1236 - val_loss: 0.1121\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1199 - val_loss: 0.1093\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 70us/sample - loss: 0.1170 - val_loss: 0.1109\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 70us/sample - loss: 0.1156 - val_loss: 0.1070\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1148 - val_loss: 0.1060\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1141 - val_loss: 0.1046\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1137 - val_loss: 0.1051\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1132 - val_loss: 0.1058\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1127 - val_loss: 0.1053\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1125 - val_loss: 0.1047\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1120 - val_loss: 0.1047\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1117 - val_loss: 0.1050\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1114 - val_loss: 0.1046\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1111 - val_loss: 0.1040\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1108 - val_loss: 0.1036\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 58us/sample - loss: 0.1105 - val_loss: 0.1029\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1102 - val_loss: 0.1054\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1100 - val_loss: 0.1021\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1096 - val_loss: 0.1038\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1094 - val_loss: 0.1042\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1091 - val_loss: 0.1036\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1089 - val_loss: 0.1010\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1088 - val_loss: 0.1016\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1085 - val_loss: 0.1011\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1082 - val_loss: 0.1007\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1080 - val_loss: 0.1007\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 2s 72us/sample - loss: 0.1078 - val_loss: 0.0997\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1076 - val_loss: 0.0998\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1074 - val_loss: 0.0993\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1072 - val_loss: 0.1004\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1070 - val_loss: 0.0986\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1068 - val_loss: 0.0989\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1066 - val_loss: 0.0999\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1065 - val_loss: 0.0994\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1063 - val_loss: 0.0988\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1061 - val_loss: 0.0984\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1060 - val_loss: 0.0963\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1058 - val_loss: 0.0988\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1057 - val_loss: 0.0977\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1054 - val_loss: 0.0971\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1054 - val_loss: 0.0989\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1052 - val_loss: 0.0970\n",
      "Epoch 47/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1051 - val_loss: 0.0963\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1049 - val_loss: 0.0971\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1048 - val_loss: 0.0953\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1046 - val_loss: 0.0959\n",
      "Epoch 51/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1045 - val_loss: 0.0957\n",
      "Epoch 52/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1043 - val_loss: 0.0972\n",
      "Epoch 53/100\n",
      "33820/33820 [==============================] - 2s 70us/sample - loss: 0.1042 - val_loss: 0.0956\n",
      "Epoch 54/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1041 - val_loss: 0.0956\n",
      "Epoch 55/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1040 - val_loss: 0.0959\n",
      "Epoch 56/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1039 - val_loss: 0.0947\n",
      "Epoch 57/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1037 - val_loss: 0.0949\n",
      "Epoch 58/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1036 - val_loss: 0.0946\n",
      "Epoch 59/100\n",
      "33820/33820 [==============================] - 2s 71us/sample - loss: 0.1034 - val_loss: 0.0934\n",
      "Epoch 60/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1033 - val_loss: 0.0933\n",
      "Epoch 61/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1032 - val_loss: 0.0934\n",
      "Epoch 62/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1031 - val_loss: 0.0942\n",
      "Epoch 63/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1030 - val_loss: 0.0937\n",
      "Epoch 64/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1029 - val_loss: 0.0925\n",
      "Epoch 65/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1027 - val_loss: 0.0939\n",
      "Epoch 66/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1027 - val_loss: 0.0932\n",
      "Epoch 67/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1025 - val_loss: 0.0933\n",
      "Epoch 68/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1024 - val_loss: 0.0923\n",
      "Epoch 69/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1023 - val_loss: 0.0937\n",
      "Epoch 70/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1023 - val_loss: 0.0923\n",
      "Epoch 71/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1022 - val_loss: 0.0915\n",
      "Epoch 72/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1021 - val_loss: 0.0917\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1020 - val_loss: 0.0925\n",
      "Epoch 74/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1019 - val_loss: 0.0916\n",
      "Epoch 75/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1018 - val_loss: 0.0915\n",
      "Epoch 76/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1017 - val_loss: 0.0911\n",
      "Epoch 77/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1017 - val_loss: 0.0907\n",
      "Epoch 78/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1016 - val_loss: 0.0921\n",
      "Epoch 79/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1014 - val_loss: 0.0909\n",
      "Epoch 80/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1014 - val_loss: 0.0913\n",
      "Epoch 81/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1013 - val_loss: 0.0919\n",
      "Epoch 82/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1013 - val_loss: 0.0914\n",
      "Epoch 83/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1012 - val_loss: 0.0908\n",
      "Epoch 84/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1012 - val_loss: 0.0908\n",
      "Epoch 85/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1011 - val_loss: 0.0913\n",
      "Epoch 86/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1010 - val_loss: 0.0909\n",
      "Epoch 87/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1010 - val_loss: 0.0907\n",
      "Epoch 88/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1009 - val_loss: 0.0900\n",
      "Epoch 89/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1009 - val_loss: 0.0903\n",
      "Epoch 90/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1008 - val_loss: 0.0904\n",
      "Epoch 91/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1008 - val_loss: 0.0903\n",
      "Epoch 92/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1007 - val_loss: 0.0907\n",
      "Epoch 93/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1006 - val_loss: 0.0901\n",
      "Epoch 94/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1006 - val_loss: 0.0901\n",
      "Epoch 95/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1005 - val_loss: 0.0896\n",
      "Epoch 96/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1005 - val_loss: 0.0900\n",
      "Epoch 97/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1004 - val_loss: 0.0895\n",
      "Epoch 98/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1004 - val_loss: 0.0897\n",
      "Epoch 99/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1004 - val_loss: 0.0890\n",
      "Epoch 100/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1002 - val_loss: 0.0893\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 4\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 8s 238us/sample - loss: 0.1670 - val_loss: 0.1189\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1283 - val_loss: 0.1094\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1217 - val_loss: 0.1046\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1187 - val_loss: 0.1033\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1168 - val_loss: 0.1082\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1153 - val_loss: 0.1032\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1143 - val_loss: 0.1029\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1136 - val_loss: 0.1046\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1127 - val_loss: 0.1008\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1120 - val_loss: 0.0998\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1115 - val_loss: 0.1006\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1110 - val_loss: 0.1013\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1106 - val_loss: 0.1013\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1102 - val_loss: 0.1003\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1097 - val_loss: 0.0994\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1092 - val_loss: 0.1011\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1088 - val_loss: 0.1015\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1083 - val_loss: 0.1008\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1080 - val_loss: 0.1021\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1077 - val_loss: 0.1008\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1072 - val_loss: 0.0996\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1069 - val_loss: 0.0977\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1066 - val_loss: 0.0990\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1064 - val_loss: 0.1020\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1061 - val_loss: 0.0981\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1057 - val_loss: 0.0967\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1056 - val_loss: 0.0988\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1052 - val_loss: 0.1006\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1051 - val_loss: 0.0969\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1050 - val_loss: 0.0979\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1047 - val_loss: 0.0970\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1045 - val_loss: 0.0964\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1043 - val_loss: 0.0958\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1040 - val_loss: 0.0970\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.1040 - val_loss: 0.0955\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1037 - val_loss: 0.0954\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1035 - val_loss: 0.0984\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 2s 71us/sample - loss: 0.1034 - val_loss: 0.0974\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1033 - val_loss: 0.0957\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1031 - val_loss: 0.0949\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1029 - val_loss: 0.0935\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1029 - val_loss: 0.0956\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1028 - val_loss: 0.0973\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1027 - val_loss: 0.0939\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1027 - val_loss: 0.0952\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1025 - val_loss: 0.0942\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1024 - val_loss: 0.0945\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1023 - val_loss: 0.0956\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1023 - val_loss: 0.0962\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1020 - val_loss: 0.0956\n",
      "Epoch 51/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1020 - val_loss: 0.0953\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 8\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 7s 211us/sample - loss: 0.1574 - val_loss: 0.1121\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1256 - val_loss: 0.1083\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1196 - val_loss: 0.1071\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1166 - val_loss: 0.1052\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1152 - val_loss: 0.1104\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1137 - val_loss: 0.1052\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1130 - val_loss: 0.1045\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1121 - val_loss: 0.1032\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1113 - val_loss: 0.1011\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1106 - val_loss: 0.1000\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 71us/sample - loss: 0.1101 - val_loss: 0.1004\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1096 - val_loss: 0.1001\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 72us/sample - loss: 0.1091 - val_loss: 0.1036\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1089 - val_loss: 0.1033\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 70us/sample - loss: 0.1083 - val_loss: 0.0987\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1080 - val_loss: 0.1004\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1075 - val_loss: 0.1017\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1072 - val_loss: 0.1017\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1067 - val_loss: 0.1044\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1064 - val_loss: 0.1037\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1060 - val_loss: 0.0996\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1059 - val_loss: 0.0980\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1054 - val_loss: 0.0994\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1050 - val_loss: 0.1021\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1047 - val_loss: 0.0979\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1045 - val_loss: 0.0945\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1041 - val_loss: 0.0975\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 2s 70us/sample - loss: 0.1037 - val_loss: 0.1016\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1035 - val_loss: 0.1007\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1033 - val_loss: 0.1009\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1031 - val_loss: 0.0949\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1027 - val_loss: 0.0998\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1027 - val_loss: 0.0956\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1022 - val_loss: 0.0945\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1020 - val_loss: 0.0942\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1018 - val_loss: 0.0952\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.1015 - val_loss: 0.1000\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1015 - val_loss: 0.0948\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1009 - val_loss: 0.0970\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1008 - val_loss: 0.0962\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1006 - val_loss: 0.0922\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1005 - val_loss: 0.0954\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1003 - val_loss: 0.0955\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1002 - val_loss: 0.0913\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1000 - val_loss: 0.0929\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0997 - val_loss: 0.0924\n",
      "Epoch 47/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.0997 - val_loss: 0.0927\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0995 - val_loss: 0.0908\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.0995 - val_loss: 0.0981\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.0990 - val_loss: 0.0937\n",
      "Epoch 51/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.0989 - val_loss: 0.0951\n",
      "Epoch 52/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 53/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.0987 - val_loss: 0.0958\n",
      "Epoch 54/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0986 - val_loss: 0.0979\n",
      "Epoch 55/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0984 - val_loss: 0.0958\n",
      "Epoch 56/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0985 - val_loss: 0.0918\n",
      "Epoch 57/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.0980 - val_loss: 0.0927\n",
      "Epoch 58/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0981 - val_loss: 0.0919\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 16\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 8s 244us/sample - loss: 0.1477 - val_loss: 0.1087\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1203 - val_loss: 0.1090\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1164 - val_loss: 0.1036\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1144 - val_loss: 0.1071\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1133 - val_loss: 0.1113\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1122 - val_loss: 0.1074\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1116 - val_loss: 0.1042\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1107 - val_loss: 0.1019\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1098 - val_loss: 0.1004\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1091 - val_loss: 0.0977\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.1084 - val_loss: 0.1000\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1076 - val_loss: 0.0977\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1068 - val_loss: 0.1045\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1060 - val_loss: 0.1019\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1052 - val_loss: 0.0969\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1045 - val_loss: 0.1004\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1037 - val_loss: 0.0986\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1031 - val_loss: 0.0955\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1029 - val_loss: 0.1014\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.1024 - val_loss: 0.1033\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.1018 - val_loss: 0.1013\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.1018 - val_loss: 0.0994\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.1015 - val_loss: 0.1016\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1009 - val_loss: 0.0963\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 2s 58us/sample - loss: 0.1006 - val_loss: 0.0930\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.1003 - val_loss: 0.0917\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1001 - val_loss: 0.0961\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 2s 69us/sample - loss: 0.0998 - val_loss: 0.0968\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0995 - val_loss: 0.0928\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 2s 68us/sample - loss: 0.0995 - val_loss: 0.0952\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0995 - val_loss: 0.0916\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0992 - val_loss: 0.0964\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0993 - val_loss: 0.0930\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0986 - val_loss: 0.0913\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0986 - val_loss: 0.0882\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0985 - val_loss: 0.0955\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0984 - val_loss: 0.0943\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0980 - val_loss: 0.0912\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0977 - val_loss: 0.0944\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.0976 - val_loss: 0.0886\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0976 - val_loss: 0.0891\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0973 - val_loss: 0.0874\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0972 - val_loss: 0.0928\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0971 - val_loss: 0.0903\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.0971 - val_loss: 0.0921\n",
      "Epoch 46/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0968 - val_loss: 0.0888\n",
      "Epoch 47/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0966 - val_loss: 0.0934\n",
      "Epoch 48/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0968 - val_loss: 0.0862\n",
      "Epoch 49/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0964 - val_loss: 0.0953\n",
      "Epoch 50/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0963 - val_loss: 0.0933\n",
      "Epoch 51/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0962 - val_loss: 0.0914\n",
      "Epoch 52/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0962 - val_loss: 0.0894\n",
      "Epoch 53/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0960 - val_loss: 0.0916\n",
      "Epoch 54/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0959 - val_loss: 0.0956\n",
      "Epoch 55/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0958 - val_loss: 0.0916\n",
      "Epoch 56/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0959 - val_loss: 0.0922\n",
      "Epoch 57/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0955 - val_loss: 0.0927\n",
      "Epoch 58/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0957 - val_loss: 0.0880\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 32\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 7s 212us/sample - loss: 0.1387 - val_loss: 0.1045\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1178 - val_loss: 0.1052\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1151 - val_loss: 0.1070\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1143 - val_loss: 0.1068\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1129 - val_loss: 0.1096\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1126 - val_loss: 0.1137\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1124 - val_loss: 0.1082\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1115 - val_loss: 0.1013\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1104 - val_loss: 0.1021\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1097 - val_loss: 0.0994\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1091 - val_loss: 0.1022\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1083 - val_loss: 0.1013\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1076 - val_loss: 0.1049\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1068 - val_loss: 0.0984\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1058 - val_loss: 0.1001\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1048 - val_loss: 0.1003\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1041 - val_loss: 0.0949\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1028 - val_loss: 0.0948\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1027 - val_loss: 0.0982\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1016 - val_loss: 0.0979\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1008 - val_loss: 0.0978\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1004 - val_loss: 0.0980\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1005 - val_loss: 0.0982\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0989 - val_loss: 0.0956\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0985 - val_loss: 0.0893\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0984 - val_loss: 0.0950\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0981 - val_loss: 0.0971\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0972 - val_loss: 0.0985\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0971 - val_loss: 0.0931\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0965 - val_loss: 0.0895\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0967 - val_loss: 0.0863\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0959 - val_loss: 0.0934\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0964 - val_loss: 0.0937\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0959 - val_loss: 0.0873\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0956 - val_loss: 0.0848\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 2s 67us/sample - loss: 0.0953 - val_loss: 0.0932\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0953 - val_loss: 0.0886\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0951 - val_loss: 0.0855\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0949 - val_loss: 0.0897\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0946 - val_loss: 0.0869\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0944 - val_loss: 0.0861\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0945 - val_loss: 0.0863\n",
      "Epoch 43/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0943 - val_loss: 0.0881\n",
      "Epoch 44/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0943 - val_loss: 0.0864\n",
      "Epoch 45/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0942 - val_loss: 0.0852\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 64\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 8s 239us/sample - loss: 0.1365 - val_loss: 0.1097\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1175 - val_loss: 0.1110\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1154 - val_loss: 0.1067\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1143 - val_loss: 0.1113\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1128 - val_loss: 0.1056\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1126 - val_loss: 0.1150\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1119 - val_loss: 0.1185\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1114 - val_loss: 0.1065\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1093 - val_loss: 0.1033\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1087 - val_loss: 0.0986\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1078 - val_loss: 0.0989\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1069 - val_loss: 0.0983\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1060 - val_loss: 0.0992\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1059 - val_loss: 0.0963\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1044 - val_loss: 0.0954\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.1039 - val_loss: 0.0986\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1028 - val_loss: 0.0922\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1015 - val_loss: 0.0944\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1011 - val_loss: 0.0944\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1005 - val_loss: 0.0935\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0993 - val_loss: 0.0952\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0983 - val_loss: 0.0922\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0988 - val_loss: 0.0976\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 2s 58us/sample - loss: 0.0970 - val_loss: 0.0910\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0966 - val_loss: 0.0860\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0963 - val_loss: 0.0906\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0958 - val_loss: 0.0953\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0950 - val_loss: 0.0892\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0948 - val_loss: 0.0852\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0945 - val_loss: 0.0902\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0945 - val_loss: 0.0903\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0939 - val_loss: 0.0843\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0945 - val_loss: 0.0883\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0934 - val_loss: 0.0855\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0934 - val_loss: 0.0848\n",
      "Epoch 36/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0930 - val_loss: 0.0889\n",
      "Epoch 37/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0930 - val_loss: 0.0852\n",
      "Epoch 38/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0928 - val_loss: 0.0882\n",
      "Epoch 39/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0928 - val_loss: 0.0886\n",
      "Epoch 40/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0925 - val_loss: 0.0848\n",
      "Epoch 41/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0923 - val_loss: 0.0877\n",
      "Epoch 42/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0923 - val_loss: 0.0876\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 128\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 7s 201us/sample - loss: 0.1343 - val_loss: 0.1080\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1177 - val_loss: 0.1030\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1168 - val_loss: 0.1074\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1148 - val_loss: 0.1129\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1133 - val_loss: 0.1055\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.1124 - val_loss: 0.1051\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1122 - val_loss: 0.1216\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.1110 - val_loss: 0.1103\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1097 - val_loss: 0.0986\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1092 - val_loss: 0.0990\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.1076 - val_loss: 0.1042\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1065 - val_loss: 0.1027\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.1053 - val_loss: 0.0936\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.1041 - val_loss: 0.0918\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.1024 - val_loss: 0.0886\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.1008 - val_loss: 0.0988\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.0992 - val_loss: 0.0933\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 71us/sample - loss: 0.0984 - val_loss: 0.0940\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0977 - val_loss: 0.0969\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0971 - val_loss: 0.0868\n",
      "Epoch 21/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0960 - val_loss: 0.0966\n",
      "Epoch 22/100\n",
      "33820/33820 [==============================] - 2s 62us/sample - loss: 0.0952 - val_loss: 0.0900\n",
      "Epoch 23/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.0952 - val_loss: 0.0948\n",
      "Epoch 24/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0944 - val_loss: 0.0903\n",
      "Epoch 25/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0937 - val_loss: 0.0850\n",
      "Epoch 26/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.0940 - val_loss: 0.0895\n",
      "Epoch 27/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0938 - val_loss: 0.0923\n",
      "Epoch 28/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0928 - val_loss: 0.0872\n",
      "Epoch 29/100\n",
      "33820/33820 [==============================] - 2s 65us/sample - loss: 0.0928 - val_loss: 0.0853\n",
      "Epoch 30/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0924 - val_loss: 0.0862\n",
      "Epoch 31/100\n",
      "33820/33820 [==============================] - 2s 66us/sample - loss: 0.0926 - val_loss: 0.0905\n",
      "Epoch 32/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0920 - val_loss: 0.0865\n",
      "Epoch 33/100\n",
      "33820/33820 [==============================] - 2s 63us/sample - loss: 0.0925 - val_loss: 0.0901\n",
      "Epoch 34/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0917 - val_loss: 0.0902\n",
      "Epoch 35/100\n",
      "33820/33820 [==============================] - 2s 64us/sample - loss: 0.0915 - val_loss: 0.0861\n"
     ]
    }
   ],
   "source": [
    "lag_vec = [7,14]\n",
    "units_vec = [2,4,8,16,32,64,128]\n",
    "results = cross_validation(lag_vec, units_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3929.60170938 3883.32556113 3591.53441123 3490.08542461 4579.52783203\n",
      "   3062.04192485 3112.10257017]\n",
      "  [3593.45525217 3968.69343065 4109.62550561 3902.59298253 3171.15272729\n",
      "   2896.50459221 2904.81966868]]\n",
      "\n",
      " [[4876.0219254  4472.16310425 4560.25616101 4285.52128434 3638.0126453\n",
      "   3703.66528182 3562.22176277]\n",
      "  [4105.13830173 4058.85475326 3929.34595475 4214.3711666  3638.55287692\n",
      "   3244.8319458  3167.23859804]]\n",
      "\n",
      " [[5454.77015706 4803.11263204 4973.18034342 4793.28921183 4589.48162476\n",
      "   4455.83374146 4368.85466146]\n",
      "  [4480.5038385  4692.47069092 4556.72524129 4326.92134766 4257.2273641\n",
      "   4231.08220418 4262.8593455 ]]]\n",
      "14 128\n"
     ]
    }
   ],
   "source": [
    "MAE = results[2,:]\n",
    "print(MAE)\n",
    "MAE = numpy.sum(MAE, axis=0)\n",
    "ind = numpy.unravel_index(numpy.argmin(MAE, axis=None), MAE.shape)\n",
    "\n",
    "lag = lag_vec[ind[0]]\n",
    "units = units_vec[ind[1]]\n",
    "print(lag,units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34400 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "34400/34400 [==============================] - 7s 197us/sample - loss: 0.1353 - val_loss: 0.1551\n",
      "Epoch 2/100\n",
      "34400/34400 [==============================] - 2s 63us/sample - loss: 0.1171 - val_loss: 0.1529\n",
      "Epoch 3/100\n",
      "34400/34400 [==============================] - 2s 62us/sample - loss: 0.1145 - val_loss: 0.1572\n",
      "Epoch 4/100\n",
      "34400/34400 [==============================] - 2s 63us/sample - loss: 0.1136 - val_loss: 0.1585\n",
      "Epoch 5/100\n",
      "34400/34400 [==============================] - 2s 67us/sample - loss: 0.1131 - val_loss: 0.1628\n",
      "Epoch 6/100\n",
      "34400/34400 [==============================] - 2s 70us/sample - loss: 0.1137 - val_loss: 0.1642\n",
      "Epoch 7/100\n",
      "34400/34400 [==============================] - 2s 66us/sample - loss: 0.1119 - val_loss: 0.1700\n",
      "Epoch 8/100\n",
      "34400/34400 [==============================] - 2s 69us/sample - loss: 0.1112 - val_loss: 0.1726\n",
      "Epoch 9/100\n",
      "34400/34400 [==============================] - 2s 63us/sample - loss: 0.1108 - val_loss: 0.1721\n",
      "Epoch 10/100\n",
      "34400/34400 [==============================] - 2s 69us/sample - loss: 0.1104 - val_loss: 0.1689\n",
      "Epoch 11/100\n",
      "34400/34400 [==============================] - 2s 71us/sample - loss: 0.1097 - val_loss: 0.1748\n",
      "Epoch 12/100\n",
      "34400/34400 [==============================] - 2s 64us/sample - loss: 0.1091 - val_loss: 0.1775\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 11417.4189453125\n",
      "MAPE: 0.2743346374793961\n",
      "MAE: 7670.610955613659\n",
      "R2 score: 0.31702450232545454\n",
      " \n",
      " \n",
      "---------------------------------------------------\n",
      "Train on 35020 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "35020/35020 [==============================] - 8s 233us/sample - loss: 0.1355 - val_loss: 0.1267\n",
      "Epoch 2/100\n",
      "35020/35020 [==============================] - 2s 64us/sample - loss: 0.1187 - val_loss: 0.1384\n",
      "Epoch 3/100\n",
      "35020/35020 [==============================] - 2s 62us/sample - loss: 0.1159 - val_loss: 0.1490\n",
      "Epoch 4/100\n",
      "35020/35020 [==============================] - 2s 61us/sample - loss: 0.1145 - val_loss: 0.1425\n",
      "Epoch 5/100\n",
      "35020/35020 [==============================] - 2s 64us/sample - loss: 0.1137 - val_loss: 0.1412\n",
      "Epoch 6/100\n",
      "35020/35020 [==============================] - 2s 61us/sample - loss: 0.1130 - val_loss: 0.1527\n",
      "Epoch 7/100\n",
      "35020/35020 [==============================] - 2s 65us/sample - loss: 0.1127 - val_loss: 0.1480\n",
      "Epoch 8/100\n",
      "35020/35020 [==============================] - 2s 66us/sample - loss: 0.1122 - val_loss: 0.1492\n",
      "Epoch 9/100\n",
      "35020/35020 [==============================] - 2s 70us/sample - loss: 0.1113 - val_loss: 0.1489\n",
      "Epoch 10/100\n",
      "35020/35020 [==============================] - 2s 69us/sample - loss: 0.1105 - val_loss: 0.1498\n",
      "Epoch 11/100\n",
      "35020/35020 [==============================] - 2s 70us/sample - loss: 0.1102 - val_loss: 0.1511\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 9762.2158203125\n",
      "MAPE: 0.7047015502146738\n",
      "MAE: 6359.570556640625\n",
      "R2 score: -0.26533932703876495\n",
      " \n",
      " \n",
      "---------------------------------------------------\n",
      "Train on 35620 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "35620/35620 [==============================] - 7s 201us/sample - loss: 0.1371 - val_loss: 0.0833\n",
      "Epoch 2/100\n",
      "35620/35620 [==============================] - 2s 68us/sample - loss: 0.1184 - val_loss: 0.0789\n",
      "Epoch 3/100\n",
      "35620/35620 [==============================] - 2s 66us/sample - loss: 0.1162 - val_loss: 0.0785\n",
      "Epoch 4/100\n",
      "35620/35620 [==============================] - 2s 65us/sample - loss: 0.1150 - val_loss: 0.0785\n",
      "Epoch 5/100\n",
      "35620/35620 [==============================] - 2s 68us/sample - loss: 0.1139 - val_loss: 0.0779\n",
      "Epoch 6/100\n",
      "35620/35620 [==============================] - 2s 69us/sample - loss: 0.1130 - val_loss: 0.0766\n",
      "Epoch 7/100\n",
      "35620/35620 [==============================] - 2s 68us/sample - loss: 0.1128 - val_loss: 0.0779\n",
      "Epoch 8/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.1117 - val_loss: 0.0762\n",
      "Epoch 9/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.1109 - val_loss: 0.0762\n",
      "Epoch 10/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.1104 - val_loss: 0.0804\n",
      "Epoch 11/100\n",
      "35620/35620 [==============================] - 2s 65us/sample - loss: 0.1098 - val_loss: 0.0767\n",
      "Epoch 12/100\n",
      "35620/35620 [==============================] - 2s 65us/sample - loss: 0.1086 - val_loss: 0.0780\n",
      "Epoch 13/100\n",
      "35620/35620 [==============================] - 2s 70us/sample - loss: 0.1081 - val_loss: 0.0760\n",
      "Epoch 14/100\n",
      "35620/35620 [==============================] - 2s 68us/sample - loss: 0.1072 - val_loss: 0.0791\n",
      "Epoch 15/100\n",
      "35620/35620 [==============================] - 2s 65us/sample - loss: 0.1060 - val_loss: 0.0764\n",
      "Epoch 16/100\n",
      "35620/35620 [==============================] - 2s 67us/sample - loss: 0.1052 - val_loss: 0.0768\n",
      "Epoch 17/100\n",
      "35620/35620 [==============================] - 2s 69us/sample - loss: 0.1034 - val_loss: 0.0798\n",
      "Epoch 18/100\n",
      "35620/35620 [==============================] - 2s 65us/sample - loss: 0.1022 - val_loss: 0.0762\n",
      "Epoch 19/100\n",
      "35620/35620 [==============================] - 2s 63us/sample - loss: 0.1015 - val_loss: 0.0784\n",
      "Epoch 20/100\n",
      "35620/35620 [==============================] - 2s 66us/sample - loss: 0.1004 - val_loss: 0.0774\n",
      "Epoch 21/100\n",
      "35620/35620 [==============================] - 2s 69us/sample - loss: 0.0995 - val_loss: 0.0791\n",
      "Epoch 22/100\n",
      "35620/35620 [==============================] - 2s 68us/sample - loss: 0.0993 - val_loss: 0.0748\n",
      "Epoch 23/100\n",
      "35620/35620 [==============================] - 2s 67us/sample - loss: 0.0989 - val_loss: 0.0753\n",
      "Epoch 24/100\n",
      "35620/35620 [==============================] - 2s 67us/sample - loss: 0.0981 - val_loss: 0.0757\n",
      "Epoch 25/100\n",
      "35620/35620 [==============================] - 2s 68us/sample - loss: 0.0975 - val_loss: 0.0750\n",
      "Epoch 26/100\n",
      "35620/35620 [==============================] - 2s 66us/sample - loss: 0.0975 - val_loss: 0.0754\n",
      "Epoch 27/100\n",
      "35620/35620 [==============================] - 2s 63us/sample - loss: 0.0969 - val_loss: 0.0750\n",
      "Epoch 28/100\n",
      "35620/35620 [==============================] - 2s 62us/sample - loss: 0.0963 - val_loss: 0.0754\n",
      "Epoch 29/100\n",
      "35620/35620 [==============================] - 2s 62us/sample - loss: 0.0963 - val_loss: 0.0756\n",
      "Epoch 30/100\n",
      "35620/35620 [==============================] - 2s 66us/sample - loss: 0.0959 - val_loss: 0.0745\n",
      "Epoch 31/100\n",
      "35620/35620 [==============================] - 2s 66us/sample - loss: 0.0957 - val_loss: 0.0752\n",
      "Epoch 32/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.0953 - val_loss: 0.0749\n",
      "Epoch 33/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.0954 - val_loss: 0.0760\n",
      "Epoch 34/100\n",
      "35620/35620 [==============================] - 2s 67us/sample - loss: 0.0949 - val_loss: 0.0757\n",
      "Epoch 35/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.0946 - val_loss: 0.0766\n",
      "Epoch 36/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.0942 - val_loss: 0.0739\n",
      "Epoch 37/100\n",
      "35620/35620 [==============================] - 2s 66us/sample - loss: 0.0943 - val_loss: 0.0731\n",
      "Epoch 38/100\n",
      "35620/35620 [==============================] - 2s 65us/sample - loss: 0.0942 - val_loss: 0.0738\n",
      "Epoch 39/100\n",
      "35620/35620 [==============================] - 2s 66us/sample - loss: 0.0940 - val_loss: 0.0732\n",
      "Epoch 40/100\n",
      "35620/35620 [==============================] - 2s 66us/sample - loss: 0.0938 - val_loss: 0.0738\n",
      "Epoch 41/100\n",
      "35620/35620 [==============================] - 2s 60us/sample - loss: 0.0936 - val_loss: 0.0747\n",
      "Epoch 42/100\n",
      "35620/35620 [==============================] - 2s 65us/sample - loss: 0.0936 - val_loss: 0.0739\n",
      "Epoch 43/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.0931 - val_loss: 0.0720\n",
      "Epoch 44/100\n",
      "35620/35620 [==============================] - 2s 65us/sample - loss: 0.0930 - val_loss: 0.0740\n",
      "Epoch 45/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.0930 - val_loss: 0.0746\n",
      "Epoch 46/100\n",
      "35620/35620 [==============================] - 2s 67us/sample - loss: 0.0926 - val_loss: 0.0738\n",
      "Epoch 47/100\n",
      "35620/35620 [==============================] - 2s 63us/sample - loss: 0.0924 - val_loss: 0.0733\n",
      "Epoch 48/100\n",
      "35620/35620 [==============================] - 2s 62us/sample - loss: 0.0922 - val_loss: 0.0736\n",
      "Epoch 49/100\n",
      "35620/35620 [==============================] - 2s 68us/sample - loss: 0.0921 - val_loss: 0.0739\n",
      "Epoch 50/100\n",
      "35620/35620 [==============================] - 2s 67us/sample - loss: 0.0924 - val_loss: 0.0719\n",
      "Epoch 51/100\n",
      "35620/35620 [==============================] - 2s 67us/sample - loss: 0.0922 - val_loss: 0.0729\n",
      "Epoch 52/100\n",
      "35620/35620 [==============================] - 2s 68us/sample - loss: 0.0921 - val_loss: 0.0729\n",
      "Epoch 53/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.0919 - val_loss: 0.0753\n",
      "Epoch 54/100\n",
      "35620/35620 [==============================] - 2s 63us/sample - loss: 0.0914 - val_loss: 0.0766\n",
      "Epoch 55/100\n",
      "35620/35620 [==============================] - 2s 62us/sample - loss: 0.0915 - val_loss: 0.0739\n",
      "Epoch 56/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.0913 - val_loss: 0.0722\n",
      "Epoch 57/100\n",
      "35620/35620 [==============================] - 2s 67us/sample - loss: 0.0912 - val_loss: 0.0729\n",
      "Epoch 58/100\n",
      "35620/35620 [==============================] - 2s 62us/sample - loss: 0.0913 - val_loss: 0.0747\n",
      "Epoch 59/100\n",
      "35620/35620 [==============================] - 2s 64us/sample - loss: 0.0909 - val_loss: 0.0730\n",
      "Epoch 60/100\n",
      "35620/35620 [==============================] - 2s 63us/sample - loss: 0.0908 - val_loss: 0.0740\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 5482.359375\n",
      "MAPE: 0.132677282176648\n",
      "MAE: 3608.950357547883\n",
      "R2 score: 0.4963696975662908\n",
      " \n",
      " \n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# lag=14\n",
    "ds=['october','november','december']\n",
    "\n",
    "data_LSTM_X1,data_LSTM_X2,data_LSTM_Y = sequence_data_build(data, lag)\n",
    "totday=int(data_LSTM_X1.shape[0]/20)\n",
    "m1 = [totday-61, totday-31, totday, ]\n",
    "m2 = [m1[0]-31, m1[1]-30, m1[2]-31, ]\n",
    "for i in range(len(m1)):\n",
    "    trainX1, trainY, testX1, testY= train_test_build(data_LSTM_X1, data_LSTM_Y, m1[i], m2[i])\n",
    "    trainX2, trainY, testX2, testY= train_test_build(data_LSTM_X2, data_LSTM_Y, m1[i], m2[i])\n",
    "    testYcopy=testY\n",
    "    saving = True\n",
    "    EarlyStop = True\n",
    "    rmse, mape, mae, r2, history = model_build(trainX1, trainX2, testX1, testX2, trainY, testY, 64, saving, ds[i], EarlyStop)\n",
    "        \n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"Epochs\")\n",
    "    pyplot.ylabel(\"Mean absolute error\")\n",
    "    pyplot.savefig(\"PM2F_\" + ds[i] + \".png\")\n",
    "    pyplot.show()\n",
    "    \n",
    "    \n",
    "    print(\"Root mean square error: {0}\".format(rmse))\n",
    "    print(\"MAPE: {0}\".format(mape))\n",
    "    print(\"MAE: {0}\".format(mae))\n",
    "    print(\"R2 score: {0}\".format(r2))\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lag vector\n",
      "[7, 14]\n",
      "LSTM units\n",
      "[2, 4, 8, 16, 32, 64, 128]\n",
      "Cross validation results\n",
      "14 128\n",
      "----------------------------\n",
      "RMSE\n",
      "[[[5223.23974609 5183.72216797 4739.52490234 4682.44726562 6032.97949219\n",
      "   4271.42431641 4328.53369141]\n",
      "  [4798.86230469 5252.18505859 5409.08642578 5071.21826172 4350.84375\n",
      "   4081.88354492 4087.23852539]]\n",
      "\n",
      " [[6483.26025391 5898.38232422 5968.90380859 5656.96289062 5200.71582031\n",
      "   5088.54052734 4946.47509766]\n",
      "  [5509.51171875 5435.91894531 5368.37011719 5527.95410156 5017.94042969\n",
      "   4558.94287109 4398.53955078]]\n",
      "\n",
      " [[8033.62060547 7251.34179688 7423.62646484 7190.68261719 7042.26464844\n",
      "   6943.33251953 6935.40673828]\n",
      "  [6820.96728516 7052.44042969 6849.46875    6599.36914062 6602.51855469\n",
      "   6552.71777344 6654.85253906]]]\n",
      "----------------------------\n",
      "MAE\n",
      "[[[0.21876315 0.18891257 0.24406128 0.17137016 0.29816363 0.1337827\n",
      "   0.12840522]\n",
      "  [0.18041701 0.1922496  0.19597865 0.20177413 0.16011434 0.11450462\n",
      "   0.12082384]]\n",
      "\n",
      " [[0.27918116 0.25648338 0.2772354  0.24532896 0.16448788 0.17477477\n",
      "   0.15225948]\n",
      "  [0.21471318 0.22411881 0.17786195 0.23242133 0.19125866 0.13154544\n",
      "   0.13344621]]\n",
      "\n",
      " [[0.23987887 0.18395984 0.22947165 0.21393974 0.19526394 0.17322613\n",
      "   0.1725356 ]\n",
      "  [0.16648799 0.18177922 0.180583   0.16971304 0.16692999 0.16011278\n",
      "   0.15850162]]]\n",
      "----------------------------\n",
      "MAPE\n",
      "[[[3929.60170938 3883.32556113 3591.53441123 3490.08542461 4579.52783203\n",
      "   3062.04192485 3112.10257017]\n",
      "  [3593.45525217 3968.69343065 4109.62550561 3902.59298253 3171.15272729\n",
      "   2896.50459221 2904.81966868]]\n",
      "\n",
      " [[4876.0219254  4472.16310425 4560.25616101 4285.52128434 3638.0126453\n",
      "   3703.66528182 3562.22176277]\n",
      "  [4105.13830173 4058.85475326 3929.34595475 4214.3711666  3638.55287692\n",
      "   3244.8319458  3167.23859804]]\n",
      "\n",
      " [[5454.77015706 4803.11263204 4973.18034342 4793.28921183 4589.48162476\n",
      "   4455.83374146 4368.85466146]\n",
      "  [4480.5038385  4692.47069092 4556.72524129 4326.92134766 4257.2273641\n",
      "   4231.08220418 4262.8593455 ]]]\n",
      "----------------------------\n",
      "R2-score\n",
      "[[[0.79943205 0.80245544 0.8348604  0.83881398 0.7324251  0.86586967\n",
      "   0.86225899]\n",
      "  [0.83069954 0.79720293 0.78490542 0.81093715 0.86083547 0.87750938\n",
      "   0.87718778]]\n",
      "\n",
      " [[0.6914119  0.74457813 0.73843394 0.76505896 0.80142779 0.80990148\n",
      "   0.8203679 ]\n",
      "  [0.77714698 0.78306072 0.78841879 0.77565256 0.81513986 0.84741194\n",
      "   0.85796046]]\n",
      "\n",
      " [[0.6173748  0.68826349 0.67327441 0.69345715 0.7059809  0.71418379\n",
      "   0.71483593]\n",
      "  [0.72416912 0.70513056 0.7218592  0.74180027 0.7415538  0.74543785\n",
      "   0.73744044]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"lag vector\")\n",
    "print(lag_vec)\n",
    "print(\"LSTM units\")\n",
    "print(units_vec)\n",
    "print(\"Cross validation results\")\n",
    "print(lag,units)\n",
    "print(\"----------------------------\")\n",
    "print(\"RMSE\")\n",
    "print(results[0,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"MAE\")\n",
    "print(results[1,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"MAPE\")\n",
    "print(results[2,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"R2-score\")\n",
    "print(results[3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14260.39379185 13158.60129742 13124.97091566 12568.89592078\n",
      "  12807.02210209 11221.54094813 11043.1789944 ]\n",
      " [12179.0973924  12720.01887483 12595.69670165 12443.88549679\n",
      "  11066.9329683  10372.41874219 10334.91761222]]\n"
     ]
    }
   ],
   "source": [
    "print(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Root mean square error: 11242.35546875\n",
    "MAPE: 0.2818463959398697\n",
    "MAE: 7585.569150075605\n",
    "R2 score: 0.33780802147786104\n",
    "    \n",
    "    Root mean square error: 9881.5869140625\n",
    "MAPE: 0.6899350744815699\n",
    "MAE: 6406.98930501302\n",
    "R2 score: -0.29647326541825714\n",
    "    \n",
    "Root mean square error: 5494.4228515625\n",
    "MAPE: 0.13161742438101248\n",
    "MAE: 3545.5047709803425\n",
    "R2 score: 0.4941508412434956"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
