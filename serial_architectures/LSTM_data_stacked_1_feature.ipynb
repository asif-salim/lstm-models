{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,  mean_absolute_error, r2_score\n",
    " \n",
    "# tensorflow.reset_default_graph()\n",
    "tensorflow.random.set_seed(0)\n",
    "# random.seed(0)\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=14, day_offset=5):\n",
    "    dataX, dataY= [],[]\n",
    "    dataX=numpy.zeros([(len(dataset)-look_back),2,look_back])\n",
    "    \n",
    "    #print(dataX.shape)\n",
    "    for i in range(look_back,len(dataset)):\n",
    "       # print(i)\n",
    "        a = numpy.zeros([2,look_back])\n",
    "        t1=dataset[(i-look_back):i, 0]\n",
    "        t1=numpy.reshape(t1,[1,look_back])\n",
    "        t4=dataset[(i-look_back):i, 48]\n",
    "        t4=numpy.reshape(t4,[1,look_back])\n",
    "        #print(t1.shape)\n",
    "#         t2=dataset[i, 48-look_back:48]\n",
    "#         t2=numpy.reshape(t2,[1,look_back])\n",
    "#         t6=dataset[i,-(look_back+3):-3]\n",
    "#         t6=numpy.reshape(t6,[1,look_back])\n",
    "#         t3=numpy.zeros([1,look_back])\n",
    "#         if i>=((day_offset+1)*7+look_back):\n",
    "#             t3[0,0:day_offset]=[dataset[j,0] for j in range(i-(((day_offset+1)*7)+look_back),i-(look_back+7),7)]\n",
    "#         t5=numpy.zeros([1,look_back])\n",
    "#         if i>=((day_offset+1)*7+look_back):\n",
    "#             t5[0,0:day_offset]=[dataset[j,48] for j in range(i-(((day_offset+1)*7)+look_back),i-(look_back+7),7)]\n",
    "            \n",
    "            \n",
    "        #print(t2.shape)\n",
    "        a[0,:] = t1\n",
    "        a[1,:] = t4\n",
    "#         a[2,:] = t2\n",
    "#         a[3,:] = t6\n",
    "        \n",
    "        dataX[i-look_back,:,:]=a\n",
    "#         a = numpy.concatenate([dataset[(i-look_back-7):i-7, 0], dataset[i,-14:]],axis=1)\n",
    "        \n",
    "        #dataX.append(a)\n",
    "        dataY.append(dataset[i,-1])\n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(datarange, categorical=[]):  \n",
    "    datarange= pd.DataFrame(datarange)\n",
    "    if not categorical:\n",
    "        meandata=datarange.mean()\n",
    "        meandata=meandata.to_numpy()\n",
    "    else:\n",
    "        meandata=datarange.mean()\n",
    "        meandata=meandata.to_numpy()\n",
    "        \n",
    "        modedata = datarange.mode()\n",
    "        modedata = modedata.to_numpy()\n",
    "        modedata = modedata[0,:]\n",
    "        \n",
    "        for i in categorical:\n",
    "                meandata[i-1] = modedata[i]\n",
    "                \n",
    "    datetime_series = pd.to_datetime(datarange['fltdat'])\n",
    "    miss_idx=pd.date_range(start = '01-01-2015', end = '31-12-2019' ).difference(datetime_series)\n",
    "    datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
    "    datarange=datarange.set_index(datetime_index)\n",
    "\n",
    "    datarange.drop('fltdat',axis=1,inplace=True)\n",
    "    newidx = pd.date_range('01-01-2015', '31-12-2019')\n",
    "    datarange = datarange.reindex(newidx, fill_value=0)\n",
    "    \n",
    "    meandata=meandata.reshape(1,meandata.shape[0])\n",
    "    dat = numpy.tile(meandata, [miss_idx.shape[0],1])\n",
    "    datarange.loc[miss_idx]=dat\n",
    "    return datarange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_test, y_pred):\n",
    "        import numpy as np\n",
    "        t = np.array(y_test)\n",
    "        p = np.array(y_pred)\n",
    "        mae = list()\n",
    "        mape = list()\n",
    "        for i in range(len(t)):\n",
    "            if (t[i] == 0):\n",
    "                mae.append(abs(p[i]))\n",
    "            else:\n",
    "                mae.append(float(abs(t[i] - p[i])))\n",
    "                mape.append(float(abs((t[i] - p[i])/t[i])))\n",
    "        return np.mean(mae) , np.mean(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fltdat', 'paxcntfc', 'fltnum', 'legorg', 'legdst', 'acrtypcod',\n",
      "       'keyidr', 'totpaylodwgt', 'totpaylodvol', 'totpaylodpos', 'totsetfc',\n",
      "       'totpaxwgt', 'dp_51', 'dp_50', 'dp_49', 'dp_48', 'dp_47', 'dp_46',\n",
      "       'dp_45', 'dp_44', 'dp_43', 'dp_42', 'dp_41', 'dp_40', 'dp_39', 'dp_38',\n",
      "       'dp_37', 'dp_36', 'dp_35', 'dp_34', 'dp_33', 'dp_32', 'dp_31', 'dp_30',\n",
      "       'dp_29', 'dp_28', 'dp_27', 'dp_26', 'dp_25', 'dp_24', 'dp_23', 'dp_22',\n",
      "       'dp_21', 'dp_20', 'dp_19', 'dp_18', 'dp_17', 'dp_16', 'dp_15', 'dp_14',\n",
      "       'dp_13', 'dp_12', 'dp_11', 'dp_10', 'dp_9', 'dp_8', 'dp_7', 'dp_6',\n",
      "       'dp_5', 'dp_4', 'dp_3', 'dp_2', 'dp_1'],\n",
      "      dtype='object')\n",
      "Index(['fltdat', 'paxcntfc', 'acrtypcod', 'totpaylodwgt', 'totpaxwgt', 'dp_51',\n",
      "       'dp_50', 'dp_49', 'dp_48', 'dp_47', 'dp_46', 'dp_45', 'dp_44', 'dp_43',\n",
      "       'dp_42', 'dp_41', 'dp_40', 'dp_39', 'dp_38', 'dp_37', 'dp_36', 'dp_35',\n",
      "       'dp_34', 'dp_33', 'dp_32', 'dp_31', 'dp_30', 'dp_29', 'dp_28', 'dp_27',\n",
      "       'dp_26', 'dp_25', 'dp_24', 'dp_23', 'dp_22', 'dp_21', 'dp_20', 'dp_19',\n",
      "       'dp_18', 'dp_17', 'dp_16', 'dp_15', 'dp_14', 'dp_13', 'dp_12', 'dp_11',\n",
      "       'dp_10', 'dp_9', 'dp_8', 'paxcnty', 'dcp_51', 'dcp_50', 'dcp_49',\n",
      "       'dcp_48', 'dcp_47', 'dcp_46', 'dcp_45', 'dcp_44', 'dcp_43', 'dcp_42',\n",
      "       'dcp_41', 'dcp_40', 'dcp_39', 'dcp_38', 'dcp_37', 'dcp_36', 'dcp_35',\n",
      "       'dcp_34', 'dcp_33', 'dcp_3', 'dcp_31', 'dcp_30', 'dcp_29', 'dcp_28',\n",
      "       'dcp_27', 'dcp_26', 'dcp_25', 'dcp_24', 'dcp_23', 'dcp_22', 'dcp_21',\n",
      "       'dcp_20', 'dcp_19', 'dcp_18', 'dcp_17', 'dcp_16', 'dcp_15', 'dcp_14',\n",
      "       'dcp_13', 'dcp_12', 'dcp_11', 'dcp_10', 'dcp_9', 'dcp_8'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('data/rmscapfc.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "print(dataframe.columns)\n",
    "dataframe.drop(dataframe.columns[[2,3,4,6,8,9,10,56,57,58,59,60,61,62] ], axis=1, inplace=True)\n",
    "\n",
    "dataframe2 = read_csv('data/rmscapy.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "# print(dataframe2.columns)\n",
    "f_column = dataframe2[[\"paxcnty\", \"dcp_51\", \"dcp_50\", \"dcp_49\", \"dcp_48\", \"dcp_47\", \"dcp_46\",\n",
    "       \"dcp_45\", \"dcp_44\", \"dcp_43\", \"dcp_42\", \"dcp_41\", \"dcp_40\", \"dcp_39\",\n",
    "       \"dcp_38\", \"dcp_37\", \"dcp_36\", \"dcp_35\", \"dcp_34\", \"dcp_33\", \"dcp_3\",\n",
    "       \"dcp_31\", \"dcp_30\", \"dcp_29\", \"dcp_28\", \"dcp_27\", \"dcp_26\", \"dcp_25\",\n",
    "       \"dcp_24\", \"dcp_23\", \"dcp_22\", \"dcp_21\", \"dcp_20\", \"dcp_19\", \"dcp_18\",\n",
    "       \"dcp_17\", \"dcp_16\", \"dcp_15\", \"dcp_14\", \"dcp_13\", \"dcp_12\", \"dcp_11\",\n",
    "       \"dcp_10\", \"dcp_9\", \"dcp_8\"]]\n",
    " \n",
    "\n",
    "dataframe = pd.concat([dataframe,f_column], axis = 1)\n",
    "# print(dataframe.columns)\n",
    "dataframe3 = read_csv('data/uldfc.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "\n",
    "dataframe3.drop(dataframe3.columns[[1,2,3] ], axis=1, inplace=True)\n",
    "dataframe4 = read_csv('data/uldy.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "dataframe4.drop(dataframe4.columns[[1,2,3] ], axis=1, inplace=True)\n",
    "f_column = dataframe4[[\"county\"]]\n",
    "dataframe3 = pd.concat([dataframe3,f_column], axis = 1)\n",
    "print(dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM=[0,1825,1825,1817,1816,1812,1821,1825,1824,1819,1825,1825,1824,1826,1819,1825,1822,1823,1813, 1826, 1820]\n",
    "NUMuld=[0,1817,1574,1808,1802,1807,1808,1730,1817,1385,1820,1816,1606,1819,1810,1817,421,1813,434,1532,1814]\n",
    "cat_inp=[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iist\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  import sys\n",
      "C:\\Users\\iist\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# dataset, datasetY = numpy.empty([1805,3,look_back]), []\n",
    "for i in range(0,len(NUM)-1):\n",
    "#     print(i)\n",
    "    datasub = dataframe.iloc[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "#     datasub=datasetall[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "    datasub = missing_values(datasub, cat_inp)\n",
    "    datasub = datasub.values\n",
    "    datasub = datasub.astype('float32')\n",
    "    \n",
    "    datasubuld = dataframe3.iloc[sum(NUMuld[0:i+1]):sum(NUMuld[0:i+2]),:]\n",
    "#     datasub=datasetall[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "    datasubuld = missing_values(datasubuld)\n",
    "    datasubuld = datasubuld.values\n",
    "    datasubuld = datasubuld.astype('float32')\n",
    "    \n",
    "    datasub = numpy.concatenate([datasub, datasubuld], axis=1)\n",
    "    if i==0:\n",
    "        data = datasub\n",
    "    else:\n",
    "        data = numpy.concatenate([data, datasub], axis =0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36520, 96)\n"
     ]
    }
   ],
   "source": [
    "out = data[:,2] - data[:,3] - (data[:,-1]*data[:,-2]*114)\n",
    "out = out.reshape(out.shape[0],1)\n",
    "data = numpy.concatenate([data,out], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_data_build(data, look_back):\n",
    "    for i in range(0,20):\n",
    "        datasub = data[i*1826:((i+1)*1826),:]\n",
    "        X, Y = create_dataset(datasub, look_back)\n",
    "        Y = Y.reshape(Y.shape[0],1)\n",
    "        if i==0: \n",
    "            data_LSTM_X = X\n",
    "            data_LSTM_Y = Y\n",
    "        else:\n",
    "            data_LSTM_X = numpy.concatenate([data_LSTM_X, X],axis=0)\n",
    "            data_LSTM_Y = numpy.concatenate([data_LSTM_Y, Y],axis=0)\n",
    "    return data_LSTM_X, data_LSTM_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_build(X, Y, m1, m2):\n",
    "    spliter = int(X.shape[0]/20)\n",
    "    tot_len = m1\n",
    "    train_len = m2\n",
    "    for i in range(0,20):\n",
    "        Xsub = X[i*1826:((i+1)*1826),:]\n",
    "        Ysub = Y[i*1826:((i+1)*1826)]\n",
    "        if i==0:\n",
    "            trainX = Xsub[0:m2,:]\n",
    "            testX = Xsub[m2:m1,:]\n",
    "            trainY = Ysub[0:m2]\n",
    "            testY = Ysub[m2:m1]\n",
    "        else:\n",
    "            trainX = numpy.concatenate([trainX, Xsub[0:m2,:]], axis=0)\n",
    "            testX = numpy.concatenate([testX, Xsub[m2:m1,:]], axis=0)\n",
    "            trainY = numpy.concatenate([trainY, Ysub[0:m2]], axis=0)\n",
    "            testY = numpy.concatenate([testY, Ysub[m2:m1]], axis=0)\n",
    "        \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(trainX, trainY, testX, testY, units, saving =False, month=None, EarlyStop = False):\n",
    "    if EarlyStop:\n",
    "        callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                                            mode = 'min', restore_best_weights=True)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    if EarlyStop:\n",
    "        history = model.fit(trainX, trainY, epochs=100, batch_size=150, validation_data=(testX, testY),verbose=1,\n",
    "                            callbacks=[callback])\n",
    "    else:\n",
    "        history = model.fit(trainX, trainY, epochs=100, batch_size=150, validation_data=(testX, testY),verbose=1, )#callbacks=[callback]\n",
    "        \n",
    "    testPredict = model.predict(testX)\n",
    "                \n",
    "    sh = testPredict.shape\n",
    "    inv_yhat = testPredict.reshape(sh[0]*sh[1],1)\n",
    "    inv_yhat = numpy.concatenate([ data[0:inv_yhat.shape[0],0:95], inv_yhat], axis=1)\n",
    "\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "    testY = testY.reshape(sh[0]*sh[1],1)\n",
    " \n",
    "    inv_y = numpy.concatenate([data[0:testY.shape[0],0:95], testY], axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-1]\n",
    "             \n",
    "    rmse = numpy.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    mae, mape = mean_absolute_percentage_error(inv_y, inv_yhat) \n",
    "    r2 = r2_score(inv_y, inv_yhat)\n",
    "    \n",
    "    res=[]\n",
    "    if saving:\n",
    "        inv_y = inv_y.reshape(inv_y.shape[0],1) \n",
    "        inv_yhat = inv_yhat.reshape(inv_yhat.shape[0],1) \n",
    "        res = numpy.concatenate([inv_y, inv_yhat], axis=1)\n",
    "        df = pd.DataFrame(res)\n",
    "        res = df.to_csv(\"ds_\" + month + \".csv\", index = False)\n",
    "    return rmse, mape,mae,r2, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(lag_vec = [7,14,21,28], units_vec = [2,4,8,16,32,64,128]):\n",
    "    results = numpy.zeros([4,3,len(lag_vec),len(units_vec)])\n",
    "    \n",
    "    for folds in range(0,3):\n",
    "        for i in range(0,len(lag_vec)):\n",
    "            data_LSTM_X, data_LSTM_Y = sequence_data_build(data, lag_vec[i])\n",
    "            totday = int(data_LSTM_X.shape[0]/20)\n",
    "            m1 = [totday-152, totday-121, totday-91, ]\n",
    "            m2 = [m1[0]-31, m1[1]-31, m1[2]-30, ]\n",
    "            lag = lag_vec[i]\n",
    "            trainX, trainY, testX, testY= train_test_build(data_LSTM_X, data_LSTM_Y, m1[folds], m2[folds])\n",
    "            testYcopy=testY\n",
    "            for j in range(0, len(units_vec)):\n",
    "                print(\" \")\n",
    "                print(\" \")\n",
    "                print(\" \")\n",
    "                print(\"------------------------------------------------\")\n",
    "                print(\"fold: {0}, lag: {1}, units: {2}\".format(folds, lag_vec[i], units_vec[j]))\n",
    "                units = units_vec[j]\n",
    "                rmse, mape, mae, r2, his = model_build(trainX, trainY, testX, testY, units, EarlyStop=True) \n",
    "                results[0,folds,i,j] = rmse \n",
    "                results[2,folds,i,j], results[1,folds,i,j] = mae, mape\n",
    "                results[3,folds,i,j] = r2\n",
    "    return results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 2\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 6s 196us/sample - loss: 0.1856 - val_loss: 0.1574\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1578 - val_loss: 0.1514\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1429 - val_loss: 0.1422\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1373 - val_loss: 0.1375\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1345 - val_loss: 0.1346\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1328 - val_loss: 0.1331\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1316 - val_loss: 0.1323\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1306 - val_loss: 0.1308\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1298 - val_loss: 0.1304\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1291 - val_loss: 0.1300\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1285 - val_loss: 0.1298\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1279 - val_loss: 0.1289\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1276 - val_loss: 0.1289\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1272 - val_loss: 0.1289\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1268 - val_loss: 0.1296\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1265 - val_loss: 0.1286\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1262 - val_loss: 0.1287\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1259 - val_loss: 0.1282\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1256 - val_loss: 0.1278\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1254 - val_loss: 0.1283\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1252 - val_loss: 0.1279\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1249 - val_loss: 0.1277\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1248 - val_loss: 0.1273\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1246 - val_loss: 0.1271\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1244 - val_loss: 0.1270\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1243 - val_loss: 0.1273\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.1241 - val_loss: 0.1268\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.1240 - val_loss: 0.1268\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1239 - val_loss: 0.1268\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1237 - val_loss: 0.1264\n",
      "Epoch 31/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1236 - val_loss: 0.1267\n",
      "Epoch 32/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1235 - val_loss: 0.1266\n",
      "Epoch 33/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1234 - val_loss: 0.1267\n",
      "Epoch 34/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1233 - val_loss: 0.1263\n",
      "Epoch 35/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1232 - val_loss: 0.1264\n",
      "Epoch 36/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1230 - val_loss: 0.1264\n",
      "Epoch 37/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1229 - val_loss: 0.1262\n",
      "Epoch 38/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1228 - val_loss: 0.1264\n",
      "Epoch 39/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1227 - val_loss: 0.1265\n",
      "Epoch 40/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1226 - val_loss: 0.1260\n",
      "Epoch 41/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1225 - val_loss: 0.1265\n",
      "Epoch 42/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1224 - val_loss: 0.1262\n",
      "Epoch 43/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1223 - val_loss: 0.1265\n",
      "Epoch 44/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1223 - val_loss: 0.1263\n",
      "Epoch 45/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1222 - val_loss: 0.1264\n",
      "Epoch 46/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1221 - val_loss: 0.1263\n",
      "Epoch 47/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1220 - val_loss: 0.1262\n",
      "Epoch 48/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1219 - val_loss: 0.1262\n",
      "Epoch 49/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1218 - val_loss: 0.1264\n",
      "Epoch 50/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1218 - val_loss: 0.1262\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 4\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 5s 147us/sample - loss: 0.1691 - val_loss: 0.1413\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 50us/sample - loss: 0.1361 - val_loss: 0.1349\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1326 - val_loss: 0.1333\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1311 - val_loss: 0.1324\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1304 - val_loss: 0.1307\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1297 - val_loss: 0.1298\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1292 - val_loss: 0.1287\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1289 - val_loss: 0.1277\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1285 - val_loss: 0.1272\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1282 - val_loss: 0.1273\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1278 - val_loss: 0.1276\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1274 - val_loss: 0.1267\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1272 - val_loss: 0.1276\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1267 - val_loss: 0.1265\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1265 - val_loss: 0.1262\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1263 - val_loss: 0.1256\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1260 - val_loss: 0.1257\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 50us/sample - loss: 0.1257 - val_loss: 0.1251\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1255 - val_loss: 0.1255\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1253 - val_loss: 0.1248\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 46us/sample - loss: 0.1252 - val_loss: 0.1258\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1248 - val_loss: 0.1257\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1246 - val_loss: 0.1255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1245 - val_loss: 0.1257\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1242 - val_loss: 0.1260\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1242 - val_loss: 0.1258\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1238 - val_loss: 0.1253\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1237 - val_loss: 0.1256\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1235 - val_loss: 0.1253\n",
      "Epoch 30/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1234 - val_loss: 0.1255\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 8\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 5s 139us/sample - loss: 0.1687 - val_loss: 0.1441\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1387 - val_loss: 0.1380\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1340 - val_loss: 0.1347\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1312 - val_loss: 0.1326\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1292 - val_loss: 0.1305\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1279 - val_loss: 0.1301\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1271 - val_loss: 0.1291\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1266 - val_loss: 0.1280\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1262 - val_loss: 0.1276\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.1258 - val_loss: 0.1275\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1255 - val_loss: 0.1271\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1251 - val_loss: 0.1267\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1247 - val_loss: 0.1267\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1245 - val_loss: 0.1264\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 49us/sample - loss: 0.1241 - val_loss: 0.1263\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1239 - val_loss: 0.1264\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1236 - val_loss: 0.1267\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1233 - val_loss: 0.1266\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1230 - val_loss: 0.1253\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1226 - val_loss: 0.1261\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1224 - val_loss: 0.1255\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1220 - val_loss: 0.1254\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 50us/sample - loss: 0.1217 - val_loss: 0.1256\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1214 - val_loss: 0.1258\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 61us/sample - loss: 0.1210 - val_loss: 0.1264\n",
      "Epoch 26/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1208 - val_loss: 0.1272\n",
      "Epoch 27/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1203 - val_loss: 0.1262\n",
      "Epoch 28/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1199 - val_loss: 0.1259\n",
      "Epoch 29/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1196 - val_loss: 0.1262\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 16\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 4s 137us/sample - loss: 0.1587 - val_loss: 0.1317\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1330 - val_loss: 0.1301\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 48us/sample - loss: 0.1303 - val_loss: 0.1271\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1284 - val_loss: 0.1250\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1277 - val_loss: 0.1246\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 50us/sample - loss: 0.1269 - val_loss: 0.1249\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1264 - val_loss: 0.1255\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1260 - val_loss: 0.1260\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 49us/sample - loss: 0.1252 - val_loss: 0.1265\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1249 - val_loss: 0.1283\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 50us/sample - loss: 0.1246 - val_loss: 0.1275\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 50us/sample - loss: 0.1244 - val_loss: 0.1287\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1238 - val_loss: 0.1284\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1235 - val_loss: 0.1297\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1233 - val_loss: 0.1316\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 32\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 4s 137us/sample - loss: 0.1571 - val_loss: 0.1319\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1331 - val_loss: 0.1306\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1311 - val_loss: 0.1270\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 46us/sample - loss: 0.1291 - val_loss: 0.1229\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1282 - val_loss: 0.1234\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1272 - val_loss: 0.1233\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1268 - val_loss: 0.1231\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1260 - val_loss: 0.1255\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1256 - val_loss: 0.1248\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 50us/sample - loss: 0.1251 - val_loss: 0.1255\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1249 - val_loss: 0.1259\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1246 - val_loss: 0.1274\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 49us/sample - loss: 0.1240 - val_loss: 0.1278\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1235 - val_loss: 0.1297\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 64\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 5s 138us/sample - loss: 0.1513 - val_loss: 0.1326\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1330 - val_loss: 0.1313\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1312 - val_loss: 0.1262\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1295 - val_loss: 0.1231\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1290 - val_loss: 0.1240\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1275 - val_loss: 0.1240\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1272 - val_loss: 0.1238\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.1261 - val_loss: 0.1257\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.1259 - val_loss: 0.1248\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 61us/sample - loss: 0.1253 - val_loss: 0.1268\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 62us/sample - loss: 0.1249 - val_loss: 0.1261\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.1250 - val_loss: 0.1264\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1239 - val_loss: 0.1282\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 49us/sample - loss: 0.1237 - val_loss: 0.1293\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 128\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 5s 143us/sample - loss: 0.1491 - val_loss: 0.1369\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.1326 - val_loss: 0.1271\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1314 - val_loss: 0.1261\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1293 - val_loss: 0.1233\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1291 - val_loss: 0.1254\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1273 - val_loss: 0.1239\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1271 - val_loss: 0.1240\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1262 - val_loss: 0.1280\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.1257 - val_loss: 0.1273\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1251 - val_loss: 0.1289\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.1246 - val_loss: 0.1292\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.1244 - val_loss: 0.1285\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.1230 - val_loss: 0.1317\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.1228 - val_loss: 0.1318\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 2\n",
      "Train on 32497 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "32497/32497 [==============================] - 5s 140us/sample - loss: 0.1860 - val_loss: 0.1648\n",
      "Epoch 2/100\n",
      "32497/32497 [==============================] - 2s 47us/sample - loss: 0.1495 - val_loss: 0.1431\n",
      "Epoch 3/100\n",
      "32497/32497 [==============================] - 2s 50us/sample - loss: 0.1397 - val_loss: 0.1331\n",
      "Epoch 4/100\n",
      "32497/32497 [==============================] - 2s 49us/sample - loss: 0.1345 - val_loss: 0.1270\n",
      "Epoch 5/100\n",
      "32497/32497 [==============================] - 2s 48us/sample - loss: 0.1312 - val_loss: 0.1227\n",
      "Epoch 6/100\n",
      "32497/32497 [==============================] - 2s 49us/sample - loss: 0.1290 - val_loss: 0.1186\n",
      "Epoch 7/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1273 - val_loss: 0.1154\n",
      "Epoch 8/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1264 - val_loss: 0.1144\n",
      "Epoch 9/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1253 - val_loss: 0.1119\n",
      "Epoch 10/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1246 - val_loss: 0.1093\n",
      "Epoch 11/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1241 - val_loss: 0.1079\n",
      "Epoch 12/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1236 - val_loss: 0.1078\n",
      "Epoch 13/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1233 - val_loss: 0.1067\n",
      "Epoch 14/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1230 - val_loss: 0.1067\n",
      "Epoch 15/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1227 - val_loss: 0.1075\n",
      "Epoch 16/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1224 - val_loss: 0.1071\n",
      "Epoch 17/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1223 - val_loss: 0.1061\n",
      "Epoch 18/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1221 - val_loss: 0.1046\n",
      "Epoch 19/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1219 - val_loss: 0.1040\n",
      "Epoch 20/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1218 - val_loss: 0.1051\n",
      "Epoch 21/100\n",
      "32497/32497 [==============================] - 2s 58us/sample - loss: 0.1216 - val_loss: 0.1035\n",
      "Epoch 22/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1215 - val_loss: 0.1050\n",
      "Epoch 23/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1214 - val_loss: 0.1049\n",
      "Epoch 24/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1213 - val_loss: 0.1045\n",
      "Epoch 25/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1212 - val_loss: 0.1036\n",
      "Epoch 26/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1210 - val_loss: 0.1033\n",
      "Epoch 27/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1210 - val_loss: 0.1045\n",
      "Epoch 28/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1208 - val_loss: 0.1037\n",
      "Epoch 29/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1206 - val_loss: 0.1033\n",
      "Epoch 30/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1205 - val_loss: 0.1034\n",
      "Epoch 31/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1205 - val_loss: 0.1049\n",
      "Epoch 32/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1204 - val_loss: 0.1050\n",
      "Epoch 33/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1203 - val_loss: 0.1041\n",
      "Epoch 34/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1202 - val_loss: 0.1036\n",
      "Epoch 35/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1201 - val_loss: 0.1039\n",
      "Epoch 36/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1201 - val_loss: 0.1048\n",
      "Epoch 37/100\n",
      "32497/32497 [==============================] - 2s 59us/sample - loss: 0.1199 - val_loss: 0.1036\n",
      "Epoch 38/100\n",
      "32497/32497 [==============================] - 2s 58us/sample - loss: 0.1198 - val_loss: 0.1029\n",
      "Epoch 39/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1197 - val_loss: 0.1042\n",
      "Epoch 40/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1197 - val_loss: 0.1044\n",
      "Epoch 41/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1196 - val_loss: 0.1040\n",
      "Epoch 42/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1195 - val_loss: 0.1031\n",
      "Epoch 43/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1194 - val_loss: 0.1032\n",
      "Epoch 44/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1193 - val_loss: 0.1039\n",
      "Epoch 45/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1192 - val_loss: 0.1049\n",
      "Epoch 46/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1192 - val_loss: 0.1041\n",
      "Epoch 47/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1191 - val_loss: 0.1049\n",
      "Epoch 48/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1190 - val_loss: 0.1034\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 4\n",
      "Train on 32497 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "32497/32497 [==============================] - 4s 136us/sample - loss: 0.1690 - val_loss: 0.1486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1382 - val_loss: 0.1271\n",
      "Epoch 3/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1320 - val_loss: 0.1215\n",
      "Epoch 4/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1294 - val_loss: 0.1188\n",
      "Epoch 5/100\n",
      "32497/32497 [==============================] - 2s 50us/sample - loss: 0.1278 - val_loss: 0.1171\n",
      "Epoch 6/100\n",
      "32497/32497 [==============================] - 2s 48us/sample - loss: 0.1266 - val_loss: 0.1156\n",
      "Epoch 7/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1255 - val_loss: 0.1137\n",
      "Epoch 8/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1249 - val_loss: 0.1131\n",
      "Epoch 9/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1239 - val_loss: 0.1127\n",
      "Epoch 10/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1232 - val_loss: 0.1095\n",
      "Epoch 11/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1226 - val_loss: 0.1096\n",
      "Epoch 12/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1217 - val_loss: 0.1083\n",
      "Epoch 13/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1209 - val_loss: 0.1088\n",
      "Epoch 14/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1201 - val_loss: 0.1101\n",
      "Epoch 15/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1194 - val_loss: 0.1078\n",
      "Epoch 16/100\n",
      "32497/32497 [==============================] - 2s 50us/sample - loss: 0.1187 - val_loss: 0.1091\n",
      "Epoch 17/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1182 - val_loss: 0.1067\n",
      "Epoch 18/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1179 - val_loss: 0.1062\n",
      "Epoch 19/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1174 - val_loss: 0.1050\n",
      "Epoch 20/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1170 - val_loss: 0.1053\n",
      "Epoch 21/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1167 - val_loss: 0.1057\n",
      "Epoch 22/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1162 - val_loss: 0.1064\n",
      "Epoch 23/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1161 - val_loss: 0.1073\n",
      "Epoch 24/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1157 - val_loss: 0.1074\n",
      "Epoch 25/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1154 - val_loss: 0.1050\n",
      "Epoch 26/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1150 - val_loss: 0.1070\n",
      "Epoch 27/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1149 - val_loss: 0.1069\n",
      "Epoch 28/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1145 - val_loss: 0.1056\n",
      "Epoch 29/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1142 - val_loss: 0.1069\n",
      "Epoch 30/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1140 - val_loss: 0.1043\n",
      "Epoch 31/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1137 - val_loss: 0.1076\n",
      "Epoch 32/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1135 - val_loss: 0.1071\n",
      "Epoch 33/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1134 - val_loss: 0.1054\n",
      "Epoch 34/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1131 - val_loss: 0.1046\n",
      "Epoch 35/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1130 - val_loss: 0.1047\n",
      "Epoch 36/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1128 - val_loss: 0.1073\n",
      "Epoch 37/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1125 - val_loss: 0.1053\n",
      "Epoch 38/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1125 - val_loss: 0.1053\n",
      "Epoch 39/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1123 - val_loss: 0.1051\n",
      "Epoch 40/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1122 - val_loss: 0.1075\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 8\n",
      "Train on 32497 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "32497/32497 [==============================] - 5s 151us/sample - loss: 0.1614 - val_loss: 0.1334\n",
      "Epoch 2/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1337 - val_loss: 0.1190\n",
      "Epoch 3/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1286 - val_loss: 0.1139\n",
      "Epoch 4/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1259 - val_loss: 0.1120\n",
      "Epoch 5/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1243 - val_loss: 0.1111\n",
      "Epoch 6/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1228 - val_loss: 0.1113\n",
      "Epoch 7/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1217 - val_loss: 0.1082\n",
      "Epoch 8/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1206 - val_loss: 0.1093\n",
      "Epoch 9/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1196 - val_loss: 0.1073\n",
      "Epoch 10/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1187 - val_loss: 0.1070\n",
      "Epoch 11/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1180 - val_loss: 0.1067\n",
      "Epoch 12/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1173 - val_loss: 0.1063\n",
      "Epoch 13/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1168 - val_loss: 0.1091\n",
      "Epoch 14/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1163 - val_loss: 0.1083\n",
      "Epoch 15/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1157 - val_loss: 0.1092\n",
      "Epoch 16/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1153 - val_loss: 0.1117\n",
      "Epoch 17/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1150 - val_loss: 0.1062\n",
      "Epoch 18/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1147 - val_loss: 0.1077\n",
      "Epoch 19/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1143 - val_loss: 0.1054\n",
      "Epoch 20/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1140 - val_loss: 0.1070\n",
      "Epoch 21/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1137 - val_loss: 0.1063\n",
      "Epoch 22/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1133 - val_loss: 0.1064\n",
      "Epoch 23/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1132 - val_loss: 0.1042\n",
      "Epoch 24/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1130 - val_loss: 0.1071\n",
      "Epoch 25/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1127 - val_loss: 0.1042\n",
      "Epoch 26/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1124 - val_loss: 0.1047\n",
      "Epoch 27/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1122 - val_loss: 0.1049\n",
      "Epoch 28/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1119 - val_loss: 0.1082\n",
      "Epoch 29/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1118 - val_loss: 0.1050\n",
      "Epoch 30/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1114 - val_loss: 0.1039\n",
      "Epoch 31/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1114 - val_loss: 0.1038\n",
      "Epoch 32/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1112 - val_loss: 0.1041\n",
      "Epoch 33/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1110 - val_loss: 0.1060\n",
      "Epoch 34/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1107 - val_loss: 0.1039\n",
      "Epoch 35/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1106 - val_loss: 0.1038\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1104 - val_loss: 0.1040\n",
      "Epoch 37/100\n",
      "32497/32497 [==============================] - 2s 49us/sample - loss: 0.1102 - val_loss: 0.1030\n",
      "Epoch 38/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1101 - val_loss: 0.1030\n",
      "Epoch 39/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1100 - val_loss: 0.1037\n",
      "Epoch 40/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1098 - val_loss: 0.1041\n",
      "Epoch 41/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1098 - val_loss: 0.1018\n",
      "Epoch 42/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1095 - val_loss: 0.1034\n",
      "Epoch 43/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1094 - val_loss: 0.1016\n",
      "Epoch 44/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1094 - val_loss: 0.1016\n",
      "Epoch 45/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1092 - val_loss: 0.1036\n",
      "Epoch 46/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1090 - val_loss: 0.1018\n",
      "Epoch 47/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1088 - val_loss: 0.1002\n",
      "Epoch 48/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1087 - val_loss: 0.1003\n",
      "Epoch 49/100\n",
      "32497/32497 [==============================] - 2s 58us/sample - loss: 0.1085 - val_loss: 0.0997\n",
      "Epoch 50/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1085 - val_loss: 0.1009\n",
      "Epoch 51/100\n",
      "32497/32497 [==============================] - 2s 59us/sample - loss: 0.1083 - val_loss: 0.0995\n",
      "Epoch 52/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1082 - val_loss: 0.0993\n",
      "Epoch 53/100\n",
      "32497/32497 [==============================] - 2s 60us/sample - loss: 0.1081 - val_loss: 0.0991\n",
      "Epoch 54/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1083 - val_loss: 0.1007\n",
      "Epoch 55/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1079 - val_loss: 0.0992\n",
      "Epoch 56/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1077 - val_loss: 0.0996\n",
      "Epoch 57/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1076 - val_loss: 0.1002\n",
      "Epoch 58/100\n",
      "32497/32497 [==============================] - 2s 50us/sample - loss: 0.1076 - val_loss: 0.0992\n",
      "Epoch 59/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1074 - val_loss: 0.0992\n",
      "Epoch 60/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1073 - val_loss: 0.0984\n",
      "Epoch 61/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1073 - val_loss: 0.0990\n",
      "Epoch 62/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1071 - val_loss: 0.1002\n",
      "Epoch 63/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1070 - val_loss: 0.1000\n",
      "Epoch 64/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1068 - val_loss: 0.0979\n",
      "Epoch 65/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1069 - val_loss: 0.1008\n",
      "Epoch 66/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1067 - val_loss: 0.0978\n",
      "Epoch 67/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1067 - val_loss: 0.1011\n",
      "Epoch 68/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1065 - val_loss: 0.0975\n",
      "Epoch 69/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1064 - val_loss: 0.0980\n",
      "Epoch 70/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1063 - val_loss: 0.0993\n",
      "Epoch 71/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1065 - val_loss: 0.0980\n",
      "Epoch 72/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1063 - val_loss: 0.0981\n",
      "Epoch 73/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1059 - val_loss: 0.0991\n",
      "Epoch 74/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1061 - val_loss: 0.1011\n",
      "Epoch 75/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1060 - val_loss: 0.0979\n",
      "Epoch 76/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1059 - val_loss: 0.0995\n",
      "Epoch 77/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1058 - val_loss: 0.0988\n",
      "Epoch 78/100\n",
      "32497/32497 [==============================] - 2s 50us/sample - loss: 0.1058 - val_loss: 0.0973\n",
      "Epoch 79/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1057 - val_loss: 0.0971\n",
      "Epoch 80/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1055 - val_loss: 0.0999\n",
      "Epoch 81/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1056 - val_loss: 0.0972\n",
      "Epoch 82/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1055 - val_loss: 0.0965\n",
      "Epoch 83/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1052 - val_loss: 0.0988\n",
      "Epoch 84/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1052 - val_loss: 0.0989\n",
      "Epoch 85/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1051 - val_loss: 0.0978\n",
      "Epoch 86/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1051 - val_loss: 0.0983\n",
      "Epoch 87/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1051 - val_loss: 0.0970\n",
      "Epoch 88/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1049 - val_loss: 0.0985\n",
      "Epoch 89/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1049 - val_loss: 0.0979\n",
      "Epoch 90/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1048 - val_loss: 0.0979\n",
      "Epoch 91/100\n",
      "32497/32497 [==============================] - 2s 58us/sample - loss: 0.1050 - val_loss: 0.0970\n",
      "Epoch 92/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1048 - val_loss: 0.0979\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 16\n",
      "Train on 32497 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "32497/32497 [==============================] - 5s 154us/sample - loss: 0.1579 - val_loss: 0.1206\n",
      "Epoch 2/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1303 - val_loss: 0.1142\n",
      "Epoch 3/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1262 - val_loss: 0.1098\n",
      "Epoch 4/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1246 - val_loss: 0.1068\n",
      "Epoch 5/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1236 - val_loss: 0.1036\n",
      "Epoch 6/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1228 - val_loss: 0.1057\n",
      "Epoch 7/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1221 - val_loss: 0.1058\n",
      "Epoch 8/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1216 - val_loss: 0.1035\n",
      "Epoch 9/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1210 - val_loss: 0.1035\n",
      "Epoch 10/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1205 - val_loss: 0.1047\n",
      "Epoch 11/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1198 - val_loss: 0.1057\n",
      "Epoch 12/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1195 - val_loss: 0.1036\n",
      "Epoch 13/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1190 - val_loss: 0.1094\n",
      "Epoch 14/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1179 - val_loss: 0.1052\n",
      "Epoch 15/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1171 - val_loss: 0.1077\n",
      "Epoch 16/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1163 - val_loss: 0.1104\n",
      "Epoch 17/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1157 - val_loss: 0.1067\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1151 - val_loss: 0.1075\n",
      "Epoch 19/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1143 - val_loss: 0.1106\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 32\n",
      "Train on 32497 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "32497/32497 [==============================] - 5s 153us/sample - loss: 0.1462 - val_loss: 0.1143\n",
      "Epoch 2/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1275 - val_loss: 0.1067\n",
      "Epoch 3/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1249 - val_loss: 0.1075\n",
      "Epoch 4/100\n",
      "32497/32497 [==============================] - 2s 59us/sample - loss: 0.1239 - val_loss: 0.1089\n",
      "Epoch 5/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1227 - val_loss: 0.1019\n",
      "Epoch 6/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1221 - val_loss: 0.1085\n",
      "Epoch 7/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1216 - val_loss: 0.1106\n",
      "Epoch 8/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1206 - val_loss: 0.1036\n",
      "Epoch 9/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1200 - val_loss: 0.1049\n",
      "Epoch 10/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1200 - val_loss: 0.1076\n",
      "Epoch 11/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1191 - val_loss: 0.1059\n",
      "Epoch 12/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1187 - val_loss: 0.1015\n",
      "Epoch 13/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1182 - val_loss: 0.1039\n",
      "Epoch 14/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1174 - val_loss: 0.1072\n",
      "Epoch 15/100\n",
      "32497/32497 [==============================] - 2s 49us/sample - loss: 0.1170 - val_loss: 0.1089\n",
      "Epoch 16/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1164 - val_loss: 0.1057\n",
      "Epoch 17/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1158 - val_loss: 0.1051\n",
      "Epoch 18/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1153 - val_loss: 0.1097\n",
      "Epoch 19/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1147 - val_loss: 0.1038\n",
      "Epoch 20/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1143 - val_loss: 0.1060\n",
      "Epoch 21/100\n",
      "32497/32497 [==============================] - 2s 49us/sample - loss: 0.1138 - val_loss: 0.1062\n",
      "Epoch 22/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1128 - val_loss: 0.1049\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 64\n",
      "Train on 32497 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "32497/32497 [==============================] - 5s 150us/sample - loss: 0.1434 - val_loss: 0.1125\n",
      "Epoch 2/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1273 - val_loss: 0.1076\n",
      "Epoch 3/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1249 - val_loss: 0.1080\n",
      "Epoch 4/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1231 - val_loss: 0.1077\n",
      "Epoch 5/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1221 - val_loss: 0.1017\n",
      "Epoch 6/100\n",
      "32497/32497 [==============================] - 2s 56us/sample - loss: 0.1221 - val_loss: 0.1138\n",
      "Epoch 7/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1213 - val_loss: 0.1111\n",
      "Epoch 8/100\n",
      "32497/32497 [==============================] - 2s 55us/sample - loss: 0.1205 - val_loss: 0.1088\n",
      "Epoch 9/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1202 - val_loss: 0.1124\n",
      "Epoch 10/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1194 - val_loss: 0.1078\n",
      "Epoch 11/100\n",
      "32497/32497 [==============================] - 2s 54us/sample - loss: 0.1184 - val_loss: 0.1058\n",
      "Epoch 12/100\n",
      "32497/32497 [==============================] - 2s 57us/sample - loss: 0.1175 - val_loss: 0.1055\n",
      "Epoch 13/100\n",
      "32497/32497 [==============================] - 2s 50us/sample - loss: 0.1167 - val_loss: 0.1093\n",
      "Epoch 14/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1156 - val_loss: 0.1069\n",
      "Epoch 15/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1147 - val_loss: 0.1166\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 128\n",
      "Train on 32497 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "32497/32497 [==============================] - 4s 137us/sample - loss: 0.1412 - val_loss: 0.1079\n",
      "Epoch 2/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1284 - val_loss: 0.1074\n",
      "Epoch 3/100\n",
      "32497/32497 [==============================] - 2s 49us/sample - loss: 0.1260 - val_loss: 0.1108\n",
      "Epoch 4/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1244 - val_loss: 0.1118\n",
      "Epoch 5/100\n",
      "32497/32497 [==============================] - 2s 53us/sample - loss: 0.1230 - val_loss: 0.1015\n",
      "Epoch 6/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1227 - val_loss: 0.1106\n",
      "Epoch 7/100\n",
      "32497/32497 [==============================] - 2s 50us/sample - loss: 0.1221 - val_loss: 0.1125\n",
      "Epoch 8/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1210 - val_loss: 0.1109\n",
      "Epoch 9/100\n",
      "32497/32497 [==============================] - 2s 50us/sample - loss: 0.1202 - val_loss: 0.1088\n",
      "Epoch 10/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1196 - val_loss: 0.1105\n",
      "Epoch 11/100\n",
      "32497/32497 [==============================] - 2s 50us/sample - loss: 0.1187 - val_loss: 0.1069\n",
      "Epoch 12/100\n",
      "32497/32497 [==============================] - 2s 52us/sample - loss: 0.1177 - val_loss: 0.1099\n",
      "Epoch 13/100\n",
      "32497/32497 [==============================] - 2s 48us/sample - loss: 0.1164 - val_loss: 0.1114\n",
      "Epoch 14/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1155 - val_loss: 0.1062\n",
      "Epoch 15/100\n",
      "32497/32497 [==============================] - 2s 51us/sample - loss: 0.1149 - val_loss: 0.1232\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 2\n",
      "Train on 33340 samples, validate on 608 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 138us/sample - loss: 0.1864 - val_loss: 0.1497\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1568 - val_loss: 0.1386\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1429 - val_loss: 0.1387\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1377 - val_loss: 0.1382\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1351 - val_loss: 0.1384\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1334 - val_loss: 0.1388\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1324 - val_loss: 0.1388\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1316 - val_loss: 0.1371\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1310 - val_loss: 0.1365\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1305 - val_loss: 0.1365\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1300 - val_loss: 0.1361\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1296 - val_loss: 0.1348\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1293 - val_loss: 0.1343\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1290 - val_loss: 0.1344\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1287 - val_loss: 0.1330\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1285 - val_loss: 0.1321\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1282 - val_loss: 0.1316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 49us/sample - loss: 0.1280 - val_loss: 0.1307\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1277 - val_loss: 0.1310\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1275 - val_loss: 0.1306\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1273 - val_loss: 0.1301\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1270 - val_loss: 0.1283\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1268 - val_loss: 0.1280\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1266 - val_loss: 0.1275\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1264 - val_loss: 0.1277\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1262 - val_loss: 0.1263\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1259 - val_loss: 0.1252\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1257 - val_loss: 0.1252\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1255 - val_loss: 0.1248\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1252 - val_loss: 0.1232\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1250 - val_loss: 0.1229\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1248 - val_loss: 0.1221\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1246 - val_loss: 0.1216\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1244 - val_loss: 0.1214\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1242 - val_loss: 0.1205\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1240 - val_loss: 0.1200\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1238 - val_loss: 0.1198\n",
      "Epoch 38/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1236 - val_loss: 0.1196\n",
      "Epoch 39/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1234 - val_loss: 0.1184\n",
      "Epoch 40/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1233 - val_loss: 0.1179\n",
      "Epoch 41/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1230 - val_loss: 0.1178\n",
      "Epoch 42/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1228 - val_loss: 0.1167\n",
      "Epoch 43/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1226 - val_loss: 0.1164\n",
      "Epoch 44/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1223 - val_loss: 0.1166\n",
      "Epoch 45/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1222 - val_loss: 0.1163\n",
      "Epoch 46/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1220 - val_loss: 0.1162\n",
      "Epoch 47/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1218 - val_loss: 0.1153\n",
      "Epoch 48/100\n",
      "33340/33340 [==============================] - 2s 49us/sample - loss: 0.1216 - val_loss: 0.1147\n",
      "Epoch 49/100\n",
      "33340/33340 [==============================] - 2s 49us/sample - loss: 0.1213 - val_loss: 0.1145\n",
      "Epoch 50/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1211 - val_loss: 0.1144\n",
      "Epoch 51/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1209 - val_loss: 0.1140\n",
      "Epoch 52/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1207 - val_loss: 0.1135\n",
      "Epoch 53/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1205 - val_loss: 0.1134\n",
      "Epoch 54/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1203 - val_loss: 0.1128\n",
      "Epoch 55/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1200 - val_loss: 0.1123\n",
      "Epoch 56/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1199 - val_loss: 0.1120\n",
      "Epoch 57/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1196 - val_loss: 0.1120\n",
      "Epoch 58/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1194 - val_loss: 0.1114\n",
      "Epoch 59/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1191 - val_loss: 0.1114\n",
      "Epoch 60/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1190 - val_loss: 0.1114\n",
      "Epoch 61/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1187 - val_loss: 0.1108\n",
      "Epoch 62/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1185 - val_loss: 0.1103\n",
      "Epoch 63/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1184 - val_loss: 0.1101\n",
      "Epoch 64/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1181 - val_loss: 0.1101\n",
      "Epoch 65/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1179 - val_loss: 0.1098\n",
      "Epoch 66/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1178 - val_loss: 0.1091\n",
      "Epoch 67/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1176 - val_loss: 0.1089\n",
      "Epoch 68/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1174 - val_loss: 0.1089\n",
      "Epoch 69/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1172 - val_loss: 0.1085\n",
      "Epoch 70/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1171 - val_loss: 0.1087\n",
      "Epoch 71/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1169 - val_loss: 0.1082\n",
      "Epoch 72/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1168 - val_loss: 0.1079\n",
      "Epoch 73/100\n",
      "33340/33340 [==============================] - 2s 48us/sample - loss: 0.1166 - val_loss: 0.1078\n",
      "Epoch 74/100\n",
      "33340/33340 [==============================] - 2s 46us/sample - loss: 0.1165 - val_loss: 0.1080\n",
      "Epoch 75/100\n",
      "33340/33340 [==============================] - 2s 46us/sample - loss: 0.1163 - val_loss: 0.1077\n",
      "Epoch 76/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1162 - val_loss: 0.1074\n",
      "Epoch 77/100\n",
      "33340/33340 [==============================] - 2s 45us/sample - loss: 0.1160 - val_loss: 0.1075\n",
      "Epoch 78/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1159 - val_loss: 0.1075\n",
      "Epoch 79/100\n",
      "33340/33340 [==============================] - 2s 47us/sample - loss: 0.1158 - val_loss: 0.1075\n",
      "Epoch 80/100\n",
      "33340/33340 [==============================] - 2s 47us/sample - loss: 0.1156 - val_loss: 0.1072\n",
      "Epoch 81/100\n",
      "33340/33340 [==============================] - 2s 47us/sample - loss: 0.1155 - val_loss: 0.1072\n",
      "Epoch 82/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1154 - val_loss: 0.1071\n",
      "Epoch 83/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1153 - val_loss: 0.1068\n",
      "Epoch 84/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1152 - val_loss: 0.1066\n",
      "Epoch 85/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1151 - val_loss: 0.1067\n",
      "Epoch 86/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1149 - val_loss: 0.1069\n",
      "Epoch 87/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1149 - val_loss: 0.1067\n",
      "Epoch 88/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1148 - val_loss: 0.1065\n",
      "Epoch 89/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1147 - val_loss: 0.1067\n",
      "Epoch 90/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1146 - val_loss: 0.1067\n",
      "Epoch 91/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1145 - val_loss: 0.1065\n",
      "Epoch 92/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1144 - val_loss: 0.1063\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1143 - val_loss: 0.1062\n",
      "Epoch 94/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1142 - val_loss: 0.1064\n",
      "Epoch 95/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1141 - val_loss: 0.1063\n",
      "Epoch 96/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1140 - val_loss: 0.1064\n",
      "Epoch 97/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.1140 - val_loss: 0.1062\n",
      "Epoch 98/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1139 - val_loss: 0.1064\n",
      "Epoch 99/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1139 - val_loss: 0.1061\n",
      "Epoch 100/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1138 - val_loss: 0.1061\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 4\n",
      "Train on 33340 samples, validate on 608 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 155us/sample - loss: 0.1789 - val_loss: 0.1549\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1412 - val_loss: 0.1299\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1332 - val_loss: 0.1275\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1311 - val_loss: 0.1250\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1299 - val_loss: 0.1230\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1288 - val_loss: 0.1219\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1279 - val_loss: 0.1206\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1272 - val_loss: 0.1193\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1268 - val_loss: 0.1190\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1262 - val_loss: 0.1191\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1258 - val_loss: 0.1183\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.1255 - val_loss: 0.1180\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1251 - val_loss: 0.1180\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1248 - val_loss: 0.1179\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1246 - val_loss: 0.1181\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1244 - val_loss: 0.1177\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1242 - val_loss: 0.1184\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1240 - val_loss: 0.1190\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1238 - val_loss: 0.1193\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1237 - val_loss: 0.1188\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1234 - val_loss: 0.1199\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1233 - val_loss: 0.1196\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1232 - val_loss: 0.1199\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1231 - val_loss: 0.1219\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1230 - val_loss: 0.1192\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1228 - val_loss: 0.1201\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 8\n",
      "Train on 33340 samples, validate on 608 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 144us/sample - loss: 0.1656 - val_loss: 0.1335\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1347 - val_loss: 0.1298\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1313 - val_loss: 0.1277\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1297 - val_loss: 0.1247\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1286 - val_loss: 0.1225\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1276 - val_loss: 0.1203\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1268 - val_loss: 0.1178\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.1260 - val_loss: 0.1147\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.1255 - val_loss: 0.1137\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1249 - val_loss: 0.1135\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1243 - val_loss: 0.1129\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1238 - val_loss: 0.1130\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1234 - val_loss: 0.1134\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1230 - val_loss: 0.1137\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 59us/sample - loss: 0.1228 - val_loss: 0.1150\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1224 - val_loss: 0.1143\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.1222 - val_loss: 0.1156\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1220 - val_loss: 0.1159\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.1215 - val_loss: 0.1166\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.1213 - val_loss: 0.1164\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1209 - val_loss: 0.1176\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 16\n",
      "Train on 33340 samples, validate on 608 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 4s 133us/sample - loss: 0.1570 - val_loss: 0.1327\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1340 - val_loss: 0.1266\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 49us/sample - loss: 0.1303 - val_loss: 0.1223\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1287 - val_loss: 0.1215\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1275 - val_loss: 0.1168\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1271 - val_loss: 0.1156\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1262 - val_loss: 0.1147\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1256 - val_loss: 0.1128\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1256 - val_loss: 0.1133\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1249 - val_loss: 0.1131\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1244 - val_loss: 0.1135\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 58us/sample - loss: 0.1244 - val_loss: 0.1134\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1240 - val_loss: 0.1141\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.1237 - val_loss: 0.1133\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 49us/sample - loss: 0.1235 - val_loss: 0.1160\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1232 - val_loss: 0.1135\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1231 - val_loss: 0.1148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1225 - val_loss: 0.1159\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 32\n",
      "Train on 33340 samples, validate on 608 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 4s 131us/sample - loss: 0.1529 - val_loss: 0.1300\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1330 - val_loss: 0.1258\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1303 - val_loss: 0.1215\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1287 - val_loss: 0.1189\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1278 - val_loss: 0.1172\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1271 - val_loss: 0.1161\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 58us/sample - loss: 0.1263 - val_loss: 0.1153\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1256 - val_loss: 0.1140\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1256 - val_loss: 0.1143\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 49us/sample - loss: 0.1246 - val_loss: 0.1149\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1242 - val_loss: 0.1151\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1239 - val_loss: 0.1141\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1234 - val_loss: 0.1157\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 49us/sample - loss: 0.1229 - val_loss: 0.1147\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1226 - val_loss: 0.1168\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1222 - val_loss: 0.1141\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1222 - val_loss: 0.1144\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1216 - val_loss: 0.1158\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 64\n",
      "Train on 33340 samples, validate on 608 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 146us/sample - loss: 0.1502 - val_loss: 0.1297\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1326 - val_loss: 0.1256\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1303 - val_loss: 0.1195\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 48us/sample - loss: 0.1288 - val_loss: 0.1176\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 49us/sample - loss: 0.1281 - val_loss: 0.1154\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1269 - val_loss: 0.1143\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1267 - val_loss: 0.1139\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1262 - val_loss: 0.1135\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1258 - val_loss: 0.1143\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1253 - val_loss: 0.1139\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1249 - val_loss: 0.1163\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1245 - val_loss: 0.1134\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1239 - val_loss: 0.1175\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1234 - val_loss: 0.1151\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1229 - val_loss: 0.1192\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1224 - val_loss: 0.1167\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1221 - val_loss: 0.1158\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1211 - val_loss: 0.1205\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1208 - val_loss: 0.1168\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1206 - val_loss: 0.1175\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1194 - val_loss: 0.1195\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1188 - val_loss: 0.1186\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 128\n",
      "Train on 33340 samples, validate on 608 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 143us/sample - loss: 0.1483 - val_loss: 0.1302\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1325 - val_loss: 0.1262\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1310 - val_loss: 0.1203\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1294 - val_loss: 0.1190\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.1286 - val_loss: 0.1158\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.1273 - val_loss: 0.1149\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1270 - val_loss: 0.1137\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1263 - val_loss: 0.1133\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1256 - val_loss: 0.1140\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1250 - val_loss: 0.1140\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1243 - val_loss: 0.1172\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.1240 - val_loss: 0.1149\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1232 - val_loss: 0.1186\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.1227 - val_loss: 0.1161\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.1221 - val_loss: 0.1195\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1212 - val_loss: 0.1180\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.1210 - val_loss: 0.1155\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.1197 - val_loss: 0.1187\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 2\n",
      "Train on 33086 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "33086/33086 [==============================] - 5s 142us/sample - loss: 0.1884 - val_loss: 0.1871\n",
      "Epoch 2/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1522 - val_loss: 0.1710\n",
      "Epoch 3/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1417 - val_loss: 0.1578\n",
      "Epoch 4/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1355 - val_loss: 0.1489\n",
      "Epoch 5/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1320 - val_loss: 0.1416\n",
      "Epoch 6/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1296 - val_loss: 0.1394\n",
      "Epoch 7/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1280 - val_loss: 0.1368\n",
      "Epoch 8/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1269 - val_loss: 0.1351\n",
      "Epoch 9/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1260 - val_loss: 0.1328\n",
      "Epoch 10/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1254 - val_loss: 0.1313\n",
      "Epoch 11/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1249 - val_loss: 0.1306\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1246 - val_loss: 0.1291\n",
      "Epoch 13/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1242 - val_loss: 0.1275\n",
      "Epoch 14/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1239 - val_loss: 0.1268\n",
      "Epoch 15/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1236 - val_loss: 0.1268\n",
      "Epoch 16/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1233 - val_loss: 0.1253\n",
      "Epoch 17/100\n",
      "33086/33086 [==============================] - 2s 48us/sample - loss: 0.1230 - val_loss: 0.1279\n",
      "Epoch 18/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1228 - val_loss: 0.1244\n",
      "Epoch 19/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1226 - val_loss: 0.1240\n",
      "Epoch 20/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1223 - val_loss: 0.1226\n",
      "Epoch 21/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1222 - val_loss: 0.1256\n",
      "Epoch 22/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1219 - val_loss: 0.1245\n",
      "Epoch 23/100\n",
      "33086/33086 [==============================] - 2s 49us/sample - loss: 0.1217 - val_loss: 0.1229\n",
      "Epoch 24/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1215 - val_loss: 0.1220\n",
      "Epoch 25/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1212 - val_loss: 0.1207\n",
      "Epoch 26/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1210 - val_loss: 0.1230\n",
      "Epoch 27/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1208 - val_loss: 0.1211\n",
      "Epoch 28/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1205 - val_loss: 0.1218\n",
      "Epoch 29/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1203 - val_loss: 0.1212\n",
      "Epoch 30/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1201 - val_loss: 0.1196\n",
      "Epoch 31/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1198 - val_loss: 0.1214\n",
      "Epoch 32/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1196 - val_loss: 0.1199\n",
      "Epoch 33/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1193 - val_loss: 0.1194\n",
      "Epoch 34/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1190 - val_loss: 0.1207\n",
      "Epoch 35/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1187 - val_loss: 0.1209\n",
      "Epoch 36/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1185 - val_loss: 0.1202\n",
      "Epoch 37/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1182 - val_loss: 0.1228\n",
      "Epoch 38/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1179 - val_loss: 0.1198\n",
      "Epoch 39/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1177 - val_loss: 0.1190\n",
      "Epoch 40/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1174 - val_loss: 0.1196\n",
      "Epoch 41/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1172 - val_loss: 0.1185\n",
      "Epoch 42/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1168 - val_loss: 0.1183\n",
      "Epoch 43/100\n",
      "33086/33086 [==============================] - 2s 49us/sample - loss: 0.1166 - val_loss: 0.1205\n",
      "Epoch 44/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1163 - val_loss: 0.1184\n",
      "Epoch 45/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1160 - val_loss: 0.1180\n",
      "Epoch 46/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1157 - val_loss: 0.1179\n",
      "Epoch 47/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1154 - val_loss: 0.1179\n",
      "Epoch 48/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1151 - val_loss: 0.1177\n",
      "Epoch 49/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1148 - val_loss: 0.1188\n",
      "Epoch 50/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1145 - val_loss: 0.1161\n",
      "Epoch 51/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1142 - val_loss: 0.1177\n",
      "Epoch 52/100\n",
      "33086/33086 [==============================] - 2s 57us/sample - loss: 0.1139 - val_loss: 0.1170\n",
      "Epoch 53/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1137 - val_loss: 0.1170\n",
      "Epoch 54/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1133 - val_loss: 0.1163\n",
      "Epoch 55/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1130 - val_loss: 0.1173\n",
      "Epoch 56/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1128 - val_loss: 0.1183\n",
      "Epoch 57/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1126 - val_loss: 0.1156\n",
      "Epoch 58/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1124 - val_loss: 0.1164\n",
      "Epoch 59/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1120 - val_loss: 0.1152\n",
      "Epoch 60/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1118 - val_loss: 0.1162\n",
      "Epoch 61/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1117 - val_loss: 0.1166\n",
      "Epoch 62/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1114 - val_loss: 0.1155\n",
      "Epoch 63/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1113 - val_loss: 0.1159\n",
      "Epoch 64/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1111 - val_loss: 0.1158\n",
      "Epoch 65/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1110 - val_loss: 0.1141\n",
      "Epoch 66/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1108 - val_loss: 0.1141\n",
      "Epoch 67/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1107 - val_loss: 0.1147\n",
      "Epoch 68/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1106 - val_loss: 0.1141\n",
      "Epoch 69/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1105 - val_loss: 0.1144\n",
      "Epoch 70/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1104 - val_loss: 0.1134\n",
      "Epoch 71/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1103 - val_loss: 0.1139\n",
      "Epoch 72/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1101 - val_loss: 0.1145\n",
      "Epoch 73/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1102 - val_loss: 0.1132\n",
      "Epoch 74/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1100 - val_loss: 0.1131\n",
      "Epoch 75/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1099 - val_loss: 0.1150\n",
      "Epoch 76/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1099 - val_loss: 0.1144\n",
      "Epoch 77/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1098 - val_loss: 0.1147\n",
      "Epoch 78/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1097 - val_loss: 0.1125\n",
      "Epoch 79/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1096 - val_loss: 0.1149\n",
      "Epoch 80/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1096 - val_loss: 0.1124\n",
      "Epoch 81/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1095 - val_loss: 0.1125\n",
      "Epoch 82/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1095 - val_loss: 0.1140\n",
      "Epoch 83/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1094 - val_loss: 0.1153\n",
      "Epoch 84/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1093 - val_loss: 0.1161\n",
      "Epoch 85/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1093 - val_loss: 0.1133\n",
      "Epoch 86/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1093 - val_loss: 0.1132\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1092 - val_loss: 0.1132\n",
      "Epoch 88/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1092 - val_loss: 0.1128\n",
      "Epoch 89/100\n",
      "33086/33086 [==============================] - 2s 60us/sample - loss: 0.1091 - val_loss: 0.1135\n",
      "Epoch 90/100\n",
      "33086/33086 [==============================] - 2s 59us/sample - loss: 0.1091 - val_loss: 0.1127\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 4\n",
      "Train on 33086 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "33086/33086 [==============================] - 5s 151us/sample - loss: 0.1789 - val_loss: 0.1826\n",
      "Epoch 2/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1414 - val_loss: 0.1589\n",
      "Epoch 3/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1332 - val_loss: 0.1503\n",
      "Epoch 4/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1296 - val_loss: 0.1440\n",
      "Epoch 5/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1275 - val_loss: 0.1395\n",
      "Epoch 6/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1261 - val_loss: 0.1378\n",
      "Epoch 7/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1251 - val_loss: 0.1366\n",
      "Epoch 8/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1243 - val_loss: 0.1357\n",
      "Epoch 9/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1236 - val_loss: 0.1341\n",
      "Epoch 10/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1231 - val_loss: 0.1331\n",
      "Epoch 11/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1229 - val_loss: 0.1330\n",
      "Epoch 12/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1224 - val_loss: 0.1315\n",
      "Epoch 13/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1221 - val_loss: 0.1309\n",
      "Epoch 14/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1217 - val_loss: 0.1315\n",
      "Epoch 15/100\n",
      "33086/33086 [==============================] - 2s 48us/sample - loss: 0.1215 - val_loss: 0.1307\n",
      "Epoch 16/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1211 - val_loss: 0.1287\n",
      "Epoch 17/100\n",
      "33086/33086 [==============================] - 2s 58us/sample - loss: 0.1210 - val_loss: 0.1296\n",
      "Epoch 18/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1206 - val_loss: 0.1291\n",
      "Epoch 19/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1204 - val_loss: 0.1278\n",
      "Epoch 20/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1201 - val_loss: 0.1273\n",
      "Epoch 21/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1200 - val_loss: 0.1295\n",
      "Epoch 22/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1197 - val_loss: 0.1266\n",
      "Epoch 23/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1197 - val_loss: 0.1270\n",
      "Epoch 24/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1193 - val_loss: 0.1269\n",
      "Epoch 25/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1192 - val_loss: 0.1262\n",
      "Epoch 26/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1190 - val_loss: 0.1270\n",
      "Epoch 27/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1186 - val_loss: 0.1255\n",
      "Epoch 28/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1184 - val_loss: 0.1262\n",
      "Epoch 29/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1183 - val_loss: 0.1252\n",
      "Epoch 30/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1180 - val_loss: 0.1245\n",
      "Epoch 31/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1179 - val_loss: 0.1257\n",
      "Epoch 32/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1177 - val_loss: 0.1252\n",
      "Epoch 33/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1173 - val_loss: 0.1252\n",
      "Epoch 34/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1172 - val_loss: 0.1247\n",
      "Epoch 35/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1171 - val_loss: 0.1261\n",
      "Epoch 36/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1166 - val_loss: 0.1248\n",
      "Epoch 37/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1164 - val_loss: 0.1255\n",
      "Epoch 38/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1161 - val_loss: 0.1264\n",
      "Epoch 39/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1159 - val_loss: 0.1261\n",
      "Epoch 40/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1157 - val_loss: 0.1266\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 8\n",
      "Train on 33086 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "33086/33086 [==============================] - 5s 145us/sample - loss: 0.1600 - val_loss: 0.1625\n",
      "Epoch 2/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1309 - val_loss: 0.1443\n",
      "Epoch 3/100\n",
      "33086/33086 [==============================] - 2s 57us/sample - loss: 0.1269 - val_loss: 0.1397\n",
      "Epoch 4/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1245 - val_loss: 0.1346\n",
      "Epoch 5/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1227 - val_loss: 0.1335\n",
      "Epoch 6/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1216 - val_loss: 0.1337\n",
      "Epoch 7/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1204 - val_loss: 0.1324\n",
      "Epoch 8/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1197 - val_loss: 0.1324\n",
      "Epoch 9/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1190 - val_loss: 0.1322\n",
      "Epoch 10/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1183 - val_loss: 0.1314\n",
      "Epoch 11/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1178 - val_loss: 0.1323\n",
      "Epoch 12/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1175 - val_loss: 0.1317\n",
      "Epoch 13/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1170 - val_loss: 0.1316\n",
      "Epoch 14/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1165 - val_loss: 0.1337\n",
      "Epoch 15/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1162 - val_loss: 0.1326\n",
      "Epoch 16/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1157 - val_loss: 0.1327\n",
      "Epoch 17/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1154 - val_loss: 0.1341\n",
      "Epoch 18/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1150 - val_loss: 0.1326\n",
      "Epoch 19/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1147 - val_loss: 0.1322\n",
      "Epoch 20/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1143 - val_loss: 0.1331\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 16\n",
      "Train on 33086 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "33086/33086 [==============================] - 5s 136us/sample - loss: 0.1538 - val_loss: 0.1515\n",
      "Epoch 2/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1286 - val_loss: 0.1378\n",
      "Epoch 3/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1255 - val_loss: 0.1335\n",
      "Epoch 4/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1240 - val_loss: 0.1303\n",
      "Epoch 5/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1230 - val_loss: 0.1298\n",
      "Epoch 6/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1221 - val_loss: 0.1290\n",
      "Epoch 7/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1211 - val_loss: 0.1287\n",
      "Epoch 8/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1203 - val_loss: 0.1287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1199 - val_loss: 0.1293\n",
      "Epoch 10/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1194 - val_loss: 0.1270\n",
      "Epoch 11/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1188 - val_loss: 0.1269\n",
      "Epoch 12/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1180 - val_loss: 0.1253\n",
      "Epoch 13/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1178 - val_loss: 0.1270\n",
      "Epoch 14/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1173 - val_loss: 0.1275\n",
      "Epoch 15/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1170 - val_loss: 0.1257\n",
      "Epoch 16/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1162 - val_loss: 0.1244\n",
      "Epoch 17/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1160 - val_loss: 0.1239\n",
      "Epoch 18/100\n",
      "33086/33086 [==============================] - 2s 47us/sample - loss: 0.1153 - val_loss: 0.1243\n",
      "Epoch 19/100\n",
      "33086/33086 [==============================] - 1s 44us/sample - loss: 0.1150 - val_loss: 0.1243\n",
      "Epoch 20/100\n",
      "33086/33086 [==============================] - 1s 43us/sample - loss: 0.1146 - val_loss: 0.1236\n",
      "Epoch 21/100\n",
      "33086/33086 [==============================] - 2s 47us/sample - loss: 0.1141 - val_loss: 0.1246\n",
      "Epoch 22/100\n",
      "33086/33086 [==============================] - 2s 45us/sample - loss: 0.1136 - val_loss: 0.1222\n",
      "Epoch 23/100\n",
      "33086/33086 [==============================] - 2s 46us/sample - loss: 0.1131 - val_loss: 0.1217\n",
      "Epoch 24/100\n",
      "33086/33086 [==============================] - 1s 45us/sample - loss: 0.1123 - val_loss: 0.1222\n",
      "Epoch 25/100\n",
      "33086/33086 [==============================] - 2s 47us/sample - loss: 0.1116 - val_loss: 0.1222\n",
      "Epoch 26/100\n",
      "33086/33086 [==============================] - 2s 49us/sample - loss: 0.1110 - val_loss: 0.1235\n",
      "Epoch 27/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1107 - val_loss: 0.1212\n",
      "Epoch 28/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1100 - val_loss: 0.1203\n",
      "Epoch 29/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1096 - val_loss: 0.1197\n",
      "Epoch 30/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1092 - val_loss: 0.1191\n",
      "Epoch 31/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1087 - val_loss: 0.1187\n",
      "Epoch 32/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1083 - val_loss: 0.1167\n",
      "Epoch 33/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1079 - val_loss: 0.1179\n",
      "Epoch 34/100\n",
      "33086/33086 [==============================] - 2s 59us/sample - loss: 0.1076 - val_loss: 0.1166\n",
      "Epoch 35/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1073 - val_loss: 0.1168\n",
      "Epoch 36/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1069 - val_loss: 0.1150\n",
      "Epoch 37/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1069 - val_loss: 0.1149\n",
      "Epoch 38/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1064 - val_loss: 0.1150\n",
      "Epoch 39/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1062 - val_loss: 0.1145\n",
      "Epoch 40/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1060 - val_loss: 0.1138\n",
      "Epoch 41/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1059 - val_loss: 0.1140\n",
      "Epoch 42/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1057 - val_loss: 0.1158\n",
      "Epoch 43/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1055 - val_loss: 0.1135\n",
      "Epoch 44/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1052 - val_loss: 0.1123\n",
      "Epoch 45/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1052 - val_loss: 0.1126\n",
      "Epoch 46/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1052 - val_loss: 0.1179\n",
      "Epoch 47/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1049 - val_loss: 0.1138\n",
      "Epoch 48/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1048 - val_loss: 0.1130\n",
      "Epoch 49/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1047 - val_loss: 0.1131\n",
      "Epoch 50/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1046 - val_loss: 0.1131\n",
      "Epoch 51/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1043 - val_loss: 0.1131\n",
      "Epoch 52/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1043 - val_loss: 0.1120\n",
      "Epoch 53/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1043 - val_loss: 0.1111\n",
      "Epoch 54/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1040 - val_loss: 0.1114\n",
      "Epoch 55/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1039 - val_loss: 0.1127\n",
      "Epoch 56/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1039 - val_loss: 0.1116\n",
      "Epoch 57/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1037 - val_loss: 0.1109\n",
      "Epoch 58/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1035 - val_loss: 0.1115\n",
      "Epoch 59/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1036 - val_loss: 0.1124\n",
      "Epoch 60/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1034 - val_loss: 0.1126\n",
      "Epoch 61/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1032 - val_loss: 0.1109\n",
      "Epoch 62/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1031 - val_loss: 0.1114\n",
      "Epoch 63/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1031 - val_loss: 0.1109\n",
      "Epoch 64/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1030 - val_loss: 0.1154\n",
      "Epoch 65/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1028 - val_loss: 0.1111\n",
      "Epoch 66/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1027 - val_loss: 0.1127\n",
      "Epoch 67/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1026 - val_loss: 0.1114\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 32\n",
      "Train on 33086 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "33086/33086 [==============================] - 5s 150us/sample - loss: 0.1467 - val_loss: 0.1416\n",
      "Epoch 2/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1268 - val_loss: 0.1319\n",
      "Epoch 3/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1239 - val_loss: 0.1301\n",
      "Epoch 4/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1231 - val_loss: 0.1264\n",
      "Epoch 5/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1224 - val_loss: 0.1272\n",
      "Epoch 6/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1217 - val_loss: 0.1268\n",
      "Epoch 7/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1202 - val_loss: 0.1268\n",
      "Epoch 8/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1195 - val_loss: 0.1293\n",
      "Epoch 9/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1187 - val_loss: 0.1290\n",
      "Epoch 10/100\n",
      "33086/33086 [==============================] - 2s 48us/sample - loss: 0.1174 - val_loss: 0.1290\n",
      "Epoch 11/100\n",
      "33086/33086 [==============================] - 2s 49us/sample - loss: 0.1168 - val_loss: 0.1283\n",
      "Epoch 12/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1159 - val_loss: 0.1288\n",
      "Epoch 13/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1155 - val_loss: 0.1307\n",
      "Epoch 14/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1149 - val_loss: 0.1322\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 64\n",
      "Train on 33086 samples, validate on 589 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33086/33086 [==============================] - 5s 138us/sample - loss: 0.1436 - val_loss: 0.1396\n",
      "Epoch 2/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1270 - val_loss: 0.1298\n",
      "Epoch 3/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1244 - val_loss: 0.1325\n",
      "Epoch 4/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1240 - val_loss: 0.1242\n",
      "Epoch 5/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1239 - val_loss: 0.1264\n",
      "Epoch 6/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1228 - val_loss: 0.1245\n",
      "Epoch 7/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1216 - val_loss: 0.1278\n",
      "Epoch 8/100\n",
      "33086/33086 [==============================] - 2s 49us/sample - loss: 0.1213 - val_loss: 0.1270\n",
      "Epoch 9/100\n",
      "33086/33086 [==============================] - 2s 50us/sample - loss: 0.1211 - val_loss: 0.1271\n",
      "Epoch 10/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1204 - val_loss: 0.1243\n",
      "Epoch 11/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1197 - val_loss: 0.1343\n",
      "Epoch 12/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1188 - val_loss: 0.1254\n",
      "Epoch 13/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1189 - val_loss: 0.1334\n",
      "Epoch 14/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1175 - val_loss: 0.1284\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 128\n",
      "Train on 33086 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "33086/33086 [==============================] - 4s 129us/sample - loss: 0.1419 - val_loss: 0.1367\n",
      "Epoch 2/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1270 - val_loss: 0.1273\n",
      "Epoch 3/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1245 - val_loss: 0.1354\n",
      "Epoch 4/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1243 - val_loss: 0.1245\n",
      "Epoch 5/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1237 - val_loss: 0.1256\n",
      "Epoch 6/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1223 - val_loss: 0.1261\n",
      "Epoch 7/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1208 - val_loss: 0.1278\n",
      "Epoch 8/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1201 - val_loss: 0.1271\n",
      "Epoch 9/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1193 - val_loss: 0.1272\n",
      "Epoch 10/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1183 - val_loss: 0.1264\n",
      "Epoch 11/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1178 - val_loss: 0.1399\n",
      "Epoch 12/100\n",
      "33086/33086 [==============================] - 2s 51us/sample - loss: 0.1160 - val_loss: 0.1260\n",
      "Epoch 13/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1159 - val_loss: 0.1318\n",
      "Epoch 14/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1141 - val_loss: 0.1213\n",
      "Epoch 15/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1136 - val_loss: 0.1186\n",
      "Epoch 16/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1118 - val_loss: 0.1188\n",
      "Epoch 17/100\n",
      "33086/33086 [==============================] - 2s 54us/sample - loss: 0.1110 - val_loss: 0.1180\n",
      "Epoch 18/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1096 - val_loss: 0.1122\n",
      "Epoch 19/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1088 - val_loss: 0.1144\n",
      "Epoch 20/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1071 - val_loss: 0.1127\n",
      "Epoch 21/100\n",
      "33086/33086 [==============================] - 2s 49us/sample - loss: 0.1067 - val_loss: 0.1162\n",
      "Epoch 22/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1054 - val_loss: 0.1183\n",
      "Epoch 23/100\n",
      "33086/33086 [==============================] - 2s 55us/sample - loss: 0.1045 - val_loss: 0.1153\n",
      "Epoch 24/100\n",
      "33086/33086 [==============================] - 2s 52us/sample - loss: 0.1041 - val_loss: 0.1130\n",
      "Epoch 25/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1031 - val_loss: 0.1125\n",
      "Epoch 26/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1028 - val_loss: 0.1132\n",
      "Epoch 27/100\n",
      "33086/33086 [==============================] - 2s 56us/sample - loss: 0.1018 - val_loss: 0.1130\n",
      "Epoch 28/100\n",
      "33086/33086 [==============================] - 2s 53us/sample - loss: 0.1016 - val_loss: 0.1131\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 2\n",
      "Train on 33948 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33948/33948 [==============================] - 5s 152us/sample - loss: 0.1851 - val_loss: 0.1855\n",
      "Epoch 2/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1516 - val_loss: 0.1657\n",
      "Epoch 3/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1392 - val_loss: 0.1491\n",
      "Epoch 4/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1345 - val_loss: 0.1397\n",
      "Epoch 5/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1327 - val_loss: 0.1358\n",
      "Epoch 6/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1316 - val_loss: 0.1328\n",
      "Epoch 7/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1307 - val_loss: 0.1303\n",
      "Epoch 8/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1301 - val_loss: 0.1291\n",
      "Epoch 9/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1296 - val_loss: 0.1259\n",
      "Epoch 10/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1290 - val_loss: 0.1249\n",
      "Epoch 11/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1287 - val_loss: 0.1239\n",
      "Epoch 12/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1283 - val_loss: 0.1220\n",
      "Epoch 13/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1281 - val_loss: 0.1206\n",
      "Epoch 14/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1277 - val_loss: 0.1196\n",
      "Epoch 15/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1276 - val_loss: 0.1193\n",
      "Epoch 16/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1273 - val_loss: 0.1178\n",
      "Epoch 17/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1270 - val_loss: 0.1171\n",
      "Epoch 18/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1268 - val_loss: 0.1160\n",
      "Epoch 19/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1266 - val_loss: 0.1159\n",
      "Epoch 20/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1265 - val_loss: 0.1150\n",
      "Epoch 21/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1265 - val_loss: 0.1144\n",
      "Epoch 22/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1262 - val_loss: 0.1138\n",
      "Epoch 23/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1260 - val_loss: 0.1139\n",
      "Epoch 24/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1260 - val_loss: 0.1124\n",
      "Epoch 25/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1257 - val_loss: 0.1124\n",
      "Epoch 26/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1255 - val_loss: 0.1121\n",
      "Epoch 27/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1254 - val_loss: 0.1117\n",
      "Epoch 28/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1252 - val_loss: 0.1113\n",
      "Epoch 29/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1250 - val_loss: 0.1109\n",
      "Epoch 30/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1249 - val_loss: 0.1105\n",
      "Epoch 31/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1248 - val_loss: 0.1107\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1247 - val_loss: 0.1095\n",
      "Epoch 33/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1244 - val_loss: 0.1099\n",
      "Epoch 34/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1242 - val_loss: 0.1086\n",
      "Epoch 35/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1241 - val_loss: 0.1092\n",
      "Epoch 36/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1239 - val_loss: 0.1086\n",
      "Epoch 37/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1237 - val_loss: 0.1082\n",
      "Epoch 38/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1235 - val_loss: 0.1075\n",
      "Epoch 39/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1233 - val_loss: 0.1082\n",
      "Epoch 40/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1232 - val_loss: 0.1077\n",
      "Epoch 41/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1230 - val_loss: 0.1071\n",
      "Epoch 42/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1228 - val_loss: 0.1069\n",
      "Epoch 43/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1226 - val_loss: 0.1068\n",
      "Epoch 44/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1224 - val_loss: 0.1066\n",
      "Epoch 45/100\n",
      "33948/33948 [==============================] - 2s 58us/sample - loss: 0.1223 - val_loss: 0.1063\n",
      "Epoch 46/100\n",
      "33948/33948 [==============================] - 2s 58us/sample - loss: 0.1221 - val_loss: 0.1068\n",
      "Epoch 47/100\n",
      "33948/33948 [==============================] - 2s 60us/sample - loss: 0.1219 - val_loss: 0.1066\n",
      "Epoch 48/100\n",
      "33948/33948 [==============================] - 2s 61us/sample - loss: 0.1218 - val_loss: 0.1062\n",
      "Epoch 49/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1216 - val_loss: 0.1070\n",
      "Epoch 50/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1215 - val_loss: 0.1057\n",
      "Epoch 51/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1213 - val_loss: 0.1060\n",
      "Epoch 52/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1211 - val_loss: 0.1064\n",
      "Epoch 53/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1210 - val_loss: 0.1055\n",
      "Epoch 54/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1208 - val_loss: 0.1056\n",
      "Epoch 55/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1207 - val_loss: 0.1051\n",
      "Epoch 56/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1205 - val_loss: 0.1055\n",
      "Epoch 57/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1204 - val_loss: 0.1052\n",
      "Epoch 58/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1203 - val_loss: 0.1046\n",
      "Epoch 59/100\n",
      "33948/33948 [==============================] - 2s 58us/sample - loss: 0.1202 - val_loss: 0.1051\n",
      "Epoch 60/100\n",
      "33948/33948 [==============================] - 2s 61us/sample - loss: 0.1199 - val_loss: 0.1044\n",
      "Epoch 61/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1199 - val_loss: 0.1048\n",
      "Epoch 62/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1196 - val_loss: 0.1053\n",
      "Epoch 63/100\n",
      "33948/33948 [==============================] - 2s 62us/sample - loss: 0.1196 - val_loss: 0.1050\n",
      "Epoch 64/100\n",
      "33948/33948 [==============================] - 2s 61us/sample - loss: 0.1195 - val_loss: 0.1045\n",
      "Epoch 65/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1193 - val_loss: 0.1054\n",
      "Epoch 66/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1192 - val_loss: 0.1059\n",
      "Epoch 67/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1190 - val_loss: 0.1052\n",
      "Epoch 68/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1189 - val_loss: 0.1043\n",
      "Epoch 69/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1187 - val_loss: 0.1044\n",
      "Epoch 70/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1186 - val_loss: 0.1040\n",
      "Epoch 71/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1185 - val_loss: 0.1043\n",
      "Epoch 72/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1184 - val_loss: 0.1044\n",
      "Epoch 73/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1183 - val_loss: 0.1038\n",
      "Epoch 74/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1181 - val_loss: 0.1044\n",
      "Epoch 75/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1180 - val_loss: 0.1036\n",
      "Epoch 76/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1179 - val_loss: 0.1037\n",
      "Epoch 77/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1178 - val_loss: 0.1038\n",
      "Epoch 78/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1177 - val_loss: 0.1036\n",
      "Epoch 79/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1176 - val_loss: 0.1037\n",
      "Epoch 80/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1175 - val_loss: 0.1034\n",
      "Epoch 81/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1174 - val_loss: 0.1030\n",
      "Epoch 82/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1173 - val_loss: 0.1034\n",
      "Epoch 83/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1172 - val_loss: 0.1032\n",
      "Epoch 84/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1170 - val_loss: 0.1031\n",
      "Epoch 85/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1169 - val_loss: 0.1035\n",
      "Epoch 86/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1169 - val_loss: 0.1032\n",
      "Epoch 87/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1167 - val_loss: 0.1033\n",
      "Epoch 88/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1166 - val_loss: 0.1028\n",
      "Epoch 89/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1165 - val_loss: 0.1033\n",
      "Epoch 90/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1164 - val_loss: 0.1025\n",
      "Epoch 91/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1163 - val_loss: 0.1029\n",
      "Epoch 92/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1162 - val_loss: 0.1021\n",
      "Epoch 93/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1161 - val_loss: 0.1025\n",
      "Epoch 94/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1161 - val_loss: 0.1022\n",
      "Epoch 95/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1159 - val_loss: 0.1026\n",
      "Epoch 96/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1158 - val_loss: 0.1023\n",
      "Epoch 97/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1157 - val_loss: 0.1020\n",
      "Epoch 98/100\n",
      "33948/33948 [==============================] - 2s 57us/sample - loss: 0.1157 - val_loss: 0.1021\n",
      "Epoch 99/100\n",
      "33948/33948 [==============================] - 2s 59us/sample - loss: 0.1155 - val_loss: 0.1025\n",
      "Epoch 100/100\n",
      "33948/33948 [==============================] - 2s 58us/sample - loss: 0.1155 - val_loss: 0.1020\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 4\n",
      "Train on 33948 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33948/33948 [==============================] - 5s 134us/sample - loss: 0.1766 - val_loss: 0.1644\n",
      "Epoch 2/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1474 - val_loss: 0.1588\n",
      "Epoch 3/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1379 - val_loss: 0.1459\n",
      "Epoch 4/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1340 - val_loss: 0.1372\n",
      "Epoch 5/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1318 - val_loss: 0.1333\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1306 - val_loss: 0.1287\n",
      "Epoch 7/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1296 - val_loss: 0.1241\n",
      "Epoch 8/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1289 - val_loss: 0.1222\n",
      "Epoch 9/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1283 - val_loss: 0.1176\n",
      "Epoch 10/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1277 - val_loss: 0.1163\n",
      "Epoch 11/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1273 - val_loss: 0.1149\n",
      "Epoch 12/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1269 - val_loss: 0.1129\n",
      "Epoch 13/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1266 - val_loss: 0.1120\n",
      "Epoch 14/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1261 - val_loss: 0.1115\n",
      "Epoch 15/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1259 - val_loss: 0.1112\n",
      "Epoch 16/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1256 - val_loss: 0.1106\n",
      "Epoch 17/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1252 - val_loss: 0.1101\n",
      "Epoch 18/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1250 - val_loss: 0.1095\n",
      "Epoch 19/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1248 - val_loss: 0.1100\n",
      "Epoch 20/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1245 - val_loss: 0.1094\n",
      "Epoch 21/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1244 - val_loss: 0.1095\n",
      "Epoch 22/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1241 - val_loss: 0.1097\n",
      "Epoch 23/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1239 - val_loss: 0.1099\n",
      "Epoch 24/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1240 - val_loss: 0.1088\n",
      "Epoch 25/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1237 - val_loss: 0.1101\n",
      "Epoch 26/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1234 - val_loss: 0.1102\n",
      "Epoch 27/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1233 - val_loss: 0.1103\n",
      "Epoch 28/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1231 - val_loss: 0.1096\n",
      "Epoch 29/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1230 - val_loss: 0.1097\n",
      "Epoch 30/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1228 - val_loss: 0.1095\n",
      "Epoch 31/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1227 - val_loss: 0.1101\n",
      "Epoch 32/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1226 - val_loss: 0.1091\n",
      "Epoch 33/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1223 - val_loss: 0.1109\n",
      "Epoch 34/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1222 - val_loss: 0.1091\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 8\n",
      "Train on 33948 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33948/33948 [==============================] - 5s 147us/sample - loss: 0.1736 - val_loss: 0.1620\n",
      "Epoch 2/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1382 - val_loss: 0.1415\n",
      "Epoch 3/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1329 - val_loss: 0.1330\n",
      "Epoch 4/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1305 - val_loss: 0.1272\n",
      "Epoch 5/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1289 - val_loss: 0.1232\n",
      "Epoch 6/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1278 - val_loss: 0.1212\n",
      "Epoch 7/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1271 - val_loss: 0.1219\n",
      "Epoch 8/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1265 - val_loss: 0.1183\n",
      "Epoch 9/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1258 - val_loss: 0.1200\n",
      "Epoch 10/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1254 - val_loss: 0.1181\n",
      "Epoch 11/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1254 - val_loss: 0.1191\n",
      "Epoch 12/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1247 - val_loss: 0.1193\n",
      "Epoch 13/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1245 - val_loss: 0.1217\n",
      "Epoch 14/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1241 - val_loss: 0.1200\n",
      "Epoch 15/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1241 - val_loss: 0.1206\n",
      "Epoch 16/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1237 - val_loss: 0.1203\n",
      "Epoch 17/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1235 - val_loss: 0.1224\n",
      "Epoch 18/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1234 - val_loss: 0.1218\n",
      "Epoch 19/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1231 - val_loss: 0.1234\n",
      "Epoch 20/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1229 - val_loss: 0.1257\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 16\n",
      "Train on 33948 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33948/33948 [==============================] - 5s 135us/sample - loss: 0.1613 - val_loss: 0.1443\n",
      "Epoch 2/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1327 - val_loss: 0.1250\n",
      "Epoch 3/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1300 - val_loss: 0.1189\n",
      "Epoch 4/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1282 - val_loss: 0.1146\n",
      "Epoch 5/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1271 - val_loss: 0.1151\n",
      "Epoch 6/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1263 - val_loss: 0.1169\n",
      "Epoch 7/100\n",
      "33948/33948 [==============================] - 2s 49us/sample - loss: 0.1257 - val_loss: 0.1196\n",
      "Epoch 8/100\n",
      "33948/33948 [==============================] - 2s 49us/sample - loss: 0.1255 - val_loss: 0.1158\n",
      "Epoch 9/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1245 - val_loss: 0.1170\n",
      "Epoch 10/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1240 - val_loss: 0.1165\n",
      "Epoch 11/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1240 - val_loss: 0.1174\n",
      "Epoch 12/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1234 - val_loss: 0.1172\n",
      "Epoch 13/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1233 - val_loss: 0.1180\n",
      "Epoch 14/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1229 - val_loss: 0.1183\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 32\n",
      "Train on 33948 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33948/33948 [==============================] - 4s 131us/sample - loss: 0.1548 - val_loss: 0.1408\n",
      "Epoch 2/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1321 - val_loss: 0.1258\n",
      "Epoch 3/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1295 - val_loss: 0.1187\n",
      "Epoch 4/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1281 - val_loss: 0.1152\n",
      "Epoch 5/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1272 - val_loss: 0.1148\n",
      "Epoch 6/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1267 - val_loss: 0.1146\n",
      "Epoch 7/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1261 - val_loss: 0.1169\n",
      "Epoch 8/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1260 - val_loss: 0.1148\n",
      "Epoch 9/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1249 - val_loss: 0.1148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1246 - val_loss: 0.1152\n",
      "Epoch 11/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1245 - val_loss: 0.1162\n",
      "Epoch 12/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1240 - val_loss: 0.1161\n",
      "Epoch 13/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1237 - val_loss: 0.1166\n",
      "Epoch 14/100\n",
      "33948/33948 [==============================] - 2s 52us/sample - loss: 0.1235 - val_loss: 0.1172\n",
      "Epoch 15/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1230 - val_loss: 0.1184\n",
      "Epoch 16/100\n",
      "33948/33948 [==============================] - 2s 58us/sample - loss: 0.1229 - val_loss: 0.1187\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 64\n",
      "Train on 33948 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33948/33948 [==============================] - 5s 146us/sample - loss: 0.1477 - val_loss: 0.1366\n",
      "Epoch 2/100\n",
      "33948/33948 [==============================] - 2s 55us/sample - loss: 0.1319 - val_loss: 0.1253\n",
      "Epoch 3/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1296 - val_loss: 0.1181\n",
      "Epoch 4/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1286 - val_loss: 0.1155\n",
      "Epoch 5/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1276 - val_loss: 0.1148\n",
      "Epoch 6/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1276 - val_loss: 0.1131\n",
      "Epoch 7/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1267 - val_loss: 0.1144\n",
      "Epoch 8/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1268 - val_loss: 0.1144\n",
      "Epoch 9/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1253 - val_loss: 0.1142\n",
      "Epoch 10/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1248 - val_loss: 0.1152\n",
      "Epoch 11/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1245 - val_loss: 0.1180\n",
      "Epoch 12/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1239 - val_loss: 0.1159\n",
      "Epoch 13/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1235 - val_loss: 0.1181\n",
      "Epoch 14/100\n",
      "33948/33948 [==============================] - 2s 53us/sample - loss: 0.1233 - val_loss: 0.1179\n",
      "Epoch 15/100\n",
      "33948/33948 [==============================] - 2s 54us/sample - loss: 0.1227 - val_loss: 0.1192\n",
      "Epoch 16/100\n",
      "33948/33948 [==============================] - 2s 56us/sample - loss: 0.1225 - val_loss: 0.1189\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 128\n",
      "Train on 33948 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33948/33948 [==============================] - 4s 130us/sample - loss: 0.1475 - val_loss: 0.1349\n",
      "Epoch 2/100\n",
      "33948/33948 [==============================] - 2s 49us/sample - loss: 0.1315 - val_loss: 0.1241\n",
      "Epoch 3/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1300 - val_loss: 0.1167\n",
      "Epoch 4/100\n",
      "33948/33948 [==============================] - 2s 50us/sample - loss: 0.1288 - val_loss: 0.1133\n",
      "Epoch 5/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1277 - val_loss: 0.1130\n",
      "Epoch 6/100\n",
      "33948/33948 [==============================] - 2s 47us/sample - loss: 0.1271 - val_loss: 0.1144\n",
      "Epoch 7/100\n",
      "33948/33948 [==============================] - 2s 46us/sample - loss: 0.1267 - val_loss: 0.1150\n",
      "Epoch 8/100\n",
      "33948/33948 [==============================] - 2s 45us/sample - loss: 0.1257 - val_loss: 0.1189\n",
      "Epoch 9/100\n",
      "33948/33948 [==============================] - 2s 49us/sample - loss: 0.1249 - val_loss: 0.1170\n",
      "Epoch 10/100\n",
      "33948/33948 [==============================] - 2s 48us/sample - loss: 0.1242 - val_loss: 0.1179\n",
      "Epoch 11/100\n",
      "33948/33948 [==============================] - 2s 47us/sample - loss: 0.1238 - val_loss: 0.1189\n",
      "Epoch 12/100\n",
      "33948/33948 [==============================] - 2s 45us/sample - loss: 0.1230 - val_loss: 0.1170\n",
      "Epoch 13/100\n",
      "33948/33948 [==============================] - 2s 47us/sample - loss: 0.1224 - val_loss: 0.1180\n",
      "Epoch 14/100\n",
      "33948/33948 [==============================] - 2s 48us/sample - loss: 0.1219 - val_loss: 0.1164\n",
      "Epoch 15/100\n",
      "33948/33948 [==============================] - 2s 51us/sample - loss: 0.1212 - val_loss: 0.1194\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 2\n",
      "Train on 33675 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33675/33675 [==============================] - 5s 154us/sample - loss: 0.1806 - val_loss: 0.1838\n",
      "Epoch 2/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1435 - val_loss: 0.1508\n",
      "Epoch 3/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1337 - val_loss: 0.1435\n",
      "Epoch 4/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1308 - val_loss: 0.1394\n",
      "Epoch 5/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1287 - val_loss: 0.1363\n",
      "Epoch 6/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1275 - val_loss: 0.1356\n",
      "Epoch 7/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1264 - val_loss: 0.1355\n",
      "Epoch 8/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1256 - val_loss: 0.1337\n",
      "Epoch 9/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1249 - val_loss: 0.1323\n",
      "Epoch 10/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1245 - val_loss: 0.1312\n",
      "Epoch 11/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1241 - val_loss: 0.1325\n",
      "Epoch 12/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1237 - val_loss: 0.1322\n",
      "Epoch 13/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1233 - val_loss: 0.1344\n",
      "Epoch 14/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1231 - val_loss: 0.1297\n",
      "Epoch 15/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1228 - val_loss: 0.1309\n",
      "Epoch 16/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1224 - val_loss: 0.1321\n",
      "Epoch 17/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1222 - val_loss: 0.1304\n",
      "Epoch 18/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1218 - val_loss: 0.1301\n",
      "Epoch 19/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1217 - val_loss: 0.1293\n",
      "Epoch 20/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1215 - val_loss: 0.1305\n",
      "Epoch 21/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1212 - val_loss: 0.1307\n",
      "Epoch 22/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1210 - val_loss: 0.1308\n",
      "Epoch 23/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1208 - val_loss: 0.1293\n",
      "Epoch 24/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1207 - val_loss: 0.1286\n",
      "Epoch 25/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1205 - val_loss: 0.1298\n",
      "Epoch 26/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1203 - val_loss: 0.1302\n",
      "Epoch 27/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1202 - val_loss: 0.1286\n",
      "Epoch 28/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1200 - val_loss: 0.1298\n",
      "Epoch 29/100\n",
      "33675/33675 [==============================] - 2s 58us/sample - loss: 0.1200 - val_loss: 0.1284\n",
      "Epoch 30/100\n",
      "33675/33675 [==============================] - 2s 58us/sample - loss: 0.1198 - val_loss: 0.1286\n",
      "Epoch 31/100\n",
      "33675/33675 [==============================] - 2s 57us/sample - loss: 0.1196 - val_loss: 0.1276\n",
      "Epoch 32/100\n",
      "33675/33675 [==============================] - 2s 59us/sample - loss: 0.1195 - val_loss: 0.1284\n",
      "Epoch 33/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1194 - val_loss: 0.1299\n",
      "Epoch 34/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1193 - val_loss: 0.1284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1191 - val_loss: 0.1293\n",
      "Epoch 36/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1190 - val_loss: 0.1307\n",
      "Epoch 37/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1189 - val_loss: 0.1314\n",
      "Epoch 38/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1187 - val_loss: 0.1295\n",
      "Epoch 39/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1186 - val_loss: 0.1300\n",
      "Epoch 40/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1185 - val_loss: 0.1283\n",
      "Epoch 41/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1184 - val_loss: 0.1289\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 4\n",
      "Train on 33675 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33675/33675 [==============================] - 5s 135us/sample - loss: 0.1601 - val_loss: 0.1526\n",
      "Epoch 2/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1322 - val_loss: 0.1435\n",
      "Epoch 3/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1273 - val_loss: 0.1382\n",
      "Epoch 4/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1250 - val_loss: 0.1370\n",
      "Epoch 5/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1236 - val_loss: 0.1341\n",
      "Epoch 6/100\n",
      "33675/33675 [==============================] - 2s 47us/sample - loss: 0.1229 - val_loss: 0.1339\n",
      "Epoch 7/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1220 - val_loss: 0.1332\n",
      "Epoch 8/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1216 - val_loss: 0.1312\n",
      "Epoch 9/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1211 - val_loss: 0.1306\n",
      "Epoch 10/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1209 - val_loss: 0.1295\n",
      "Epoch 11/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1208 - val_loss: 0.1288\n",
      "Epoch 12/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1203 - val_loss: 0.1296\n",
      "Epoch 13/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1201 - val_loss: 0.1286\n",
      "Epoch 14/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1198 - val_loss: 0.1271\n",
      "Epoch 15/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1197 - val_loss: 0.1287\n",
      "Epoch 16/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1194 - val_loss: 0.1278\n",
      "Epoch 17/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1193 - val_loss: 0.1260\n",
      "Epoch 18/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1191 - val_loss: 0.1271\n",
      "Epoch 19/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1190 - val_loss: 0.1266\n",
      "Epoch 20/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1189 - val_loss: 0.1276\n",
      "Epoch 21/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1188 - val_loss: 0.1272\n",
      "Epoch 22/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1188 - val_loss: 0.1271\n",
      "Epoch 23/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1185 - val_loss: 0.1261\n",
      "Epoch 24/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1184 - val_loss: 0.1267\n",
      "Epoch 25/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1183 - val_loss: 0.1275\n",
      "Epoch 26/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1181 - val_loss: 0.1282\n",
      "Epoch 27/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1179 - val_loss: 0.1246\n",
      "Epoch 28/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1178 - val_loss: 0.1261\n",
      "Epoch 29/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1180 - val_loss: 0.1258\n",
      "Epoch 30/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1175 - val_loss: 0.1239\n",
      "Epoch 31/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1176 - val_loss: 0.1235\n",
      "Epoch 32/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1175 - val_loss: 0.1258\n",
      "Epoch 33/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1174 - val_loss: 0.1261\n",
      "Epoch 34/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1173 - val_loss: 0.1233\n",
      "Epoch 35/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1171 - val_loss: 0.1252\n",
      "Epoch 36/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1170 - val_loss: 0.1248\n",
      "Epoch 37/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1169 - val_loss: 0.1287\n",
      "Epoch 38/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1169 - val_loss: 0.1244\n",
      "Epoch 39/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1167 - val_loss: 0.1247\n",
      "Epoch 40/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1166 - val_loss: 0.1237\n",
      "Epoch 41/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1166 - val_loss: 0.1237\n",
      "Epoch 42/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1164 - val_loss: 0.1244\n",
      "Epoch 43/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1163 - val_loss: 0.1251\n",
      "Epoch 44/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1162 - val_loss: 0.1251\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 8\n",
      "Train on 33675 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33675/33675 [==============================] - 5s 152us/sample - loss: 0.1610 - val_loss: 0.1554\n",
      "Epoch 2/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1319 - val_loss: 0.1424\n",
      "Epoch 3/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1269 - val_loss: 0.1359\n",
      "Epoch 4/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1244 - val_loss: 0.1336\n",
      "Epoch 5/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1232 - val_loss: 0.1312\n",
      "Epoch 6/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1225 - val_loss: 0.1327\n",
      "Epoch 7/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1215 - val_loss: 0.1324\n",
      "Epoch 8/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1212 - val_loss: 0.1312\n",
      "Epoch 9/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1206 - val_loss: 0.1297\n",
      "Epoch 10/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1202 - val_loss: 0.1288\n",
      "Epoch 11/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1199 - val_loss: 0.1298\n",
      "Epoch 12/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1191 - val_loss: 0.1300\n",
      "Epoch 13/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1186 - val_loss: 0.1288\n",
      "Epoch 14/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1179 - val_loss: 0.1264\n",
      "Epoch 15/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1175 - val_loss: 0.1296\n",
      "Epoch 16/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1169 - val_loss: 0.1294\n",
      "Epoch 17/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1164 - val_loss: 0.1263\n",
      "Epoch 18/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1160 - val_loss: 0.1258\n",
      "Epoch 19/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1157 - val_loss: 0.1271\n",
      "Epoch 20/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1153 - val_loss: 0.1276\n",
      "Epoch 21/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1148 - val_loss: 0.1280\n",
      "Epoch 22/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1146 - val_loss: 0.1295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1141 - val_loss: 0.1259\n",
      "Epoch 24/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1139 - val_loss: 0.1264\n",
      "Epoch 25/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1135 - val_loss: 0.1257\n",
      "Epoch 26/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1132 - val_loss: 0.1279\n",
      "Epoch 27/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1130 - val_loss: 0.1256\n",
      "Epoch 28/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1126 - val_loss: 0.1274\n",
      "Epoch 29/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1124 - val_loss: 0.1262\n",
      "Epoch 30/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1121 - val_loss: 0.1246\n",
      "Epoch 31/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1119 - val_loss: 0.1254\n",
      "Epoch 32/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1116 - val_loss: 0.1263\n",
      "Epoch 33/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1114 - val_loss: 0.1290\n",
      "Epoch 34/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1112 - val_loss: 0.1256\n",
      "Epoch 35/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1109 - val_loss: 0.1251\n",
      "Epoch 36/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1106 - val_loss: 0.1254\n",
      "Epoch 37/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1104 - val_loss: 0.1267\n",
      "Epoch 38/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1104 - val_loss: 0.1240\n",
      "Epoch 39/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1101 - val_loss: 0.1234\n",
      "Epoch 40/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1098 - val_loss: 0.1231\n",
      "Epoch 41/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1096 - val_loss: 0.1254\n",
      "Epoch 42/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1095 - val_loss: 0.1244\n",
      "Epoch 43/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1093 - val_loss: 0.1235\n",
      "Epoch 44/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1091 - val_loss: 0.1229\n",
      "Epoch 45/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1088 - val_loss: 0.1245\n",
      "Epoch 46/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1088 - val_loss: 0.1233\n",
      "Epoch 47/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1086 - val_loss: 0.1222\n",
      "Epoch 48/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.1083 - val_loss: 0.1220\n",
      "Epoch 49/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1082 - val_loss: 0.1228\n",
      "Epoch 50/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1080 - val_loss: 0.1214\n",
      "Epoch 51/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1079 - val_loss: 0.1254\n",
      "Epoch 52/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1078 - val_loss: 0.1213\n",
      "Epoch 53/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1076 - val_loss: 0.1196\n",
      "Epoch 54/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1073 - val_loss: 0.1197\n",
      "Epoch 55/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1073 - val_loss: 0.1223\n",
      "Epoch 56/100\n",
      "33675/33675 [==============================] - 2s 60us/sample - loss: 0.1071 - val_loss: 0.1214\n",
      "Epoch 57/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1070 - val_loss: 0.1221\n",
      "Epoch 58/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1069 - val_loss: 0.1209\n",
      "Epoch 59/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1068 - val_loss: 0.1203\n",
      "Epoch 60/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1066 - val_loss: 0.1199\n",
      "Epoch 61/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1065 - val_loss: 0.1208\n",
      "Epoch 62/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1065 - val_loss: 0.1225\n",
      "Epoch 63/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1064 - val_loss: 0.1205\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 16\n",
      "Train on 33675 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33675/33675 [==============================] - 5s 136us/sample - loss: 0.1473 - val_loss: 0.1499\n",
      "Epoch 2/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1274 - val_loss: 0.1387\n",
      "Epoch 3/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1246 - val_loss: 0.1361\n",
      "Epoch 4/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1230 - val_loss: 0.1338\n",
      "Epoch 5/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1220 - val_loss: 0.1339\n",
      "Epoch 6/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1215 - val_loss: 0.1344\n",
      "Epoch 7/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1204 - val_loss: 0.1374\n",
      "Epoch 8/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1203 - val_loss: 0.1303\n",
      "Epoch 9/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1196 - val_loss: 0.1299\n",
      "Epoch 10/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1191 - val_loss: 0.1282\n",
      "Epoch 11/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1189 - val_loss: 0.1279\n",
      "Epoch 12/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1183 - val_loss: 0.1322\n",
      "Epoch 13/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1176 - val_loss: 0.1271\n",
      "Epoch 14/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1174 - val_loss: 0.1252\n",
      "Epoch 15/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1168 - val_loss: 0.1276\n",
      "Epoch 16/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1162 - val_loss: 0.1270\n",
      "Epoch 17/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1159 - val_loss: 0.1235\n",
      "Epoch 18/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1153 - val_loss: 0.1255\n",
      "Epoch 19/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1150 - val_loss: 0.1252\n",
      "Epoch 20/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1146 - val_loss: 0.1280\n",
      "Epoch 21/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1141 - val_loss: 0.1264\n",
      "Epoch 22/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1137 - val_loss: 0.1244\n",
      "Epoch 23/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1133 - val_loss: 0.1252\n",
      "Epoch 24/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1128 - val_loss: 0.1230\n",
      "Epoch 25/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1122 - val_loss: 0.1230\n",
      "Epoch 26/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1118 - val_loss: 0.1238\n",
      "Epoch 27/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1117 - val_loss: 0.1243\n",
      "Epoch 28/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1113 - val_loss: 0.1246\n",
      "Epoch 29/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1109 - val_loss: 0.1270\n",
      "Epoch 30/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1105 - val_loss: 0.1221\n",
      "Epoch 31/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1101 - val_loss: 0.1230\n",
      "Epoch 32/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1096 - val_loss: 0.1251\n",
      "Epoch 33/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1093 - val_loss: 0.1243\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1090 - val_loss: 0.1232\n",
      "Epoch 35/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1090 - val_loss: 0.1233\n",
      "Epoch 36/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1086 - val_loss: 0.1237\n",
      "Epoch 37/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1082 - val_loss: 0.1217\n",
      "Epoch 38/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1084 - val_loss: 0.1256\n",
      "Epoch 39/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1079 - val_loss: 0.1212\n",
      "Epoch 40/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1075 - val_loss: 0.1232\n",
      "Epoch 41/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1074 - val_loss: 0.1217\n",
      "Epoch 42/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1072 - val_loss: 0.1242\n",
      "Epoch 43/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1071 - val_loss: 0.1217\n",
      "Epoch 44/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1070 - val_loss: 0.1215\n",
      "Epoch 45/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1070 - val_loss: 0.1215\n",
      "Epoch 46/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1064 - val_loss: 0.1211\n",
      "Epoch 47/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1066 - val_loss: 0.1209\n",
      "Epoch 48/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1062 - val_loss: 0.1179\n",
      "Epoch 49/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1061 - val_loss: 0.1198\n",
      "Epoch 50/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1062 - val_loss: 0.1225\n",
      "Epoch 51/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1062 - val_loss: 0.1205\n",
      "Epoch 52/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1057 - val_loss: 0.1207\n",
      "Epoch 53/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1057 - val_loss: 0.1191\n",
      "Epoch 54/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1053 - val_loss: 0.1195\n",
      "Epoch 55/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1056 - val_loss: 0.1191\n",
      "Epoch 56/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1054 - val_loss: 0.1214\n",
      "Epoch 57/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1055 - val_loss: 0.1194\n",
      "Epoch 58/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1054 - val_loss: 0.1204\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 32\n",
      "Train on 33675 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33675/33675 [==============================] - 5s 148us/sample - loss: 0.1466 - val_loss: 0.1491\n",
      "Epoch 2/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1270 - val_loss: 0.1395\n",
      "Epoch 3/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1247 - val_loss: 0.1344\n",
      "Epoch 4/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1233 - val_loss: 0.1300\n",
      "Epoch 5/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1221 - val_loss: 0.1284\n",
      "Epoch 6/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1215 - val_loss: 0.1299\n",
      "Epoch 7/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1208 - val_loss: 0.1330\n",
      "Epoch 8/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1207 - val_loss: 0.1279\n",
      "Epoch 9/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1196 - val_loss: 0.1275\n",
      "Epoch 10/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1186 - val_loss: 0.1275\n",
      "Epoch 11/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1182 - val_loss: 0.1252\n",
      "Epoch 12/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1173 - val_loss: 0.1345\n",
      "Epoch 13/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1164 - val_loss: 0.1214\n",
      "Epoch 14/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1162 - val_loss: 0.1241\n",
      "Epoch 15/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1152 - val_loss: 0.1236\n",
      "Epoch 16/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1146 - val_loss: 0.1264\n",
      "Epoch 17/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1144 - val_loss: 0.1208\n",
      "Epoch 18/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1136 - val_loss: 0.1230\n",
      "Epoch 19/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1131 - val_loss: 0.1247\n",
      "Epoch 20/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1129 - val_loss: 0.1246\n",
      "Epoch 21/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1120 - val_loss: 0.1242\n",
      "Epoch 22/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1119 - val_loss: 0.1217\n",
      "Epoch 23/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1113 - val_loss: 0.1225\n",
      "Epoch 24/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1106 - val_loss: 0.1206\n",
      "Epoch 25/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1097 - val_loss: 0.1185\n",
      "Epoch 26/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1090 - val_loss: 0.1196\n",
      "Epoch 27/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1087 - val_loss: 0.1246\n",
      "Epoch 28/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1081 - val_loss: 0.1179\n",
      "Epoch 29/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1077 - val_loss: 0.1188\n",
      "Epoch 30/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1072 - val_loss: 0.1181\n",
      "Epoch 31/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1067 - val_loss: 0.1183\n",
      "Epoch 32/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1063 - val_loss: 0.1220\n",
      "Epoch 33/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1062 - val_loss: 0.1176\n",
      "Epoch 34/100\n",
      "33675/33675 [==============================] - 2s 45us/sample - loss: 0.1056 - val_loss: 0.1161\n",
      "Epoch 35/100\n",
      "33675/33675 [==============================] - 1s 44us/sample - loss: 0.1055 - val_loss: 0.1177\n",
      "Epoch 36/100\n",
      "33675/33675 [==============================] - 1s 44us/sample - loss: 0.1056 - val_loss: 0.1182\n",
      "Epoch 37/100\n",
      "33675/33675 [==============================] - 2s 46us/sample - loss: 0.1050 - val_loss: 0.1175\n",
      "Epoch 38/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.1049 - val_loss: 0.1168\n",
      "Epoch 39/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1047 - val_loss: 0.1161\n",
      "Epoch 40/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1043 - val_loss: 0.1153\n",
      "Epoch 41/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1044 - val_loss: 0.1158\n",
      "Epoch 42/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1039 - val_loss: 0.1186\n",
      "Epoch 43/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1040 - val_loss: 0.1160\n",
      "Epoch 44/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1035 - val_loss: 0.1159\n",
      "Epoch 45/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1034 - val_loss: 0.1170\n",
      "Epoch 46/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1034 - val_loss: 0.1158\n",
      "Epoch 47/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1032 - val_loss: 0.1173\n",
      "Epoch 48/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.1027 - val_loss: 0.1123\n",
      "Epoch 49/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1030 - val_loss: 0.1153\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1030 - val_loss: 0.1156\n",
      "Epoch 51/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1023 - val_loss: 0.1141\n",
      "Epoch 52/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1022 - val_loss: 0.1149\n",
      "Epoch 53/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1024 - val_loss: 0.1139\n",
      "Epoch 54/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1020 - val_loss: 0.1161\n",
      "Epoch 55/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1019 - val_loss: 0.1148\n",
      "Epoch 56/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1020 - val_loss: 0.1153\n",
      "Epoch 57/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1017 - val_loss: 0.1135\n",
      "Epoch 58/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.1014 - val_loss: 0.1129\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 64\n",
      "Train on 33675 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33675/33675 [==============================] - 4s 129us/sample - loss: 0.1429 - val_loss: 0.1477\n",
      "Epoch 2/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1267 - val_loss: 0.1411\n",
      "Epoch 3/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1251 - val_loss: 0.1343\n",
      "Epoch 4/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1238 - val_loss: 0.1325\n",
      "Epoch 5/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1225 - val_loss: 0.1346\n",
      "Epoch 6/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1223 - val_loss: 0.1342\n",
      "Epoch 7/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1213 - val_loss: 0.1354\n",
      "Epoch 8/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1213 - val_loss: 0.1334\n",
      "Epoch 9/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1201 - val_loss: 0.1322\n",
      "Epoch 10/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1195 - val_loss: 0.1276\n",
      "Epoch 11/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1192 - val_loss: 0.1241\n",
      "Epoch 12/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1185 - val_loss: 0.1338\n",
      "Epoch 13/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.1175 - val_loss: 0.1227\n",
      "Epoch 14/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.1178 - val_loss: 0.1236\n",
      "Epoch 15/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1162 - val_loss: 0.1238\n",
      "Epoch 16/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1150 - val_loss: 0.1231\n",
      "Epoch 17/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1145 - val_loss: 0.1223\n",
      "Epoch 18/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1135 - val_loss: 0.1278\n",
      "Epoch 19/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1126 - val_loss: 0.1240\n",
      "Epoch 20/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1119 - val_loss: 0.1245\n",
      "Epoch 21/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1107 - val_loss: 0.1245\n",
      "Epoch 22/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.1101 - val_loss: 0.1204\n",
      "Epoch 23/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.1094 - val_loss: 0.1193\n",
      "Epoch 24/100\n",
      "33675/33675 [==============================] - 2s 47us/sample - loss: 0.1083 - val_loss: 0.1196\n",
      "Epoch 25/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.1071 - val_loss: 0.1179\n",
      "Epoch 26/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1065 - val_loss: 0.1175\n",
      "Epoch 27/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1057 - val_loss: 0.1205\n",
      "Epoch 28/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1051 - val_loss: 0.1170\n",
      "Epoch 29/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1049 - val_loss: 0.1177\n",
      "Epoch 30/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1044 - val_loss: 0.1166\n",
      "Epoch 31/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1042 - val_loss: 0.1173\n",
      "Epoch 32/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1032 - val_loss: 0.1174\n",
      "Epoch 33/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1030 - val_loss: 0.1172\n",
      "Epoch 34/100\n",
      "33675/33675 [==============================] - 2s 47us/sample - loss: 0.1027 - val_loss: 0.1164\n",
      "Epoch 35/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1026 - val_loss: 0.1164\n",
      "Epoch 36/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1027 - val_loss: 0.1168\n",
      "Epoch 37/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1020 - val_loss: 0.1160\n",
      "Epoch 38/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1022 - val_loss: 0.1162\n",
      "Epoch 39/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1020 - val_loss: 0.1163\n",
      "Epoch 40/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1012 - val_loss: 0.1163\n",
      "Epoch 41/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1013 - val_loss: 0.1158\n",
      "Epoch 42/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1011 - val_loss: 0.1180\n",
      "Epoch 43/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1007 - val_loss: 0.1158\n",
      "Epoch 44/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1007 - val_loss: 0.1147\n",
      "Epoch 45/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1006 - val_loss: 0.1157\n",
      "Epoch 46/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.1005 - val_loss: 0.1163\n",
      "Epoch 47/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1004 - val_loss: 0.1156\n",
      "Epoch 48/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1001 - val_loss: 0.1126\n",
      "Epoch 49/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1002 - val_loss: 0.1159\n",
      "Epoch 50/100\n",
      "33675/33675 [==============================] - 2s 51us/sample - loss: 0.0999 - val_loss: 0.1145\n",
      "Epoch 51/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.0998 - val_loss: 0.1144\n",
      "Epoch 52/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.0995 - val_loss: 0.1134\n",
      "Epoch 53/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.0998 - val_loss: 0.1153\n",
      "Epoch 54/100\n",
      "33675/33675 [==============================] - 2s 48us/sample - loss: 0.0994 - val_loss: 0.1153\n",
      "Epoch 55/100\n",
      "33675/33675 [==============================] - 2s 49us/sample - loss: 0.0997 - val_loss: 0.1143\n",
      "Epoch 56/100\n",
      "33675/33675 [==============================] - 2s 46us/sample - loss: 0.0994 - val_loss: 0.1151\n",
      "Epoch 57/100\n",
      "33675/33675 [==============================] - 1s 44us/sample - loss: 0.0992 - val_loss: 0.1148\n",
      "Epoch 58/100\n",
      "33675/33675 [==============================] - 2s 47us/sample - loss: 0.0991 - val_loss: 0.1153\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 128\n",
      "Train on 33675 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "33675/33675 [==============================] - 5s 160us/sample - loss: 0.1409 - val_loss: 0.1457\n",
      "Epoch 2/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1265 - val_loss: 0.1378\n",
      "Epoch 3/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1255 - val_loss: 0.1336\n",
      "Epoch 4/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1236 - val_loss: 0.1303\n",
      "Epoch 5/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1222 - val_loss: 0.1291\n",
      "Epoch 6/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1223 - val_loss: 0.1303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1209 - val_loss: 0.1342\n",
      "Epoch 8/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1208 - val_loss: 0.1315\n",
      "Epoch 9/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1191 - val_loss: 0.1305\n",
      "Epoch 10/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1183 - val_loss: 0.1268\n",
      "Epoch 11/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1176 - val_loss: 0.1229\n",
      "Epoch 12/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1171 - val_loss: 0.1357\n",
      "Epoch 13/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1157 - val_loss: 0.1250\n",
      "Epoch 14/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1163 - val_loss: 0.1234\n",
      "Epoch 15/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1141 - val_loss: 0.1267\n",
      "Epoch 16/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1129 - val_loss: 0.1252\n",
      "Epoch 17/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1117 - val_loss: 0.1214\n",
      "Epoch 18/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1100 - val_loss: 0.1233\n",
      "Epoch 19/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1092 - val_loss: 0.1245\n",
      "Epoch 20/100\n",
      "33675/33675 [==============================] - 2s 50us/sample - loss: 0.1079 - val_loss: 0.1215\n",
      "Epoch 21/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1067 - val_loss: 0.1187\n",
      "Epoch 22/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1062 - val_loss: 0.1155\n",
      "Epoch 23/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1048 - val_loss: 0.1183\n",
      "Epoch 24/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1039 - val_loss: 0.1170\n",
      "Epoch 25/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1032 - val_loss: 0.1152\n",
      "Epoch 26/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1029 - val_loss: 0.1159\n",
      "Epoch 27/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1020 - val_loss: 0.1163\n",
      "Epoch 28/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.1017 - val_loss: 0.1155\n",
      "Epoch 29/100\n",
      "33675/33675 [==============================] - 2s 52us/sample - loss: 0.1014 - val_loss: 0.1162\n",
      "Epoch 30/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1013 - val_loss: 0.1122\n",
      "Epoch 31/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1010 - val_loss: 0.1177\n",
      "Epoch 32/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1004 - val_loss: 0.1160\n",
      "Epoch 33/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1002 - val_loss: 0.1163\n",
      "Epoch 34/100\n",
      "33675/33675 [==============================] - 2s 55us/sample - loss: 0.1001 - val_loss: 0.1167\n",
      "Epoch 35/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.1001 - val_loss: 0.1168\n",
      "Epoch 36/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.1001 - val_loss: 0.1160\n",
      "Epoch 37/100\n",
      "33675/33675 [==============================] - 2s 53us/sample - loss: 0.0995 - val_loss: 0.1168\n",
      "Epoch 38/100\n",
      "33675/33675 [==============================] - 2s 54us/sample - loss: 0.0994 - val_loss: 0.1152\n",
      "Epoch 39/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.0991 - val_loss: 0.1152\n",
      "Epoch 40/100\n",
      "33675/33675 [==============================] - 2s 56us/sample - loss: 0.0990 - val_loss: 0.1157\n"
     ]
    }
   ],
   "source": [
    "lag_vec = [7,14]\n",
    "units_vec = [2,4,8,16,32,64,128]\n",
    "results = cross_validation(lag_vec, units_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[6323.73140554 6263.83494125 6287.49809807 6254.08987446 6169.29857926\n",
      "   6178.41568013 6185.95352823]\n",
      "  [5164.86452351 5231.30340131 4843.88825572 5192.68984234 5093.58263642\n",
      "   5101.73560234 5092.15307368]]\n",
      "\n",
      " [[5324.47222258 5904.47536268 5663.27872347 5661.02716225 5722.21289705\n",
      "   5691.83981644 5686.43871107]\n",
      "  [5641.02667786 6245.18209284 6593.02181349 5562.94022741 6341.59367166\n",
      "   6233.14839439 5629.18155938]]\n",
      "\n",
      " [[5119.85919682 5457.03654485 5926.51628546 5750.60469607 5752.06117821\n",
      "   5677.64595669 5669.45929705]\n",
      "  [6401.44133943 6188.19545256 6003.1250287  5917.19049479 5636.19368961\n",
      "   5651.09253958 5631.15161904]]]\n",
      "14 128\n"
     ]
    }
   ],
   "source": [
    "MAE = results[2,:]\n",
    "print(MAE)\n",
    "MAE = numpy.sum(MAE, axis=0)\n",
    "ind = numpy.unravel_index(numpy.argmin(MAE, axis=None), MAE.shape)\n",
    "\n",
    "lag = lag_vec[ind[0]]\n",
    "units = units_vec[ind[1]]\n",
    "print(lag,units)\n",
    "\n",
    "# units = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34226 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "34226/34226 [==============================] - 5s 133us/sample - loss: 0.1401 - val_loss: 0.1230\n",
      "Epoch 2/100\n",
      "34226/34226 [==============================] - 2s 55us/sample - loss: 0.1270 - val_loss: 0.1201\n",
      "Epoch 3/100\n",
      "34226/34226 [==============================] - 2s 53us/sample - loss: 0.1252 - val_loss: 0.1186\n",
      "Epoch 4/100\n",
      "34226/34226 [==============================] - 2s 55us/sample - loss: 0.1239 - val_loss: 0.1203\n",
      "Epoch 5/100\n",
      "34226/34226 [==============================] - 2s 54us/sample - loss: 0.1234 - val_loss: 0.1173\n",
      "Epoch 6/100\n",
      "34226/34226 [==============================] - 2s 53us/sample - loss: 0.1224 - val_loss: 0.1250\n",
      "Epoch 7/100\n",
      "34226/34226 [==============================] - 2s 53us/sample - loss: 0.1215 - val_loss: 0.1252\n",
      "Epoch 8/100\n",
      "34226/34226 [==============================] - 2s 55us/sample - loss: 0.1209 - val_loss: 0.1172\n",
      "Epoch 9/100\n",
      "34226/34226 [==============================] - 2s 56us/sample - loss: 0.1195 - val_loss: 0.1256\n",
      "Epoch 10/100\n",
      "34226/34226 [==============================] - 2s 53us/sample - loss: 0.1183 - val_loss: 0.1157\n",
      "Epoch 11/100\n",
      "34226/34226 [==============================] - 2s 53us/sample - loss: 0.1178 - val_loss: 0.1260\n",
      "Epoch 12/100\n",
      "34226/34226 [==============================] - 2s 54us/sample - loss: 0.1166 - val_loss: 0.1099\n",
      "Epoch 13/100\n",
      "34226/34226 [==============================] - 2s 54us/sample - loss: 0.1149 - val_loss: 0.1106\n",
      "Epoch 14/100\n",
      "34226/34226 [==============================] - 2s 53us/sample - loss: 0.1144 - val_loss: 0.1091\n",
      "Epoch 15/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1136 - val_loss: 0.1227\n",
      "Epoch 16/100\n",
      "34226/34226 [==============================] - 2s 50us/sample - loss: 0.1124 - val_loss: 0.1084\n",
      "Epoch 17/100\n",
      "34226/34226 [==============================] - 2s 49us/sample - loss: 0.1117 - val_loss: 0.1104\n",
      "Epoch 18/100\n",
      "34226/34226 [==============================] - 2s 53us/sample - loss: 0.1105 - val_loss: 0.1139\n",
      "Epoch 19/100\n",
      "34226/34226 [==============================] - 2s 56us/sample - loss: 0.1095 - val_loss: 0.1198\n",
      "Epoch 20/100\n",
      "34226/34226 [==============================] - 2s 55us/sample - loss: 0.1084 - val_loss: 0.1154\n",
      "Epoch 21/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1074 - val_loss: 0.1102\n",
      "Epoch 22/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1060 - val_loss: 0.1112\n",
      "Epoch 23/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1059 - val_loss: 0.1061\n",
      "Epoch 24/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1049 - val_loss: 0.1095\n",
      "Epoch 25/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1041 - val_loss: 0.1077\n",
      "Epoch 26/100\n",
      "34226/34226 [==============================] - 2s 54us/sample - loss: 0.1039 - val_loss: 0.1036\n",
      "Epoch 27/100\n",
      "34226/34226 [==============================] - 2s 53us/sample - loss: 0.1037 - val_loss: 0.1081\n",
      "Epoch 28/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1024 - val_loss: 0.1064\n",
      "Epoch 29/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1021 - val_loss: 0.1052\n",
      "Epoch 30/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1017 - val_loss: 0.1076\n",
      "Epoch 31/100\n",
      "34226/34226 [==============================] - 2s 50us/sample - loss: 0.1019 - val_loss: 0.1054\n",
      "Epoch 32/100\n",
      "34226/34226 [==============================] - 2s 51us/sample - loss: 0.1015 - val_loss: 0.1047\n",
      "Epoch 33/100\n",
      "34226/34226 [==============================] - 2s 51us/sample - loss: 0.1010 - val_loss: 0.1046\n",
      "Epoch 34/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1012 - val_loss: 0.1064\n",
      "Epoch 35/100\n",
      "34226/34226 [==============================] - 2s 50us/sample - loss: 0.1007 - val_loss: 0.1054\n",
      "Epoch 36/100\n",
      "34226/34226 [==============================] - 2s 52us/sample - loss: 0.1006 - val_loss: 0.1048\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABOHElEQVR4nO3dd3xUZfb48c9JIY0kkAaBhN6LdAQBFZVmA3VFUZDdte5ad9VVf27R76679l4QXdYO69oLShNEFOmIdCI1tIQSOiHl+f3x3IEhTJI7yUwmIef9euU1M3fm3nky4pw87RwxxqCUUkq5FRbqBiillKpZNHAopZTyiwYOpZRSftHAoZRSyi8aOJRSSvklItQNqAopKSmmWbNmoW6GUkrVKIsWLdpljEktebxWBI5mzZqxcOHCUDdDKaVqFBHZ5Ou4DlUppZTyiwYOpZRSftHAoZRSyi+1Yo5DKaX8VVBQQHZ2NkePHg11U4IuOjqajIwMIiMjXb1eA4dSSvmQnZ1NfHw8zZo1Q0RC3ZygMcawe/dusrOzad68uatzdKhKKaV8OHr0KMnJyad10AAQEZKTk/3qWQU1cIjIUBFZIyJZInK/j+fbichcEckXkXt8PB8uIktE5AuvY0kiMk1E1jm39YP5Oyilaq/TPWh4+Pt7Bi1wiEg48BIwDOgAjBKRDiVetge4A3iylMvcCawqcex+YIYxpjUww3kcFN+s3snLs7KCdXmllKqRgtnj6A1kGWPWG2OOAZOA4d4vMMbkGGMWAAUlTxaRDOAi4PUSTw0H3nTuvwmMCHC7j/s+azcvzMhCa5YopUIhLy+Pl19+2e/zLrzwQvLy8gLfIEcwA0djYIvX42znmFvPAn8Cikscb2CM2Q7g3Kb5OllEbhKRhSKyMDc314+3PaFJUixHCorYdfBYhc5XSqnKKC1wFBUVlXne5MmTqVevXpBaFdzA4WvQzNWf7iJyMZBjjFlU0Tc3xow3xvQ0xvRMTT0l1YorTZJiAdiy93BFm6GUUhV2//3388svv9C1a1d69erFwIEDueaaa+jcuTMAI0aMoEePHnTs2JHx48cfP69Zs2bs2rWLjRs30r59e2688UY6duzI4MGDOXLkSKXbFczluNlAptfjDGCby3P7AZeKyIVANJAgIu8YY0YDO0Uk3RizXUTSgZyAttpLpidw7DlM9yY6B69UbfXw5ytYuW1/QK/ZoVECf7ukY5mvefTRR1m+fDlLly5l1qxZXHTRRSxfvvz4stkJEyaQlJTEkSNH6NWrF1dccQXJycknXWPdunVMnDiR1157jZEjR/Lhhx8yevToSrU9mD2OBUBrEWkuInWAq4HP3JxojHnAGJNhjGnmnPeNEzRwrjHWuT8W+DSwzT4ho34MAJt3a49DKRV6vXv3PmmvxfPPP0+XLl3o06cPW7ZsYd26daec07x5c7p27QpAjx492LhxY6XbEbQehzGmUERuA6YA4cAEY8wKEbnFeX6ciDQEFgIJQLGI3AV0MMaUFdofBd4XkeuBzcCVwfodoiPDaZAQxeY9GjiUqs3K6xlUlbi4uOP3Z82axfTp05k7dy6xsbGce+65PvdiREVFHb8fHh5e7YeqMMZMBiaXODbO6/4O7BBWWdeYBczyerwbOD+Q7SxLk6RYDRxKqZCIj4/nwIEDPp/bt28f9evXJzY2ltWrV/Pjjz9WWbs05Ug5MpNi+fGX3aFuhlKqFkpOTqZfv3506tSJmJgYGjRocPy5oUOHMm7cOM444wzatm1Lnz59qqxdGjjK0SQplo+XbCW/sIioiPBQN0cpVcu89957Po9HRUXx1Vdf+XzOM4+RkpLC8uXLjx+/555TEnRUiOaqKkeTpFiMga17Kz8uqJRSpwMNHOU4sZdDA4dSSoEGjnJ5AodOkCullKWBoxyp8VFERYSxRQOHUkoBGjjKJSJkJsXqJkCllHJo4HBB93IopdQJGjhcaJIUy5Y9hzW9ulKqSlU0rTrAs88+y+HDwfmDVwOHC5lJsRzILyTv8CllQ5RSKmiqa+DQDYAueK+sqh9XJ8StUUrVFt5p1QcNGkRaWhrvv/8++fn5XHbZZTz88MMcOnSIkSNHkp2dTVFREX/5y1/YuXMn27ZtY+DAgaSkpDBz5syAtksDhwvedTm6ZNYLbWOUUlXvq/thx8+BvWbDzjDs0TJf4p1WferUqXzwwQfMnz8fYwyXXnops2fPJjc3l0aNGvHll18CNodVYmIiTz/9NDNnziQlJSWw7UaHqlw5nl5dJ8iVUiEydepUpk6dSrdu3ejevTurV69m3bp1dO7cmenTp3Pffffx3XffkZiYGPS2aI/DhbioCFLq1tG9HErVVuX0DKqCMYYHHniAm2+++ZTnFi1axOTJk3nggQcYPHgwf/3rX4PaFu1xuJSpS3KVUlXMO636kCFDmDBhAgcPHgRg69at5OTksG3bNmJjYxk9ejT33HMPixcvPuXcQNMeh0tNkmJZvHlvqJuhlKpFvNOqDxs2jGuuuYa+ffsCULduXd555x2ysrK49957CQsLIzIykldeeQWAm266iWHDhpGenh7wyXEJ5t4EERkKPIetAPi6MebREs+3A/4DdAceNMY86RyPBmYDUdjg9oEx5m/Ocw8BNwK5zmX+n1MwqlQ9e/Y0CxcurNTv8tTUNbw86xdW/30okeHaUVPqdLdq1Srat28f6mZUGV+/r4gsMsb0LPnaoPU4RCQceAkYBGQDC0TkM2PMSq+X7QHuAEaUOD0fOM8Yc1BEIoE5IvKVMcZT4uoZT5CpKplJsRQVG7bnHaVJcmxVvrVSSlUrwfzTuTeQZYxZb4w5BkwChnu/wBiTY4xZABSUOG6MMQedh5HOT0i3bWuWXKWUsoIZOBoDW7weZzvHXBGRcBFZCuQA04wx87yevk1ElonIBBGpX8r5N4nIQhFZmJub6+slfvHey6GUqh1qS5ohf3/PYAYO8XHMdeuMMUXGmK5ABtBbRDo5T70CtAS6AtuBp0o5f7wxpqcxpmdqaqo/7fapQUI0keGiPQ6laono6Gh279592gcPYwy7d+8mOjra9TnBXFWVDWR6Pc4Atvl7EWNMnojMAoYCy40xOz3PichrwBeVbKcr4WFCRn1dkqtUbZGRkUF2djaBGLGo7qKjo8nIyHD9+mAGjgVAaxFpDmwFrgaucXOiiKQCBU7QiAEuAB5znks3xmx3XnoZsLyUywRcppMlVyl1+ouMjKR58+ahbka1FLTAYYwpFJHbgCnY5bgTjDErROQW5/lxItIQWAgkAMUichfQAUgH3nRWZoUB7xtjPD2Lx0WkK3bYayNw6jbKIGmSFMOy7LyqejullKqWgroB0NlfMbnEsXFe93dgh7BKWgZ0K+WaYwLZRn80SYol73AB+44UkBgTGapmKKVUSOlONj8cX1mlw1VKqVpMA4cfMjVwKKWUBg5/ZOpeDqWU0sDhj4ToSOrFRuqSXKVUraaBw09NkmLZvOdIqJuhlFIho4HDT7qXQylV22ng8FOTpFiy9x6mqPj0TkOglFKl0cDhpyZJsRQUGXbsPxrqpiilVEho4PDT8fTqu3W4SilVO2ng8JNuAlRK1XYaOPyUnhhNeJjoXg6lVK2lgcNPEeFhNKoXrXs5lFK1lgaOCrB7OTRwKKVqJw0cFdBE93IopWoxDRwVkJkUy66DxziUXxjqpiilVJXTwFEBTTTZoVKqFgtq4BCRoSKyRkSyROR+H8+3E5G5IpIvIvd4HY8Wkfki8pOIrBCRh72eSxKRaSKyzrmtH8zfwRfdy6GUqs2CFjicsq8vAcOw5WBHiUiHEi/bA9wBPFnieD5wnjGmC9AVGCoifZzn7gdmGGNaAzOcx1XqeODQeQ6lVC1UZuAQkXAR+UMFr90byDLGrDfGHAMmAcO9X2CMyTHGLAAKShw3xpiDzsNI58eTHGo48KZz/01gRAXbV2GJMZHER0WQvVez5Cqlap8yA4cxpogSX/Z+aAxs8Xqc7RxzxQlaS4EcYJoxZp7zVANjzHanfduBtFLOv0lEForIwtzc3Iq0v6y2kalLcpVStZSboarvReRFERkgIt09Py7OEx/HXKeUNcYUGWO6AhlAbxHp5PZc5/zxxpiexpieqamp/pzqiu7lUErVVhEuXnOWc/t/XscMcF4552UDmV6PM4Bt7pvmvJExeSIyCxgKLAd2iki6MWa7iKRjeyRVrklyLDPX5FBcbAgL8xUjlVLq9FRu4DDGDKzgtRcArUWkObAVuBq4xs2JIpIKFDhBIwa4AHjMefozYCzwqHP7aQXbVymZSbHkFxaTezCfBgnRoWiCUkqFRLmBQ0QSgb8BZzuHvgX+zxizr6zzjDGFInIbMAUIByYYY1aIyC3O8+NEpCGwEEgAikXkLuwKrHTgTWdlVhjwvjHmC+fSjwLvi8j1wGbgSn9+4UDxXlmlgUMpVZu4GaqagB0iGuk8HgP8B7i8vBONMZOBySWOjfO6vwM7hFXSMqBbKdfcDZzvot1B5b2Xo1ezpBC3Rimlqo6bwNHSGHOF1+OHndVOtVqjetGI6F4OpVTt42ZV1RER6e95ICL9gFq/gSEqIpz0hGhNO6KUqnXc9DhuAd5y5joA9mInpWu9TM2Sq5SqhcoMHM7k9GhjTBcRSQAwxuyvkpbVAE2SYpm9LrCbC5VSqrpzs3O8h3N/vwaNkzVJimXn/nyOFhSFuilKKVVl3AxVLRGRz4D/AYc8B40xHwWtVTVEk2S7sip772FapcWHuDVKKVU13ASOJGA3J+8UN0CtDxyZXns5NHAopWoLN3Mcu4wx91ZRe2oUrcuhlKqN3MxxuEloWCslx9UhJjKc5dt06kcpVXu4GapaqnMcvokII7o1YuL8LbRrGM8NA1qEuklKKRV0OsdRSX8f3on9Rwv5x5erEBGu79881E1SSqmgcpMd9zdV0ZCaKiI8jGev6kpxseHvX6wkXODX/TR4KKVOX+WmHBGRNiIyQ0SWO4/PEJE/B79pNUdkeBjPj+rGkI4NeOjzlbw9d2Oom6SUUkHjJlfVa8ADOHXBjTHLsLU1lJfI8DBeGNWdQR0a8JdPV/DuvE2hblL1tuIT+OaRULdCKVUBbgJHrDFmfoljhcFoTE1XJyKMl67pzvnt0njw4+VMnL851E2qvha9AfPGlfsypVT14yZw7BKRljj1wkXkV8D2oLaqBqsTEcbLo7szsG0qD3z0M+8v2BLqJgVWcRG8cwWsnVq56+Suhvz9cFSXMitV07gJHLcCrwLtRGQrcBc2Y265RGSoiKwRkSwRud/H8+1EZK6I5IvIPV7HM0VkpoisEpEVInKn13MPichWEVnq/Fzopi1VKSoinFdG9+DsNqnc99EyPliUHeomBc6udZA1HVZVomLvkb1wwPnb44D+DaJUTeNmVdV64AIRiQPCjDEH3FzY2XX+EjAIyAYWiMhnxpiVXi/bA9wBjChxeiFwtzFmsYjEA4tEZJrXuc8YY550045QiY4MZ/yYHtz41kLu/eAncg4c5ZazWxIWJsF/82OH4eAOOLDDfjF734bXgUueg/DIil1722J7u3Nl2a8rS87qE/f3b4PUthW/llKqyrnZxwGAMeZQ+a86SW8gywk8iMgkYDhw/BvHGJMD5IjIRSXeazvOcJgx5oCIrAIae59bE9jg0ZN7P/iJx79ew7z1e3h6ZBeS60YF/s0KjsDbl0HOSjjqoxx8RDREJ8LBndDzt5DRs2Lvs9UJHLmrobgYwtx0WkvIXXXi/v5tFWuHUipkXAeOCmgMeA/wZwNn+nsREWmGrT8+z+vwbSJyHbAQ2zPZ6+O8m4CbAJo0aeLv2wZMTJ1wXhjVjT4tkvm/L1Zy4fPf8fzV3TizRXJg3yh7AWyeCx0vg4adIT4d4hueuI2uZ3sdT7e3r61o4Ni2xN4WHIa9GyC5pf/XyFkFkbH2Gho4lKpxKvDnomu+xmSMXxcQqQt8CNzlVQvkFaAl0BXbK3nK17nGmPHGmJ7GmJ6pqan+vG3AiQij+zTl49+fRWydCEa99iMvzFhHUbFfH0fZtjhx9eJnYMDd0PUaaHkepLWHmPogAgmNIKExZC+s2HsUHoMdP0PTfvZxTgU7gDmroEFHiE2G/Vsrdg2lVMi42QAYKyJ/EZHXnMetReRiF9fOBjK9HmcArv+8FJFIbNB41zsvljFmpzGmyBhTjN1j0tvtNUOtY6NEPr+9P5d0acRT09YydsJ8cg/kB+biW+ZDajsbJMqS0dP2OCoiZyUU5UOXUYBUfJ4jZ5Vta0IjnRxXqgZy0+P4D5AP9HUeZwP/cHHeAqC1iDQXkTrYTYOfuWmUiAjwb2CVMebpEs+lez28DFju5prVRd2oCJ69qiuPXt6ZBRv3cOHz3/HDL7sqd9HiYhsMMl3E0MY9IW8THKxAyVvPxHiz/pDUHHJW+H+NQ7vg8C7bE4pvpD0OpWogN4GjpTHmcU7sHD+C72GokxhjCoHbgCnAKuB9Y8wKEblFRG4BEJGGIpIN/BH4s4hkO7XN+wFjgPN8LLt9XER+FpFlwEDgD379xtWAiHB17yZ8els/EqIjGP36PP711Sqycg5iTAWGr3Zn2SWumS6mkDJ62dutFRiu2rYEYpKgfjNI61CxHkeOMzGe1t72OHSOQ6kax83k+DERieHEBsCW2B5IuYwxk4HJJY6N87q/AzuEVdIcSglOxpgxbt67JmjXMIHPbuvPXz9dwavfrufVb9fTLDmW89s34Pz2afRqlkRkuIvY7pnfyHDR40jvAmERtofSdph/Dd66BBp1s/MlDTrCmsl2NVdkjPtreAJHantIWASHd0PBUYiM9q8tSqmQcRM4HgK+BjJF5F1sb0Az5gZIXFQET43swh8Ht+GbVTuZviqHt+du4t9zNhAfHcE5bVK5oH0Dzm2bSr3YOr4vsmWendtIblX+G9aJtV/6/s5zHDts5zjaDLGP0zqAKYbcNdCoq/vr5K6yK7ziG9oeB9h5jiTNKKxUTeFmA+BUEVkE9MH2Au40xlRyUF6V1LheDGP6NmNM32Ycyi9kTtYuZqzayTerc/li2XbCw4RB7Rtw37B2NE+JO/nkLfNtb8PtnoqMXvDTf236kLBwd+fs+BlMETR2CkI26Ghvd67wL3DkrLLDVCKQ4ExX7d+mgUOpGqTcwCEiM4wx5wNf+jh2+isqqPgu6wqKi4pgSMeGDOnYkOJiw7Kt+5iyYgdvz93E9Ke/5bq+zbjz/NYkxkbauY1da+CMke7fIKMXLHjd9hYadHB3jmdivJETOJJa2E2F/izJNcYGjo6X2ccJje2tznMoVaOUGjhEJBqIBVJEpD4n5hwSgEZV0LbQm/UYrJsK1091/5d5gIWFCV0z69E1sx6/7decp6et4Y0fNvDRkmzuPL81Y1LW2v+IblZUeTR2Nv9lL/AjcCyxmwk9vYSwcJsqZKcfK6sO7ICjeXaYC04MVenKKqVqlLLGNm4GFgHtgMXO/UXAp9gcVKe/pBZ29dHCCaFuCQCp8VH86/Iz+PKOAXRqlMjDn69k4gcfUCzhmEbd3F8ouaWdZ/BnZdXWxSd6Gx5pHf3rcXhSjaS1s7dR8RCVEPoex6YfTs6fpZQqU6mBwxjznDGmOXCPMaa5108XY8yLVdjG0On8K2h+Dsz4OxzYGerWHNc+PYG3r+/NhF/3pGPxalYUZXLtWytYsc1HjipfROxwldsd5Ef3we51dkWVtwYdbO6rQ7vdXcfz5Zzm1cuJT4cDIQ4cH98MM/4vtG1QqgZxM5u6T0SuK/kT9JZVByJw0VM2p9K0v4S6NScREc5rnUy38PVENO3Dyu37ufiFOdw5aQlZOQfLv0BGTzvf4KYexral9rZxicDhCQBuNwLmrITYFIhLOXEs1Hs5igpgX7bdFKmUcsVN4Ojl9TMAuzz30iC2qXpJaQ397oRl/4UNs0PdmpPlrESOHaR9rwv49p6B3HR2C6au2MmgZ77ljolLyMopIwN+Rk/AnEhaWJaSE+Mex1dWuRyuyl1tV1R5S2gc2sCxL9suK87Tao1KuVVu4DDG3O71cyM2U20pGwpOU2ffA/Wawpd320R/1YVn419mbxJjI3lgWHvm3GcDyPRVOxn0zOzSA0jjHvbWzX6ObUvsbvHYpJOP121gd5K76XEYY4eqTgkcjexwV1GIqhF7ehr5++FIXmjaoFQNU5HsuIeB1oFuSLUWGQMXPgm71sLcF0LdmhO2zIe6DaHeibTxyXWjeGBYe77700BuPrvl8QBy+8QlrNvpFUBi6kNya3fzHJ4d4yV5dpC76XHsy4ZjB2xyQ28Jjexf/AdDNIe012uIat9pVuZXqSBxkx33cxH5zPn5AliDXVlVu7QZDO0uhm+fgL0bQ90aa8s8uwxXTs3Oklw3ivuHtWPOfedxyzkt+WbVTgY/O5tb313Md+tybUr3jF52ZVVZ+bEO7YJ9m08dpvJI62DnSoqLy25rro+JcfBakhui4SrvIao8DRxKueEm5Yh3idZCYJMx5jQqou2HYY/Bi73hq/tg1CSfX9hV5sBOO8zS+6YyX5YUV4f7hrbjxgEteP279bz94ya+/Hk7DROiebhRBkMO5drr1G/m+wKein+NSwkcDTpAwSF7jbJ2f3uW7ab56HGAs5ejV5m/S1DkbYI68bY3pD0OpVxxM8fxrdfP97U2aAAkZsC598Par22Cv1DKnm9vXW78S4qrw5+GtmPBgxfw0jXd6dAogRfX1QPgyQnv8Mb3G9hzyMf8zbYlgNjkiL406GRvy9vPkbPaDquVrBcS6t3jezfZlCkR0TpBrpRLpQYOETkgIvt9/BwQERdrOE9TfX5nh1u+ug+O+VuGPYC2zIPwOqV/oZciOjKci85IZ8KvezHhT7+mMCyaVsfW8NDnK+n9yHRufGshc3/x2pexbbHdIR4V7/uCnjmL8uY5claeOjEONpBERIdu93jeJrvwITFTexxKuVTWBsB4Y0yCj594Y0xCVTayWgmPhIuetl8y3z4WunZsmW8nrCOiKnyJ1MQ4IjK6MyJlG1/dOYDf9GvGks15XPP6j7w0MwtTXOzsGC9jV3pUXTvMVdbKquJiu7DAV+AQcTYBhqASYMEROylfvynUy9Q5DqVccrWqSkS6iMhtzs8ZwW5Utde0L3QdDXNfOlFfoioV5ttNef7kpypNRk/YsYz2qVE8eFEHvvvTQC4+oxFPTFnDn9+aCodySp8Y90grZ2VV3ia7idJX4IDQ7eXwBArtcSjlFzerqu4E3gXSnJ93ReR2NxcXkaEiskZEskTkfh/PtxORuSKSLyL3eB3PFJGZIrJKRFY4bfA8lyQi00RknXNbTpHtIBn0MNSpa/d2VKRqn7cNs2H6w+WvTPLYvszW/nZTuKk8Gb2g6JhNmw7E1Ann+au7cv+wduxeNxeAnfHlJEJs0MFWISwspb6Xd/EmXxJCVELWs4ejXhPb4ziUa3shSqkyuelxXA+caYz5qzHmr9i6HDeWd5KIhGOTIQ4DOgCjRKTkN9Ae4A5OXrkFdvXW3caY9s773ep17v3ADGNMa2CG87jqxaXY4LHp+8olQVz/Lbx7Jcx52u5Od8Nr41+lZXhlynWICLec05IHuhylgHAu/V8eP2SVUYIlrYOt1ZG7xvfznuSGqW19P5/QCPZvdx84A8UTOOo3hURnL8y+2rv2Qym33AQOAYq8HhfhouY40BvIMsasN8YcAyYBw71fYIzJMcYswKln7nV8uzFmsXP/ALZmubP8huHAm879N4ERLtoSHN2ugxbn2l7H/Nf8P3/TDzDxapuFN70LzHjY3YT7lnl2eCW+of/vWVJCIztU5GMHedOjqylO7UBCfDxjJszn33M2+K6J7kk9UtrKqpxVdigoupSpsYTGUFxgy8hWpb2bIDzK2USZaY/pyiqlyuUmcPwHmCciD4nIw8CPwL9dnNcY8B40zubEl79rItIMm+bE+TObBsaY7WADDHb4zNd5N4nIQhFZmJub6+/buhMWZvdztB0Gk++Bmf90P2y1ZYHtaSQ0hus+hWGP2wniH8rZmW6M/ZLPPLPy7ffI6HnqDnJj81hFNenBx7f247x2afz9i5Xc/b+fOFpQdPJrk1raL+DSanPkrD51x7i345UAq3i4Km+TDRhhYTawQWjnOYoKYNK1sPH70LVBKRfc7ON4GltjfI/z8xtjzLMuru2rV+LXZICI1AU+BO4yxvi1BNgYM94Y09MY0zM1NdWfU/0TGQMj34Zuo+0qqy/+YEuylmXbUnjnCohLhbGfQd00aNLHVsb7/rmyJ4r3bbEBJhDDVB6Ne9ov0YM5J47tWW/TqTfqTt2oCF4d3YO7LmjNR4u3MvLVuWzN85oLCI+A1Da+A0dRoa1QWNrEOIRu93je5hPpWuLTQcJDu7Jq81xY/QXMfjx0bVDKBTeT4y2BFcaY54GfgAEiUs/FtbOBTK/HGYDrbwYRicQGjXeNMR95PbVTRNKd16QDOb7Or1LhEXDpi9D/D7DoP/C/sVBw1Pdrd66At0dAdCKM/fzElybABQ9BcWHZtSG2+Lfxz5UMZ8e2d6+jxI7xsDDhrgva8OqYHvySc5Chz87m06VePYTSijrt3WAn38sMHJ5NgFXc49jr7OEA+98woXFoh6rWTrG362fBng2ha4dS5XAzVPUhUCQirYDXgebAey7OWwC0FpHmIlIHuBr4zE2jRESww2GrnB6Pt8+Asc79sVSXvFki9ot/yL9g1efw7q/sX+zectfAm5dCRIztadTLPPn5+s2gz+/hp4knvrhL2jIPIuPsF3WgpHeBsIiTKwJuW2I35pVYCTWkY0O+vGMArdLqcuekpdw+cQn7DhfYlVUHtsPhPSdf+/iKqjKGquJS7ftX5V6O/ANwZI+dGPeoF+IluWunQMPOIGGw5O3QtUOpcrgJHMXGmELgcuA5Y8wfgPTyTnLOuQ2Ygp3cft8Ys0JEbhGRWwBEpKGIZAN/BP4sItkikgD0A8YA54nIUufnQufSjwKDRGQdMMh5XH30/T1c/poddnjjohOVA3f/YoOGhNmeRml5nQbcbYsdTXnQ93zJlnmQ0cP+hRwodWLtBLf3BPm2xdDwDJ/v0ywljv/d3Je7B7Xhq5+3M+TZ2awoyrBPlux15KwCpPQVVWDrl9dtWLVDVZ6ehVdmYRJDuAlw9y+2ymK3MdB6MCx5N3Sp5pUqh5vAUSAio4DrgC+cY5FuLm6MmWyMaWOMaWmMecQ5Ns4YM865v8MYk+HsSK/n3N9vjJljjBFjzBnGmK7Oz2TnnN3GmPONMa2d2z1ltSEkzhgJo/5rvwwmDLZDD29ealcOjf0MUlqVfm50Apz3IGz+wfZcvB07BDuWB2b/RkkZvWwvp7jIfmFt/6n0xIZARHgYt5/fmo9+fxaxUeH8dvJhAAq2LT/5hbmr7F/1deLKfv+q3svhSader9mJY/UybRnbogKfpwSVZ5iq9WDoPhYO7oB1U6q+HUq54CZw/AboCzxijNkgIs2Bd4LbrNNA6wtsz+LofnhrOBw7aFdPlTXW79HtOrs3YtpfTt5Ut3Wx3S8RyBVVHhm9bBtz19jJ7ILD5e8YB87IqMeXtw9gSJ+u5Jk4psz85uTa5zmrTk2l7ktVl5D13sPhkZhpa4OEYhf7uimQ0tb2RFsPtpP1i94s/zylQsDNqqqVwD3AChHpDGw1xlSv4aHqKqMn/HYKtL8Exnxsx6/dCI+Awf+wdT/mjz9x3LPxz7NpL5Aae20E9JSTLaPH4S2mTjj/N6IzpHWgSeFGRrz0PS/NzCInb7/dUV7W/IaHJ+1IZXfhu7V3E0TGQmzyiWP1QrQkN/+AXYLbZoh9HB4BXa+FrGmwL0TJH5Uqg5tVVRcBvwDPAy8CWSIyLNgNO22ktoGr3nH9JXxcq/PtX57fPmGLKYFdUZXS9tQSroGQ3BKi69nAsXUxRCXY/Rl+qNesK50jt3JBuzSemLKGax+bCMWFfJQdz/SVOzlwtIwhoIR028spuaAgWPI22xVV3jVVPLvHq3qe45eZdhizzdATx7qPsb2fJdq5V9WPmxnWp4CBxpgsOL4890vgq2A2TGF7HS/3hVmPwoVP2Boc7S4OznuJOPMci2zG3fQudmOcPxp0RI4d5OWLU1hxXmu2ff8urIQ3smJZtnoh4WFCl4xE+rVKoV+rFLo3qU+dCOc9vPdyxNQL6K/mU96mk4epwNZbgarvcaydYpdnew9B1m8GLQba1VVn32MXEChVTbj5ZsjxBA3HeqrD3onaILUt9PytzYW1+gs4sjew+zdKyuhp5yR2LPe/hwTHU49Izio6NU5kcMpekDDef3AM7914Jr87pyXFBl6amcXV43/knCdmMm+9k2akKgs6GXPyHg6PyGio26Bq93IUF8O6qdDqglNXsPUYa4PYL99UXXuUcqGsQk6Xi8jl2LmNySLyaxEZC3yO3aOhqsK5D9gsvJ/83j4OxsS4R0ZPwNhhExcT46fwTPx7dpDnrISkFkTHxHFWyxTuGdKWT27tx5K/Dmbc6O5ER4Yz6rUfeWbaWgrjnLxbVbGy6sheWyrWeymuR1WnV9++xKaubz3k1OfaXmSXZi96o+rao5QLZfU4LnF+ooGdwDnAuUAuEJpU5rVRXDKccy/k77dzEMmtg/dejXt43a9A4IiKt1/Gnr0cuat9riJLjIlkaKd0Pr+9PyO6Nea5GesY8/5mDFI1mwB9rajyqOqCTmun2r09rS449bmIOtB1lC1V7NkPpFQ1UOochzHmN1XZEFWG3jfZvzrT2vs/7+CPmPo2MB3ZcyLpn788RZ0Kjtp8Vx0vL/WldaMieHpkVwa0TuHPHy9nd1gCRzdlkVHB5rt2fA+Hj8CRmAmrJ9shpGB+1h5rv7ZzS3HJvp/vPtYmvlz6Lgz4Y/Dbo5QL5U6Oi0g0tiZHR2zvAwBjzG+D2C7lLSIKbvzGpuUItn532t6N92ojfzToYJeR7lxhVwW52LdyWbcMumXWZ+8rqWzNWsurnyznwYvaEx0ZpAlhX7vGPeo1sUWyDuVCfIPgvL/HgR2wfSmc/9fSX5PSGpr2g8VvQb+7qiaYKVUON/8K3wYaAkOAb7HJCg8Es1HKh+jE8ndfB0L3MdD31oqfn9bBJmpc5aQQc7PhEZvGpGXLNnSoe4C3f9zEiJe+Z93OIP0zy9tkP09fq7eqMr36uqn21tf8hrfuY22yyI3fBb9NSrngJnC0Msb8BThkjHkTuAhwuZNN1Tqeok7LP7I9JD/2goQlNibN7OE/v+lF7oF8LnlxzskZeAPF14oqj+MFnTYF/n1LWjsFEjJOfGal6XCpDXSLdSe5qh5c5apybvNEpBOQCDQLWotUzZbcCsIi7V/sya3tBK9bCelwNI+BzeP46s4BnJFRjzsnLeWN7wOcYtzXHg4PT48j2BPkhfl241+bweUPC0bGwBlX29xlh6q4SqJSPrgJHONFpD7wZ2xK85XAY0Ftlaq5wiNPZMJNc5FqxNvxvRzbSUuI5q3f9mZwhwY89PlKnp2+1nfZWn8Zc2LXuC/RCfav+2APVW2cAwWHTt4tXpYeY21dk2WTgtsupVxwk6vqdWPMXmPMbGNMC2NMmjHm1aponKqhPEkN3SQ39HZ897gdnoqODOfla7vzqx4ZPDt9HQ9/vpLi4koGj4M5UHi09MABNvVIsHsc66baeifNBrh7fYOONp/YojerLp+XUqXQJRoq8Bo4AcNNckNvPnaPR4SH8fgVZ3BD/+a88cNG7v7fTxQUFVe8bWXt4fAIdkEnY+wy3Obn2FoobvUYazMXe5JdKhUiGjhU4LUYaCd9/U2PEu/UBztwctqRsDDhwYvac++Qtny8ZCu/e2cRRwvKqetemrKW4np4CjoF6y/7Xets5uM2g/07r+PlUCde062rkAtq4BCRoSKyRkSyROR+H8+3E5G5IpIvIveUeG6CiOSIyPISxx8Ska0+KgOq6qJRV/jjCohv6N95dWLt7ngf+apEhFsHtuLvIzoxY3UO102Yz/6ysu2WZu9Ge1tW4KiXaVOSHM3z//purP3a3pa3DLekqLrQ+Vew4uOqyyKslA+uAoeInCUi14jIdZ4fF+eEAy8Bw4AOwCgRKTnovQe4A3jSxyXeAEqbOXymZGVAdZrw1OUoxZg+TXn2qq4s3rSXUeN/ZNfB/FJf61PeJlvjvKw9McFeWbVuqt1hX7LmvBtdr4XCI3Z3u1Ih4qYex9vYL/b+QC/nx00lod5AljFmvTHmGDAJGO79AmNMjjFmASeW/Ho/NxsbWFRt4qKE7PCujXntup78knuQK8fN5f2FW8g94DKA5G0uu7cBwS3odCQPNv1womiTvzJ62sC24uOANkspf7jJYdET6GD8XwvZGPD+Py8bCFRq19ucXs9C4G5jzN6SLxCRm4CbAJo0KeeLQlUfCY1svfNyDGyXxtvXn8ldk5bypw+WAdAlsx7nt0vjvHZpdGyUgPjaH7F3EzTqVvbFg1nQ6ZcZtvxvRQOHCHQcAT+Os1l+YzTfqKp6boaqlmNTjvjL166mQMw2vgK0BLoC27GFpk59I2PGG2N6GmN6pqamBuBtVZVIaGTTjBceK/elvZolMee+gXx5R3/uHtQGAZ6ZvpaLX5hD3399wwMf/cz0lTs5csyZSC8ugn3ZZa+oAohLgYiY4PQ41k61X/YZvSp+jY6X2dT3OlylQsRNjyMFWCki84Hj4wHGmEvLOS8b8B7EzQAqXaXHGHM8v7SIvAZ8UdlrqmrEs5fjwPbyv+Cxk+YdGyXSsVEit5/fmtwD+cxak8PMNTl8/tM2Js7fTHxUBL/p35wbOtchobig7D0c9qJOevUAF3QqLnKKNg2qXEW/Rt3tcNuKj6HbtYFrn1IuuQkcD1Xw2guA1iLSHNgKXA1cU8FrHSci6cYYT9GGy7A9InW68C4h6yJwlJQaH8WVPTO5smcmxwqLmb9hD+/O28TzM9axbM5a3hA4GNOIuuVdKBgFnbYusinrKzpM5SFiex1zX4LDe4JTg16pMpQbOIwx31bkwsaYQhG5DZgChAMTjDErROQW5/lxItIQO0+RABSLyF3Y+ZT9IjIRWzgqRUSygb8ZY/4NPC4iXbHDXhuBmyvSPlVNHd8EWPnkhnUiwujfOoX+rVNYtX0/8z/+CXJg5PvbOb/fGq7v35x6saXk0qqX6WquxS9rvwYJh1bnV/5aHS+D75+D1V/ajMZKVSE39Tj6AC8A7YE62CBwyBiTUN65zlLZySWOjfO6vwN81+0xxowq5bj+X3I6O74JMLCVANunJ9C+vWByhFat2vLCN1n85/uN/PqsZtwwwEcAScyEw7vg2KHApLMvKoCf/wdNzwrMhHZ6V6jfzA5XaeBQVczN5PiLwChgHRAD3OAcUyrwohMhMq7MvRwVlrcZiU/n+TF9+fquAZzTJpUXZ2bR/7GZvDQzi0LvVCaeJbv7sgPz3sv+a+dMzro9MNfzDFetn2WHq5SqQq42ABpjsoBwY0yRMeY/2CEkpQJPxNVejgrxSqfermECL13bnSl3nc1ZLZN5YsoarnjlB7JynOJRgdwEWFQIs5+E9C7Q2s80I2XpeJld2rvq88BdUykX3ASOwyJSB1gqIo+LyB+AKihFp2qthEbB6XH4KODUtmE846/ryYvXdGPznsNc+PwcXpu9nqIEZwR1XwBWVv38P1vB75z7Kl6S15eGZ0BSC90MqKqcm8AxxnndbcAh7BLbK4LZKFXLJTSG/YGd46DwmE2eWMqu8YvPaMTUP5zDOW1SeWTyKkZN3IgJi6h8j6O4CGY/AQ06Q9sAp1XzDFdtmA2HdgX22kqVwU09jk3YzXzpxpiHjTF/dIaulAqOhHQ7OV5cwQy4vuzPBlNc5hLf1Pgoxo/pwdMju7Aq5zBbi+qzPmt15WqALP8I9vwC59wb2N6Ghw5XqRBwk6vqEmAp8LXzuKuIfBbkdqnaLKGR/TI8mBO4a+516nCUs/lPRLi8ewbT/nAOB6LT2b0ti9H/nkf23sP+v6ent5HWAdpdUoFGu9Cgky3Xq8NVqgq5Gap6CJuwMA/AGLMUrTmugslHQadK8+wCd7mpsGFiNO3adaBj7D5+2pLH0Ge/45+TV7E+96D791z5qS28dPa9EBakCgae4aqN3+lwlaoybv41FxpjNPm/qjrH044EMnBsspvv4hu5PkUSmxCbn8uUO/pydpsUJszZwHlPfctVr87lkyVbyy4mVVxsexspbaHD8NJfFwgdL7PDcKt0IEBVDTcpR5aLyDVAuIi0xtbP+CG4zVK1WrxX2pFA2bsJEjMg3M0/eUe9TDDFZITv5eVre5Bz4CgfLMpm0vwt3PXfpdT7PJLLujVmVO8mtGkQf/K5q7+AnJVw+euVy0vlRloHSGljh6t6/ja476UU7noctwMdsQkOJwL7gbuC2CZV28UmQ3idwO7l8NrD4VqJvRxp8dH8/txWzLrnXN694Uz6tUrhnR83MfiZ2Vzxyg98smSrnUg3Br593M49dLo8cL9DaY4PV80J7LyQUqVwk6vqMPCg86NU8IWF2dQjgZ7jaD3Iv3OO7x4/eUluWJjQr1UK/VqlsPtgPh8t3srE+Zu5679L+WBRNs9120byzp9hxLjg9zY8Ol4G3z5mh6t63VA176lqrVIDR3krp1ykVVeq4sopIeuXgiNwcCfUa+Z/G6DMvRzJdaO48ewW3DCgORPnb+EfX65g55aHiYrLIK7zr3wWpQmKtPaQ2g5WfKKBQwVdWUNVfbEJCL/Dlo59qsSPUsETyN3jfq6oOi4yGuo2cLV7XES45swmzLq0gA6ygYf3XciN7ywl58DRCjS4gjzDVQd2lv4aY2DNVzD+XPhpUpU1TZ1eygocDYH/B3QCngMGAbuMMd9WNNW6Uq4lOENVflcs9sETOMor4ORLvSbuCzoZQ9qSZzGJmbQbfAOz1+1i8DOz+WJZENKn+NJhBGBKX12VswrevgwmXg3blsBczVWqKqbUwOEkNPzaGDMW6ANkAbNEJEDpPZUqQ0JjKMovP/NrUWH519q70d6Wkm6kTImZ7tOO/DIDti5CBtzN9ee0ZfId/WmaFMtt7y3h9olL2Huo/HK4lZLWzq6wKrkZ8PAemHwvvNIPti2GoY/BoL/Djp9hlyaBUP4rc1WViESJyOXAO8CtwPPAR1XRMFXLHa8E6GNlVeExu7nu7cvhkYbwwwtl90zyNkF4lB128le9TNuG4uKyX2cMzHoMEjKgqy3n2iotng9/dxZ3D2rDVz9vZ/Czs5myYgcmEL2o0nS8DDb9YHN9FRXC/Nfghe6w4HXo8Wu4fQn0uQU6/woQWKH/Oyv/lRo4RORN7H6N7sDDxphexpi/G2Ncr5EUkaEiskZEskTkfh/PtxORuSKSLyL3lHhugojkiMjyEseTRGSaiKxzbgNQFUdVO56Jae+CTruyYOpf4JkO8P51kLsaMs+EqX+Gr+4rPbdV3mbb26jI7u3ETCg6ZifXy7JuGmTPhwF/gIgTRaEiwsO4/fzWfHpbP5Lj6nDz24u4bsJ81u484H9b3PAMV33zDxjXHybfAw07wy1z4OKnIS7Zvi6hETTpa3NpKeWnsv5PGgO0Ae4EfhCR/c7PARHZX96FRSQceAkYBnQARolIhxIv24PdUPikj0u8AQz1cfx+YIYxpjUww3msTjeeSoC7f4Fl78N/LoIXe9g625lnwjX/g7t+hrGfQ9/bYP6rNpgc85FTau+mig1TQalLck+Ssxo+usHuEu862udLOjZK5PPb+/O3Szrw05Y8hj33HQ99toK8wwEevkptY/NXLX0HCg7DVe/CdZ9Bg46nvrbT5ZC7ys59KOWHsuY4wowx8c5PgtdPvJuysdj8VlnGmPXGmGPAJOCk3AvGmBxjzAKgwMf7z8YGlpKGA286998ERrhoi6pp6jYACYMpD8BHN9rhovP/Bn9cCVe/C20G2z0SYWEw5BEY9ritv/3WpafmbKrI5j+P45sAS5kg378d3v0VRETD6A/sSqxSRIaH8Zt+zZl170BG9c7krbkbOffJWbw9d+PJ1Qcr68In7c+t86H9xaVn5e0w3H7G2utQfgpS5jUAGgPef6ZlO8cqq4ExZjuAc5vm60UicpOILBSRhbm5uQF4W1WlwiPsXEGnX9m/mG9fDAP+CPENfb/+zJvhqrfthO+/B9meCsDR/XBkb8VWVIGd4wDfPY78A/Delfb61/7Pda8mKa4O/xjRmS/vGED7hgn85dMVXPT8HL7PClCSwqZ9ofeNZQYxAOqmQbP+dp4jmPMu6rQTzMDh68+cKvvXaYwZb4zpaYzpmZqaWlVvqwJp+Ivwq39Di3PczU+0v8QOXR3Js8FjywKvpbgVHKqKiofoeqeurCoqsENjO1fCyDdtWVg/tU9P4L0bz2Tc6O4cLijk2tfncdNbC9mypwIp3Cuq4+WwO8sGXH/lbYa1UwLfJlXtBTNwZGOrBXpkAIFY0L5TRNIBnFtNzqNOyOwNN0yHqAR48xKYP94er+hQFdheh3ePwxj4/E745Ru45DlodUGFLy0iDO2UzrQ/nMO9Q9oyJ2sXg575lldm/UJBIIevStP+Ups12N/VVcbAB7+F90bC3JeD0zZVbQUzcCwAWotIc6dm+dVAIPI+fwaMde6PBT4NwDXV6SS5JVw/DRp0gMXOdJi/6Ua8JTY5uccx61+w9F049wHoPqZSTfWIjgzn1oGtmHG3LV/72Nerufj5OSzaVM4+lsqKS4YW59p5Dn+Gq7KmQ/YCW/N8ygOwcELQmqiqn6AFDmNMIbZO+RRgFfC+MWaFiNwiIrcAiEhDEckG/gj8WUSyRSTBeW4iMBdo6xy/3rn0o8AgEVmH3c3+aLB+B1WD1U2FsV9Au4vtMFVsUsWv5elxGAOL37LJBLuNhnPuC1x7HemJMbw6pievXdeTA0cLuOKVuTzw0bLAr77y1ulyu4Bg62J3rzcGZj5iP9db5kDrIfDFH2HpxOC1UVUrEtTNSNVEz549zcKFC0PdDBUqRQUQHlnx8394EaY+aGtrfHyz/Qv9mv9W7pouHMov5Nnpa5nw/UbqxUTyl4s7MLxrIyTQtcuP7IUnWtsFBkMeKf/1a76yaUsufdH2uAqOwsSrYMNsuOLfVZNKXlUJEVlkjOlZ8ngwh6qUqh4q+wXvWVn18U12P8TIN4MeNADioiJ48KIOfH5bfzKTYrnrv0sZ8+/5bNh1KLBvFFMfWp1vU5W42SE/8xGo3xy6XG2PRUbD1e/Z/TUf3QirJwe2fara0cChVHk8ezkSGttlt1HxZb8+wDo0SuDD353F30d04qfsPIY8O5unp60tu3StvzpebvfKZM8v+3Wrv7ArsM657+TgWScOrnnfri7731jImhG4tqlqRwOHUuVp2Bn63ApjPi59H0mQhYcJY/o0Zcbd5zCsU0Oen7GOC57+lukry0mF4lbbYTafV1mbAYuLYea/bGXDzlee+nx0Aoz+EFLbwqRrbYr3QCgqdJ9oUlUJDRxKlSc8Eob+E1Jah7olpMVH89zV3Zh4Yx9iIsO54a2FXP/GAjbvruTej+gEWyFx5Sel5/xa9SnkrIBz7i+9dntMfRjziZ04f+8qu5emsmY+As+dYVPPqGpBA4dSNVDflslMvnMAD17Ynh/X7+aCZ77l2emVHL7qdLlN5rjph1OfKy6CWY/afFzlTX7HpcDYz+zO9HeugO3LKt6mwmOw5G2bGuWjm2DpexW/lgoYDRxK1VCR4WHceHYLZtx9LkM6NuTZ6esY/MxsvlldweGrNkMhMtb3ZsAVH9tsxOfe766OenxDmyqmTqytBVJRa7+GQ7lwxes2g8Anv4dFb5Z/XkVs/wk+uwOO7gvO9U8jGjiUquEaJkbzwqhuvHfDmUSGC799YyG/e2cR+4+ekju0bHXioM0QWPnZyQWyigrtpse0Dk7adpfqZcJZt8OWH+2XckUsedtmSm53CYz6r1399fkdtr5IIB3ZC5NG2w2jH/y29OE6BWjgUOq0cVarFL6682zuHdKWaSt3MvzF7/2v+9HpCji8CzbOPnFs+Qc2n9W5D/hf06TrNbYXM/81/84D2LfV7lDveo2dU/Es+20zDL68G34c5/81fTEGPrsdDmyD3jfb95z218Bc+zSlgUOp00idiDBuHdiK927sw4GjhYx46Xv/ap63GgR14k+srioqtHMbDTvbXfj+iqkPZ4yEn/9Xfhngkpa+B6bY7tL3iIiCkW/Ztnx9n63+WFkLXodVn9u0/Rc+Dr1utPXYdT6lVBo4lDoN9W6exJd39Kd9egK3vbeEf3yx0l3Nj8hoaHeh/SItPAbLJsHeDXDu/6tYBUWwX8SFR2HJO+7PKS62w1TNBth8WN4i6sCVb9hhs6l/hu+erli7wE7cT3nQBsy+t9ljQ/8Fzc+2iSy3lLOvpZbSwKHUaapBQjQTb+zD2L5NeX3OBq59fR65B/LLP7Hj5XA0D7Km2bxcjbrZfR4V1bATNO0HC15zP3ew8TubP6v7WN/Ph0fa9Cadr4QZD9t67/7KPwgf/MbmMbts3InAGB4JV75pN3xOuhb2Zft/7dOcBg6lTmN1IsJ4eHgnnrmqCz9l53HJC3NYvHlv2Se1PA+iE+Hzu2zNjYEPll5F0K3eN9lrrZvq7vWL37JtaF/G8Fh4BFz2KnQZBbP+CV/db/NmufXl3bBnvV2xFZdy8nOxSTBqEhQcgUnX+C5JHCzGwOwn4N0rYduSqntfP2jgUKoWuKxbBh/9rh+REcJVr87lnR83UWqC04g6dhXToRzI6FWpeiPHtbsI4hudqI9SliN77VBZ55EQGVP2a8PCYfjLdlJ73ivw6gB3mw6XvmeH4c6531ZB9CWtnS0ktn0ZfPr7qqmSaAx88w/7s2E2jD8XPr7FLhSoRjQ7rlK1SN7hY9z136XMWpNL/dhIUupGkVy3Dil1o+z9uDqkxEfR6vBSen47FkZ/jLQ8NzBv/u0TMPMfcNvCsnfhzxsPX90LN38H6We4v37WdPjsTrs6qu+ttqfkK/DkrrFfyI17wHWflr8vZc6zMP1vMPDPcE4l9qSUxxM0vnsSevwaLngY5jwDP75si231uwP63WmXTVeR0rLjauBQqpYpLjZMXLCZVdv3s/vgMXYdzGf3wWPkHsznwNET+zcSOETbZhk8eFEHumbWq/wbH8yBpztAr+thWClzEsbAuAF2vuHm2b5fU5aj++1S2kX/sTm1hr8MTc488XzBEXjtfLtD/pY5kJBe/jWNsX/1L5sEV71jSxQHWsmgcdEzJ+Zc9m6E6Q/bjZl1G8L5f7XDcxVdrOAHDRwaOJQqV35hEXsOHWPXgWMs2bKX52esY9fBY1zapRF/GtqWjPqxlXuDD2+09TzuXuU7y/C2JbY3cNFT0OuGir/PLzPtLvB9W6DP7+G8P9td7J/fZYPKtR9Caz+G4AqOwhsXQs5quH6KXZ4cKGUFDW+b59lqi1sXQcMzYMg/ofmAwLXDh5DU4xCRoSKyRkSyROR+H8+3E5G5IpIvIve4OVdEHhKRrSKy1Pm5MJi/g1K1SVREOOmJMXTOSOS6vs2Yde9AbhvYiikrdnDeU9/y6Fer/d+R7q33TXDsAPw0yffzi9+GiGjo9KuKvwdAy4Hw+x9s7+bHl2BcP5j5Txs0+t3pX9CAE5sPoxPg9QtsxcPdv1SujeA+aIDtOV0/3a4mO7IX3rwY/j3E7kM5tLvybfFD0HocIhIOrMWWd83G1iAfZYxZ6fWaNKApMALYa4x5srxzReQh4KDntW5oj0OpytmWd4Qnp6zhoyVbSYqrwx8uaM3VvZsQGe7n357G2B5FwRG4dd7Jq7WOHYan2tqlv5e7mER3a8Ns+PQ2u7w3oxf85quKF+LauxG+e8oGvqICO+l/1h0nD4e55U/QKKngiN2Nv+Qd2LUGwiKg5fl2eXK7CwM2DxKKHkdvIMsYs94YcwyYBAz3foExJscYswAo+SdMuecqpapOo3oxPH1VV764vT9tGtTlL5+uYOizs5myYgfFxX788SliS9TuWmO/0L2t+gzy90P36wLb+OZnw+9+gGGP2zmKylRvrN8MLn0B7loOA+62NUcmDIbXB9kcX273qVQmaICd9O93hw2+N39nh+N2LoePboAnWsGHN8DaKTa4BUEwexy/AoYaY25wHo8BzjTG3ObjtQ/h1Yso61zntb8G9gMLgbuNMacsTBeRm4CbAJo0adJj06ZNAf8dlaqNjDFMX5XDvyavYv2uQ7RMjePGAS0Y0a0x0ZEuMucWHIWn20PTs+Dqd08c/89FdkXU7Ysrv2+kqhw7BEvetSlK8jbZkrp9b4X0rjYVvIhdtSVhzo9z/6eJMOfpigWN0hQXw+YfbHqXFZ/YTZwxSXZJccvzKnTJ0nocpVRjCQhf/+XdRqmyzn0F+Lvz+O/AU8BvT3mxMeOB8WCHqly+r1KqHCLCoA4NOLdtKpN/3s742eu5/6OfeXLqGsb2bcboPk2pH1en9AtERkOPsfD9c3ZTYL0mdr5g0xy7YqimBA2wQ0Jn3mTnUlZ9Dj88D5PvKf88CGzQAHudZv3tz7An4JcZtvhVSpvAXN9LMANHNpDp9TgDcJttrdRzjTHHiw2IyGvAF5VrplKqIiLDwxjetTGXdmnE3F92M/679Tw1bS0vz/qFkT0zuL5/C5okl7IKq+dvbeBYOAEueMgp1hQOXa6p0t8hYMLCoeMI6DDcppA/tAtMkU3SaIrtEJbnvimGqAS7sTJYS2oj6ti5osqkiinr8kG5qrUAaC0izYGtwNWA238VpZ4rIunGmO3O6y4Dlge01Uopv4gIZ7VK4axWKazZcYDXv1vPe/M38/aPmxjaqSHnt2tAw8RoGiRE0SAhmrpREUi9JtD2QluUacA9did368Hu9lVUZyLQqGuoWxF0Qd3H4SyVfRYIByYYYx4RkVsAjDHjRKQhdp4iASgGDgIdjDH7fZ3rXPNtoCt2qGojcLNXIPFJV1UpVbV27j/KGz9s5J0fN520qRAgtk44DRKiOT9qJX/e/QCr6w+k3d6ZLBswjtjOF9O4XiwxdVzMlaig0w2AGjiUqnL5hUVsyzvKjn1HyTlwlJ37j7JjXz47DxwlZ98Rnsi5iWYmmxxTj775L1CEDRgpdaPITIohs34smUkxdG6cyJCODZGaNP9xGgjF5LhSqpaLigineUoczVNK2Vcw/y6YfA9xvUfzfqf+bNlzhC17DpO99whb9h5myZa9fPnzdoqKDRedkc5jV5xB3Sj92go1/S+glAqdrtfC3o3EnXUHPeKT6NH01JcUFhXz+pwNPP71alZt38+40T1o08BHuhJVZTStulIqdOrEwpBHIL5BqS+JCA/jlnNa8t6Nfdh/pJDhL37Px0u0uFIoaeBQStUIfVokM/mO/nTOSOQP//2J//fxzxwtcLlTWwWUBg6lVI2RlhDNezecyc3ntOC9eZu5ctxctuypwup8CtDAoZSqYSLCw3hgWHvGj+nBxt2HuPiFOXyzemf5J6qA0clxpVSNNLhjQ75oGM/v3lnMb99YyAXt02iRWpfM+jFkJsXSJCmWxvVjiIrQPSGBpoFDKVVjNU2O46Pfn8XjX6/h27U5zF63i2OFxcefF4GGCdHOfpBYejStz4DWKWQmVbIgVS2nGwCVUqeN4mJD7sF8Nu85zJY9h51buzdk/a5D7DqYD0DT5Fj6t0phQOtU+rZMJjGm9FTr+YVF9vzcQ2zcfYiIsDB6N0+ifXoC4WGn94ZE3QColDrthYUJDRKiaZAQTa9mSSc9Z4zhl9xDfLculznrdvHJkq28O28zYQJdMusxoFUKHRsnsj3vCBt2HWLD7sNs2HWQrXuP4KvkSHx0BL2bJXFmiyTObJ5Mx0YJRPhb2KqG0h6HUqpWKigqZsnmPOasy+W7rF38tCXveICoGxVB85Q4mjm73lt47ifHcbigkHnr9zBvw27mrd/D+l2Hjp/To2l9zmyRxNmtU+nYKKHGp0jRXFUaOJRSZdh3pID1uQdpXD+G1LpRrr/0c/YfZd4GG0h+XL+HrJyDAKTFR3Fu21TOa5dGv1YpxEe7rzx4rLCYYmPcFcYKIg0cGjiUUlUg90A+s9fm8s2aHGavzeXA0UIiw4VezZIY2DaNge3SaJkaR0GRYcvew2zcdYgNuw6xafdhNu6297flHSEiLIy+LZO5oEMDBrW3qemrmgYODRxKqSpWUFTM4k17+WZNDrNW57Jm5wEAkuLqkHf42ElzJ/HRzvBYsh0WO5xfyPRVO9m4225w7Nw4kQvaN2BQhwa0T4+vkmEwDRwaOJRSIZa99zCz1uSyLDuPhgnRNEuJo2mynUepHxt5SjCwE/oHmbYyh2krd7BkSx7GQON6MVzQPo326QkkxkSSGBtJvZg6zm0ksXXCAxJYNHBo4FBK1XC5B/KZuTqHaat28t26XI4WFPt8XUSYUC82ksSYSP55WWfObJFcofcLyXJcERkKPIet4ve6MebREs+3A/4DdAceNMY8Wd65IpIE/Bdohq0AONIYszeYv4dSSlUHqfFRjOyVychemeQXFrHn0DHyDheQd7iAfUcK2Hfk2PH7eUcK2He4gIQy9qhUVNACh4iEAy8Bg4BsYIGIfGaMWen1sj3AHcAIP869H5hhjHlURO53Ht8XrN9DKaWqo6iIcNITY0hPjKny9w7mbpXeQJYxZr0x5hgwCRju/QJjTI4xZgFQ4Me5w4E3nftvUiLoKKWUCq5gBo7GwBavx9nOscqe28AYsx3AuU3zdQERuUlEForIwtzcXL8arpRSqnTBDBy+pvTdzsRX5lz7YmPGG2N6GmN6pqam+nOqUkqpMgQzcGQDmV6PM4BtATh3p4ikAzi3OZVsp1JKKT8EM3AsAFqLSHMRqQNcDXwWgHM/A8Y698cCnwawzUoppcoRtFVVxphCEbkNmIJdUjvBGLNCRG5xnh8nIg2BhUACUCwidwEdjDH7fZ3rXPpR4H0RuR7YDFwZrN9BKaXUqXQDoFJKKZ9K2wBYO5LHK6WUCpha0eMQkVxgUwVPTwF2BbA5wVaT2luT2go1q701qa1Qs9pbk9oKlWtvU2PMKctSa0XgqAwRWeirq1Zd1aT21qS2Qs1qb01qK9Ss9taktkJw2qtDVUoppfyigUMppZRfNHCUb3yoG+CnmtTemtRWqFntrUlthZrV3prUVghCe3WOQymllF+0x6GUUsovGjiUUkr5RQNHGURkqIisEZEsp2hUtSUiG0XkZxFZKiLVbpu8iEwQkRwRWe51LElEponIOue2fijb6FFKWx8Ska3O57tURC4MZRs9RCRTRGaKyCoRWSEidzrHq+tnW1p7q93nKyLRIjJfRH5y2vqwc7y6fraltTfgn63OcZTCqUK4Fq8qhMCoEhUMqw0R2Qj0NMZUy41JInI2cBB4yxjTyTn2OLDHq5pjfWNMyKs5ltLWh4CD3uWNqwMnQ3S6MWaxiMQDi7DFzX5N9fxsS2vvSKrZ5ysiAsQZYw6KSCQwB7gTuJzq+dmW1t6hBPiz1R5H6cqtYKjcM8bMxpYK9lYtqzmW0tZqyRiz3Riz2Ll/AFiFLXpWXT/b0tpb7RjroPMw0vkxVN/PtrT2BpwGjtJVpoJhKBhgqogsEpGbQt0Yl1xVc6xGbhORZc5QVrUYnvAmIs2AbsA8asBnW6K9UA0/XxEJF5Gl2Lo/04wx1fqzLaW9EODPVgNH6SpdhbCK9TPGdAeGAbc6wy0qcF4BWgJdge3AUyFtTQkiUhf4ELjLGLM/1O0pj4/2VsvP1xhTZIzpii0m11tEOoW4SWUqpb0B/2w1cJSuMhUMq5wxZptzmwN8jB1qq+5qTDVHY8xO53/KYuA1qtHn64xnfwi8a4z5yDlcbT9bX+2tzp8vgDEmD5iFnS+otp+th3d7g/HZauAoXWUqGFYpEYlzJhoRkThgMLC87LOqhRpTzdHzReG4jGry+ToTov8GVhljnvZ6qlp+tqW1tzp+viKSKiL1nPsxwAXAaqrvZ+uzvcH4bHVVVRmcZWvPcqIK4SOhbZFvItIC28sAW9XxverWVhGZCJyLTfG8E/gb8AnwPtAEp5qjMSbkk9KltPVcbFffABuBmz3j3KEkIv2B74CfgWLn8P/DzhtUx8+2tPaOopp9viJyBnbyOxz7R/b7xpj/E5FkqudnW1p73ybAn60GDqWUUn7RoSqllFJ+0cChlFLKLxo4lFJK+UUDh1JKKb9o4FBKKeUXDRxKVYKIFHllHV0qAcyiLCLNxCtDr1LVRUSoG6BUDXfESfGgVK2hPQ6lgkBsfZTHnPoI80WklXO8qYjMcBLOzRCRJs7xBiLysVNL4ScROcu5VLiIvObUV5jq7AhGRO4QkZXOdSaF6NdUtZQGDqUqJ6bEUNVVXs/tN8b0Bl7EZiDAuf+WMeYM4F3geef488C3xpguQHdghXO8NfCSMaYjkAdc4Ry/H+jmXOeW4PxqSvmmO8eVqgQROWiMqevj+EbgPGPMeiep3w5jTLKI7MIWMipwjm83xqSISC6QYYzJ97pGM2xq7NbO4/uASGPMP0Tka2yxqU+AT7zqMCgVdNrjUCp4TCn3S3uNL/le94s4MS95EfAS0ANYJCI6X6mqjAYOpYLnKq/buc79H7CZlgGuxZb3BJgB/A6OF+NJKO2iIhIGZBpjZgJ/AuoBp/R6lAoW/StFqcqJcSqueXxtjPEsyY0SkXnYP9BGOcfuACaIyL1ALvAb5/idwHgRuR7bs/gdtuiOL+HAOyKSiC049oxTf0GpKqFzHEoFgTPH0dMYsyvUbVEq0HSoSimllF+0x6GUUsov2uNQSinlFw0cSiml/KKBQymllF80cCillPKLBg6llFJ++f9vz7Sc/3MX+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 7766.654296875\n",
      "MAPE: 0.19001265911881937\n",
      "MAE: 5197.608044454319\n",
      "R2 score: 0.6506771483448042\n",
      " \n",
      " \n",
      "---------------------------------------------------\n",
      "Train on 34815 samples, validate on 570 samples\n",
      "Epoch 1/100\n",
      "34815/34815 [==============================] - 5s 150us/sample - loss: 0.1414 - val_loss: 0.1468\n",
      "Epoch 2/100\n",
      "34815/34815 [==============================] - 2s 54us/sample - loss: 0.1263 - val_loss: 0.1346\n",
      "Epoch 3/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.1245 - val_loss: 0.1428\n",
      "Epoch 4/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.1230 - val_loss: 0.1333\n",
      "Epoch 5/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1228 - val_loss: 0.1378\n",
      "Epoch 6/100\n",
      "34815/34815 [==============================] - 2s 51us/sample - loss: 0.1219 - val_loss: 0.1307\n",
      "Epoch 7/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1210 - val_loss: 0.1324\n",
      "Epoch 8/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1204 - val_loss: 0.1302\n",
      "Epoch 9/100\n",
      "34815/34815 [==============================] - 2s 55us/sample - loss: 0.1197 - val_loss: 0.1347\n",
      "Epoch 10/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1187 - val_loss: 0.1458\n",
      "Epoch 11/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1177 - val_loss: 0.1318\n",
      "Epoch 12/100\n",
      "34815/34815 [==============================] - 2s 59us/sample - loss: 0.1171 - val_loss: 0.1321\n",
      "Epoch 13/100\n",
      "34815/34815 [==============================] - 2s 56us/sample - loss: 0.1164 - val_loss: 0.1338\n",
      "Epoch 14/100\n",
      "34815/34815 [==============================] - 2s 51us/sample - loss: 0.1152 - val_loss: 0.1300\n",
      "Epoch 15/100\n",
      "34815/34815 [==============================] - 2s 50us/sample - loss: 0.1145 - val_loss: 0.1342\n",
      "Epoch 16/100\n",
      "34815/34815 [==============================] - 2s 55us/sample - loss: 0.1132 - val_loss: 0.1308\n",
      "Epoch 17/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1113 - val_loss: 0.1273\n",
      "Epoch 18/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.1102 - val_loss: 0.1242\n",
      "Epoch 19/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1092 - val_loss: 0.1272\n",
      "Epoch 20/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.1081 - val_loss: 0.1296\n",
      "Epoch 21/100\n",
      "34815/34815 [==============================] - 2s 58us/sample - loss: 0.1067 - val_loss: 0.1210\n",
      "Epoch 22/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1057 - val_loss: 0.1238\n",
      "Epoch 23/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.1051 - val_loss: 0.1175\n",
      "Epoch 24/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1042 - val_loss: 0.1216\n",
      "Epoch 25/100\n",
      "34815/34815 [==============================] - 2s 55us/sample - loss: 0.1033 - val_loss: 0.1242\n",
      "Epoch 26/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.1031 - val_loss: 0.1181\n",
      "Epoch 27/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1024 - val_loss: 0.1188\n",
      "Epoch 28/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.1020 - val_loss: 0.1176\n",
      "Epoch 29/100\n",
      "34815/34815 [==============================] - 2s 54us/sample - loss: 0.1018 - val_loss: 0.1180\n",
      "Epoch 30/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.1014 - val_loss: 0.1187\n",
      "Epoch 31/100\n",
      "34815/34815 [==============================] - 2s 51us/sample - loss: 0.1010 - val_loss: 0.1164\n",
      "Epoch 32/100\n",
      "34815/34815 [==============================] - 2s 49us/sample - loss: 0.1009 - val_loss: 0.1218\n",
      "Epoch 33/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.1006 - val_loss: 0.1164\n",
      "Epoch 34/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1006 - val_loss: 0.1207\n",
      "Epoch 35/100\n",
      "34815/34815 [==============================] - 2s 51us/sample - loss: 0.1006 - val_loss: 0.1155\n",
      "Epoch 36/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.1001 - val_loss: 0.1181\n",
      "Epoch 37/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.0999 - val_loss: 0.1170\n",
      "Epoch 38/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.0998 - val_loss: 0.1147\n",
      "Epoch 39/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.0998 - val_loss: 0.1164\n",
      "Epoch 40/100\n",
      "34815/34815 [==============================] - 2s 49us/sample - loss: 0.0996 - val_loss: 0.1189\n",
      "Epoch 41/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.0993 - val_loss: 0.1163\n",
      "Epoch 42/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.0993 - val_loss: 0.1145\n",
      "Epoch 43/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.0993 - val_loss: 0.1154\n",
      "Epoch 44/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.0992 - val_loss: 0.1178\n",
      "Epoch 45/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.0986 - val_loss: 0.1156\n",
      "Epoch 46/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.0985 - val_loss: 0.1228\n",
      "Epoch 47/100\n",
      "34815/34815 [==============================] - 2s 53us/sample - loss: 0.0985 - val_loss: 0.1153\n",
      "Epoch 48/100\n",
      "34815/34815 [==============================] - 2s 51us/sample - loss: 0.0989 - val_loss: 0.1197\n",
      "Epoch 49/100\n",
      "34815/34815 [==============================] - 2s 54us/sample - loss: 0.0984 - val_loss: 0.1162\n",
      "Epoch 50/100\n",
      "34815/34815 [==============================] - 2s 50us/sample - loss: 0.0982 - val_loss: 0.1171\n",
      "Epoch 51/100\n",
      "34815/34815 [==============================] - 2s 51us/sample - loss: 0.0984 - val_loss: 0.1221\n",
      "Epoch 52/100\n",
      "34815/34815 [==============================] - 2s 52us/sample - loss: 0.0980 - val_loss: 0.1165\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 8358.48046875\n",
      "MAPE: 0.18521555025154\n",
      "MAE: 5745.3064573053725\n",
      "R2 score: 0.6420461778207316\n",
      " \n",
      " \n",
      "---------------------------------------------------\n",
      "Train on 35385 samples, validate on 589 samples\n",
      "Epoch 1/100\n",
      "35385/35385 [==============================] - 5s 136us/sample - loss: 0.1423 - val_loss: 0.1260\n",
      "Epoch 2/100\n",
      "35385/35385 [==============================] - 2s 48us/sample - loss: 0.1276 - val_loss: 0.1201\n",
      "Epoch 3/100\n",
      "35385/35385 [==============================] - 2s 50us/sample - loss: 0.1253 - val_loss: 0.1171\n",
      "Epoch 4/100\n",
      "35385/35385 [==============================] - 2s 47us/sample - loss: 0.1242 - val_loss: 0.1175\n",
      "Epoch 5/100\n",
      "35385/35385 [==============================] - 2s 51us/sample - loss: 0.1232 - val_loss: 0.1188\n",
      "Epoch 6/100\n",
      "35385/35385 [==============================] - 2s 51us/sample - loss: 0.1228 - val_loss: 0.1191\n",
      "Epoch 7/100\n",
      "35385/35385 [==============================] - 2s 52us/sample - loss: 0.1212 - val_loss: 0.1181\n",
      "Epoch 8/100\n",
      "35385/35385 [==============================] - 2s 52us/sample - loss: 0.1206 - val_loss: 0.1222\n",
      "Epoch 9/100\n",
      "35385/35385 [==============================] - 2s 52us/sample - loss: 0.1194 - val_loss: 0.1279\n",
      "Epoch 10/100\n",
      "35385/35385 [==============================] - 2s 53us/sample - loss: 0.1184 - val_loss: 0.1188\n",
      "Epoch 11/100\n",
      "35385/35385 [==============================] - 2s 51us/sample - loss: 0.1177 - val_loss: 0.1349\n",
      "Epoch 12/100\n",
      "35385/35385 [==============================] - 2s 52us/sample - loss: 0.1163 - val_loss: 0.1262\n",
      "Epoch 13/100\n",
      "35385/35385 [==============================] - 2s 52us/sample - loss: 0.1151 - val_loss: 0.1202\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 8889.69921875\n",
      "MAPE: 0.19614418722040466\n",
      "MAE: 5878.355559940047\n",
      "R2 score: 0.5963802596330612\n",
      " \n",
      " \n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# lag=14\n",
    "ds=['october','november','december']\n",
    "\n",
    "data_LSTM_X, data_LSTM_Y = sequence_data_build(data, lag)\n",
    "totday=int(data_LSTM_X.shape[0]/20)\n",
    "m1 = [totday-61, totday-31, totday, ]\n",
    "m2 = [m1[0]-31, m1[1]-30, m1[2]-31, ]\n",
    "for i in range(len(m1)):\n",
    "    trainX, trainY, testX, testY = train_test_build(data_LSTM_X, data_LSTM_Y, m1[i], m2[i])\n",
    "    testYcopy=testY\n",
    "    saving = True\n",
    "    EarlyStop = True\n",
    "    rmse, mape, mae, r2, history = model_build(trainX, trainY, testX, testY, units, saving, ds[i], EarlyStop)\n",
    "        \n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"Epochs\")\n",
    "    pyplot.ylabel(\"Mean absolute error\")\n",
    "    pyplot.savefig(\"SM2F_\" + ds[i] + \".png\")\n",
    "    pyplot.show()\n",
    "    \n",
    "    \n",
    "    print(\"Root mean square error: {0}\".format(rmse))\n",
    "    print(\"MAPE: {0}\".format(mape))\n",
    "    print(\"MAE: {0}\".format(mae))\n",
    "    print(\"R2 score: {0}\".format(r2))\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lag vector\n",
      "[7, 14]\n",
      "LSTM units\n",
      "[2, 4, 8, 16, 32, 64, 128]\n",
      "Cross-validation results\n",
      "14 128\n",
      "----------------------------\n",
      "RMSE\n",
      "[[[9457.08691406 9514.91113281 9686.01757812 9489.01660156 9352.48242188\n",
      "   9319.546875   9292.24511719]\n",
      "  [7693.75927734 7854.38427734 7500.76074219 7687.37255859 7770.6015625\n",
      "   7500.62304688 7443.32177734]]\n",
      "\n",
      " [[8254.93945312 8776.24023438 8449.4375     8568.25195312 8603.73535156\n",
      "   8410.04101562 8514.86132812]\n",
      "  [9054.84863281 9417.25390625 9863.85253906 8498.42773438 9329.83398438\n",
      "   9467.63671875 8501.8984375 ]]\n",
      "\n",
      " [[7651.95996094 8043.68798828 8564.4453125  8267.90625    8247.70117188\n",
      "   8159.40527344 8094.27783203]\n",
      "  [9603.65429688 9188.77734375 9116.74804688 9058.38769531 8643.09277344\n",
      "   8697.1953125  8714.25585938]]]\n",
      "----------------------------\n",
      "MAE\n",
      "[[[0.66553177 0.67766316 0.67188573 0.66772661 0.67057594 0.68046078\n",
      "   0.68574428]\n",
      "  [0.18055446 0.19195683 0.15248012 0.16572908 0.16544066 0.17990643\n",
      "   0.18375309]]\n",
      "\n",
      " [[0.17020935 0.19074789 0.18318555 0.18301057 0.18564413 0.1789742\n",
      "   0.18517857]\n",
      "  [0.17085011 0.26503425 0.28779276 0.195687   0.30199497 0.23773992\n",
      "   0.23052982]]\n",
      "\n",
      " [[0.17248943 0.19593435 0.20450091 0.2153305  0.2234166  0.21194423\n",
      "   0.22166683]\n",
      "  [0.22748389 0.23237852 0.19499634 0.18772874 0.17424711 0.17352954\n",
      "   0.18509848]]]\n",
      "----------------------------\n",
      "MAPE\n",
      "[[[6323.73140554 6263.83494125 6287.49809807 6254.08987446 6169.29857926\n",
      "   6178.41568013 6185.95352823]\n",
      "  [5164.86452351 5231.30340131 4843.88825572 5192.68984234 5093.58263642\n",
      "   5101.73560234 5092.15307368]]\n",
      "\n",
      " [[5324.47222258 5904.47536268 5663.27872347 5661.02716225 5722.21289705\n",
      "   5691.83981644 5686.43871107]\n",
      "  [5641.02667786 6245.18209284 6593.02181349 5562.94022741 6341.59367166\n",
      "   6233.14839439 5629.18155938]]\n",
      "\n",
      " [[5119.85919682 5457.03654485 5926.51628546 5750.60469607 5752.06117821\n",
      "   5677.64595669 5669.45929705]\n",
      "  [6401.44133943 6188.19545256 6003.1250287  5917.19049479 5636.19368961\n",
      "   5651.09253958 5631.15161904]]]\n",
      "----------------------------\n",
      "R2-score\n",
      "[[[0.18785248 0.17789065 0.14805679 0.18235915 0.20571931 0.21130373\n",
      "   0.21591804]\n",
      "  [0.68605995 0.67281468 0.70161283 0.68658097 0.67975763 0.70162377\n",
      "   0.7061653 ]]\n",
      "\n",
      " [[0.24323427 0.14463656 0.20715331 0.18469869 0.17793209 0.21452951\n",
      "   0.19482772]\n",
      "  [0.58724867 0.55354815 0.51019946 0.63641723 0.5617984  0.54875821\n",
      "   0.63612025]]\n",
      "\n",
      " [[0.59577767 0.5533315  0.49362369 0.52808252 0.53038626 0.54038735\n",
      "   0.54769523]\n",
      "  [0.53823434 0.57726907 0.58387049 0.5891811  0.62598678 0.6212898\n",
      "   0.61980262]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"lag vector\")\n",
    "print(lag_vec)\n",
    "print(\"LSTM units\")\n",
    "print(units_vec)\n",
    "print(\"Cross-validation results\")\n",
    "print(lag,units)\n",
    "print(\"----------------------------\")\n",
    "print(\"RMSE\")\n",
    "print(results[0,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"MAE\")\n",
    "print(results[1,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"MAPE\")\n",
    "print(results[2,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"R2-score\")\n",
    "print(results[3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
