{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Concatenate, Input\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error,  mean_absolute_error, r2_score\n",
    " \n",
    "# tensorflow.reset_default_graph()\n",
    "tensorflow.random.set_seed(0)\n",
    "# random.seed(0)\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=14, day_offset=5):\n",
    "    dataY= []\n",
    "    dataX1=numpy.zeros([(len(dataset)-look_back),4,look_back])\n",
    "    dataX2=numpy.zeros([(len(dataset)-look_back),32])\n",
    "#     dataX3=numpy.zeros([(len(dataset)-look_back),2,look_back])\n",
    "    \n",
    "    #print(dataX.shape)\n",
    "    for i in range(look_back,len(dataset)):\n",
    "       # print(i)\n",
    "        a = numpy.zeros([4,look_back])\n",
    "        b = numpy.zeros([1,32])\n",
    "        t1=dataset[(i-look_back):i, 0]\n",
    "        t1=numpy.reshape(t1,[1,look_back])\n",
    "        t4=dataset[(i-look_back):i, 47]\n",
    "        t4=numpy.reshape(t4,[1,look_back])\n",
    "        #print(t1.shape)\n",
    "        t2=dataset[i, 47-look_back:47]\n",
    "        t2=numpy.reshape(t2,[1,look_back])\n",
    "        t6=dataset[i,92-(look_back):92]\n",
    "        t6=numpy.reshape(t6,[1,look_back])\n",
    "#         t3=numpy.zeros([1,look_back])\n",
    "#         if i>=((day_offset+1)*7+look_back):\n",
    "#             t3[0,0:day_offset]=[dataset[j,0] for j in range(i-(((day_offset+1)*7)+look_back),i-(look_back+7),7)]\n",
    "#         t5=numpy.zeros([1,look_back])\n",
    "#         if i>=((day_offset+1)*7+look_back):\n",
    "#             t5[0,0:day_offset]=[dataset[j,48] for j in range(i-(((day_offset+1)*7)+look_back),i-(look_back+7),7)]\n",
    "            \n",
    "            \n",
    "        #print(t2.shape)\n",
    "        a[0,:] = t1\n",
    "        a[1,:] = t4\n",
    "        a[2,:] = t2\n",
    "        a[3,:] = t6\n",
    "#         a[4,:] = t3\n",
    "#         a[5,:] = t5\n",
    "        b = dataset[i,-32:]\n",
    "        dataX1[i-look_back,:,:]=a\n",
    "        dataX2[i-look_back,:]=b\n",
    "#         dataX3[i-look_back,:,:]=a[4:,:]\n",
    "#         a = numpy.concatenate([dataset[(i-look_back-7):i-7, 0], dataset[i,-14:]],axis=1)\n",
    "        \n",
    "        #dataX.append(a)\n",
    "        dataY.append(dataset[i,-33])\n",
    "    return numpy.array(dataX1),numpy.array(dataX2),numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(datarange, categorical=[]):  \n",
    "    datarange= pd.DataFrame(datarange)\n",
    "    if not categorical:\n",
    "        meandata=datarange.mean()\n",
    "        meandata=meandata.to_numpy()\n",
    "    else:\n",
    "        meandata=datarange.mean()\n",
    "        meandata=meandata.to_numpy()\n",
    "        \n",
    "        modedata = datarange.mode()\n",
    "        modedata = modedata.to_numpy()\n",
    "        modedata = modedata[0,:]\n",
    "        \n",
    "        for i in categorical:\n",
    "                meandata[i-1] = modedata[i]\n",
    "                \n",
    "    datetime_series = pd.to_datetime(datarange['fltdat'])\n",
    "    miss_idx=pd.date_range(start = '01-01-2015', end = '31-12-2019' ).difference(datetime_series)\n",
    "    datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
    "    datarange=datarange.set_index(datetime_index)\n",
    "\n",
    "    datarange.drop('fltdat',axis=1,inplace=True)\n",
    "    newidx = pd.date_range('01-01-2015', '31-12-2019')\n",
    "    datarange = datarange.reindex(newidx, fill_value=0)\n",
    "    \n",
    "    meandata=meandata.reshape(1,meandata.shape[0])\n",
    "    dat = numpy.tile(meandata, [miss_idx.shape[0],1])\n",
    "    datarange.loc[miss_idx]=dat\n",
    "    return datarange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_test, y_pred):\n",
    "        import numpy as np\n",
    "        t = np.array(y_test)\n",
    "        p = np.array(y_pred)\n",
    "        mae = list()\n",
    "        mape = list()\n",
    "        for i in range(len(t)):\n",
    "            if (t[i] == 0):\n",
    "                mae.append(abs(p[i]))\n",
    "            else:\n",
    "                mae.append(float(abs(t[i] - p[i])))\n",
    "                mape.append(float(abs((t[i] - p[i])/t[i])))\n",
    "        return np.mean(mae) , np.mean(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fltdat', 'paxcntfc', 'fltnum', 'legorg', 'legdst', 'acrtypcod',\n",
      "       'keyidr', 'totpaylodwgt', 'totpaylodvol', 'totpaylodpos', 'totsetfc',\n",
      "       'totpaxwgt', 'dp_51', 'dp_50', 'dp_49', 'dp_48', 'dp_47', 'dp_46',\n",
      "       'dp_45', 'dp_44', 'dp_43', 'dp_42', 'dp_41', 'dp_40', 'dp_39', 'dp_38',\n",
      "       'dp_37', 'dp_36', 'dp_35', 'dp_34', 'dp_33', 'dp_32', 'dp_31', 'dp_30',\n",
      "       'dp_29', 'dp_28', 'dp_27', 'dp_26', 'dp_25', 'dp_24', 'dp_23', 'dp_22',\n",
      "       'dp_21', 'dp_20', 'dp_19', 'dp_18', 'dp_17', 'dp_16', 'dp_15', 'dp_14',\n",
      "       'dp_13', 'dp_12', 'dp_11', 'dp_10', 'dp_9', 'dp_8', 'dp_7', 'dp_6',\n",
      "       'dp_5', 'dp_4', 'dp_3', 'dp_2', 'dp_1'],\n",
      "      dtype='object')\n",
      "Index(['fltdat', 'paxcntfc', 'acrtypcod', 'keyidr', 'totpaylodwgt',\n",
      "       'totpaxwgt', 'dp_51', 'dp_50', 'dp_49', 'dp_48', 'dp_47', 'dp_46',\n",
      "       'dp_45', 'dp_44', 'dp_43', 'dp_42', 'dp_41', 'dp_40', 'dp_39', 'dp_38',\n",
      "       'dp_37', 'dp_36', 'dp_35', 'dp_34', 'dp_33', 'dp_32', 'dp_31', 'dp_30',\n",
      "       'dp_29', 'dp_28', 'dp_27', 'dp_26', 'dp_25', 'dp_24', 'dp_23', 'dp_22',\n",
      "       'dp_21', 'dp_20', 'dp_19', 'dp_18', 'dp_17', 'dp_16', 'dp_15', 'dp_14',\n",
      "       'dp_13', 'dp_12', 'dp_11', 'dp_10', 'dp_9', 'dp_8', 'paxcnty', 'dcp_51',\n",
      "       'dcp_50', 'dcp_49', 'dcp_48', 'dcp_47', 'dcp_46', 'dcp_45', 'dcp_44',\n",
      "       'dcp_43', 'dcp_42', 'dcp_41', 'dcp_40', 'dcp_39', 'dcp_38', 'dcp_37',\n",
      "       'dcp_36', 'dcp_35', 'dcp_34', 'dcp_33', 'dcp_3', 'dcp_31', 'dcp_30',\n",
      "       'dcp_29', 'dcp_28', 'dcp_27', 'dcp_26', 'dcp_25', 'dcp_24', 'dcp_23',\n",
      "       'dcp_22', 'dcp_21', 'dcp_20', 'dcp_19', 'dcp_18', 'dcp_17', 'dcp_16',\n",
      "       'dcp_15', 'dcp_14', 'dcp_13', 'dcp_12', 'dcp_11', 'dcp_10', 'dcp_9',\n",
      "       'dcp_8'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "dataframe = read_csv('data/rmscapfc.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "print(dataframe.columns)\n",
    "dataframe.drop(dataframe.columns[[2,3,4,8,9,10,56,57,58,59,60,61,62] ], axis=1, inplace=True)\n",
    "\n",
    "dataframe2 = read_csv('data/rmscapy.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "# print(dataframe2.columns)\n",
    "f_column = dataframe2[[\"paxcnty\", \"dcp_51\", \"dcp_50\", \"dcp_49\", \"dcp_48\", \"dcp_47\", \"dcp_46\",\n",
    "       \"dcp_45\", \"dcp_44\", \"dcp_43\", \"dcp_42\", \"dcp_41\", \"dcp_40\", \"dcp_39\",\n",
    "       \"dcp_38\", \"dcp_37\", \"dcp_36\", \"dcp_35\", \"dcp_34\", \"dcp_33\", \"dcp_3\",\n",
    "       \"dcp_31\", \"dcp_30\", \"dcp_29\", \"dcp_28\", \"dcp_27\", \"dcp_26\", \"dcp_25\",\n",
    "       \"dcp_24\", \"dcp_23\", \"dcp_22\", \"dcp_21\", \"dcp_20\", \"dcp_19\", \"dcp_18\",\n",
    "       \"dcp_17\", \"dcp_16\", \"dcp_15\", \"dcp_14\", \"dcp_13\", \"dcp_12\", \"dcp_11\",\n",
    "       \"dcp_10\", \"dcp_9\", \"dcp_8\"]]\n",
    " \n",
    "\n",
    "dataframe = pd.concat([dataframe,f_column], axis = 1)\n",
    "# print(dataframe.columns)\n",
    "dataframe3 = read_csv('data/uldfc.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "\n",
    "dataframe3.drop(dataframe3.columns[[1,2,3] ], axis=1, inplace=True)\n",
    "dataframe4 = read_csv('data/uldy.csv', parse_dates=['fltdat'],dayfirst=True)\n",
    "dataframe4.drop(dataframe4.columns[[1,2,3] ], axis=1, inplace=True)\n",
    "f_column = dataframe4[[\"county\"]]\n",
    "dataframe3 = pd.concat([dataframe3,f_column], axis = 1)\n",
    "print(dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM=[0,1825,1825,1817,1816,1812,1821,1825,1824,1819,1825,1825,1824,1826,1819,1825,1822,1823,1813, 1826, 1820]\n",
    "NUMuld=[0,1817,1574,1808,1802,1807,1808,1730,1817,1385,1820,1816,1606,1819,1810,1817,421,1813,434,1532,1814]\n",
    "cat_inp=[2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iist\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  import sys\n",
      "C:\\Users\\iist\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# dataset, datasetY = numpy.empty([1805,3,look_back]), []\n",
    "for i in range(0,len(NUM)-1):\n",
    "#     print(i)\n",
    "    datasub = dataframe.iloc[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "#     datasub=datasetall[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "    datasub = missing_values(datasub, cat_inp)\n",
    "    datasub = datasub.values\n",
    "    datasub = datasub.astype('float32')\n",
    "    \n",
    "    datasubuld = dataframe3.iloc[sum(NUMuld[0:i+1]):sum(NUMuld[0:i+2]),:]\n",
    "#     datasub=datasetall[sum(NUM[0:i+1]):sum(NUM[0:i+2]),:]\n",
    "    datasubuld = missing_values(datasubuld)\n",
    "    datasubuld = datasubuld.values\n",
    "    datasubuld = datasubuld.astype('float32')\n",
    "    \n",
    "    datasub = numpy.concatenate([datasub, datasubuld], axis=1)\n",
    "    if i==0:\n",
    "        data = datasub\n",
    "    else:\n",
    "        data = numpy.concatenate([data, datasub], axis =0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36520, 97)\n"
     ]
    }
   ],
   "source": [
    "out = data[:,3] - data[:,4] - (data[:,-1]*data[:,-2]*114)\n",
    "out = out.reshape(out.shape[0],1)\n",
    "data = numpy.concatenate([data,out], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "cols = dataframe.columns\n",
    "cols = cols[1:] \n",
    "newcol = pd.Index(['uldfc','uldy','out'])\n",
    "cols = cols.append(newcol)\n",
    "data = pd.DataFrame(data, columns=cols).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.reset_index()\n",
    "# Array that specifies which columns are categorical inputs\n",
    "categorical_features=[\"acrtypcod\", \"keyidr\"] # Except column \"7\", all are label encoded and \"7\" is \n",
    "                                                               # one-hot encoded. \n",
    "# Array that specifies which columns are continuous inputs\n",
    "#contnuous_features = [\"9\",\"10\",\"11\"]\n",
    "output_feature = \"out\"  # specifies which column is output\n",
    "\n",
    "\n",
    "temp={}\n",
    "label_encoders = {} \n",
    "# for loop starts for label encoding\n",
    "for i in range(0,len(categorical_features)):\n",
    "    #codes for one-hot encoding\n",
    "    label_encoders[categorical_features[i]] = OneHotEncoder(handle_unknown='ignore')\n",
    "    label_encoders[categorical_features[i]].fit(data[categorical_features[i]].values.reshape(-1,1))\n",
    "    temp[categorical_features[i]]=label_encoders[categorical_features[i]].transform(data[categorical_features[i]].values.reshape(-1,1)).toarray() \n",
    "\n",
    "    \n",
    "# Adding the one hot encoded features into the data\n",
    "# The new columns will be named from 13 onwards.\n",
    "ind=13\n",
    "new_categorical_features=[]\n",
    "for i in range(0,len(categorical_features)):\n",
    "    for j in range(0,temp[categorical_features[i]].shape[1]):\n",
    "        data[str(ind)] = temp[categorical_features[i]][:,j]\n",
    "        new_categorical_features.append(str(ind)) # Stroing the column identity of newly added one-hot encoded columns as \n",
    "                                                  # categorical input.\n",
    "        ind=ind+1\n",
    "    data=data.drop([categorical_features[i]], axis=1) # The old column is deleted from the data\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.values\n",
    "data = data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_data_build(data, look_back):\n",
    "    for i in range(0,20):\n",
    "        datasub = data[i*1826:((i+1)*1826),:]\n",
    "        X1, X2, Y = create_dataset(datasub, look_back)\n",
    "        Y = Y.reshape(Y.shape[0],1)\n",
    "        if i==0: \n",
    "            data_LSTM_X1, data_LSTM_X2 = X1, X2\n",
    "            data_LSTM_Y = Y\n",
    "        else:\n",
    "            data_LSTM_X1,data_LSTM_X2 = numpy.concatenate([data_LSTM_X1, X1],axis=0), numpy.concatenate([data_LSTM_X2, X2],axis=0)  \n",
    "            data_LSTM_Y = numpy.concatenate([data_LSTM_Y, Y],axis=0)\n",
    "    return data_LSTM_X1,data_LSTM_X2,data_LSTM_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_build(X, Y, m1, m2):\n",
    "    spliter = int(X.shape[0]/20)\n",
    "    tot_len = m1\n",
    "    train_len = m2\n",
    "    for i in range(0,20):\n",
    "        Xsub = X[i*spliter:((i+1)*spliter),:]\n",
    "        Ysub = Y[i*spliter:((i+1)*spliter)]\n",
    "        if i==0:\n",
    "            trainX = Xsub[0:m2,:]\n",
    "            testX = Xsub[m2:m1,:]\n",
    "            trainY = Ysub[0:m2]\n",
    "            testY = Ysub[m2:m1]\n",
    "        else:\n",
    "            trainX = numpy.concatenate([trainX, Xsub[0:m2,:]], axis=0)\n",
    "            testX = numpy.concatenate([testX, Xsub[m2:m1,:]], axis=0)\n",
    "            trainY = numpy.concatenate([trainY, Ysub[0:m2]], axis=0)\n",
    "            testY = numpy.concatenate([testY, Ysub[m2:m1]], axis=0)\n",
    "        \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(trainX1, trainX2, testX1, testX2, trainY, testY, units, saving =False, month=None, EarlyStop = False):\n",
    "    \n",
    "    if EarlyStop:\n",
    "        callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                                            mode = 'min', restore_best_weights=True)\n",
    "    print(trainX1.shape)\n",
    "    print(trainX2.shape)\n",
    "    inp1 = Input(shape=(4, trainX1.shape[2]))\n",
    "    inp2 = Input(shape=(trainX2.shape[1]))\n",
    "#     inp3 = Input(shape=(2, trainX3.shape[2]))\n",
    "\n",
    "    LS1 = LSTM(units, input_shape=(4, trainX1.shape[2]))\n",
    "    out1 = LS1(inp1)\n",
    "    D1 = Dense(32)\n",
    "    out2 = D1(inp2)\n",
    "#     LS2 = LSTM(units, input_shape=(2, trainX2.shape[2]))\n",
    "#     out2 = LS2(inp2)\n",
    "#     LS3 = LSTM(units, input_shape=(2, trainX3.shape[2]))\n",
    "#     out3 = LS3(inp3)\n",
    "\n",
    "    mrg = Concatenate(axis=1)([out1,out2])\n",
    "    op = Dense(1)(mrg)\n",
    "\n",
    "    model = Model(inputs=[inp1, inp2], outputs=op)\n",
    "#     model.add(Dense(1))\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    if EarlyStop:\n",
    "        history =model.fit([trainX1, trainX2], trainY, epochs=100, batch_size=150, \n",
    "                           validation_data=([testX1,testX2], testY),verbose=1,callbacks=[callback])\n",
    "    else:\n",
    "        history =model.fit([trainX1, trainX2], trainY, epochs=100, batch_size=150, \n",
    "                           validation_data=([testX1,testX2], testY),verbose=1)\n",
    "                \n",
    "    testPredict = model.predict([testX1,testX2])\n",
    "                \n",
    "    sh = testPredict.shape\n",
    "    inv_yhat = testPredict.reshape(sh[0]*sh[1],1)\n",
    "    inv_yhat = numpy.concatenate([ data[0:inv_yhat.shape[0],0:96], inv_yhat], axis=1)\n",
    "\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "    testY = testY.reshape(sh[0]*sh[1],1)\n",
    " \n",
    "    inv_y = numpy.concatenate([data[0:testY.shape[0],0:96], testY], axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-1]\n",
    "             \n",
    "    rmse = numpy.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    mae, mape = mean_absolute_percentage_error(inv_y, inv_yhat) \n",
    "    r2 = r2_score(inv_y, inv_yhat)\n",
    "    \n",
    "    res=[]\n",
    "    if saving:\n",
    "        inv_y = inv_y.reshape(inv_y.shape[0],1) \n",
    "        inv_yhat = inv_yhat.reshape(inv_yhat.shape[0],1) \n",
    "        res = numpy.concatenate([inv_y, inv_yhat], axis=1)\n",
    "        df = pd.DataFrame(res)\n",
    "        res = df.to_csv(\"hybrid_\" + month + \".csv\", index = False)\n",
    "    return rmse, mape,mae,r2, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(lag_vec = [7,14,21,28], units_vec = [2,4,8,16,32,64,128]):\n",
    "    results = numpy.zeros([4,3,len(lag_vec),len(units_vec)])\n",
    "    \n",
    "    for folds in range(0,3):\n",
    "        for i in range(0,len(lag_vec)):\n",
    "            data_LSTM_X1,data_LSTM_X2,data_LSTM_Y = sequence_data_build(data, lag_vec[i])\n",
    "            totday = int(data_LSTM_X1.shape[0]/20)\n",
    "            m1 = [totday-152, totday-121, totday-91, ]\n",
    "            m2 = [m1[0]-31, m1[1]-31, m1[2]-30, ]\n",
    "            lag = lag_vec[i]\n",
    "            trainX1, trainY, testX1, testY= train_test_build(data_LSTM_X1, data_LSTM_Y, m1[folds], m2[folds])\n",
    "            trainX2, trainY, testX2, testY= train_test_build(data_LSTM_X2, data_LSTM_Y, m1[folds], m2[folds])\n",
    "#             trainX3, trainY, testX3, testY= train_test_build(data_LSTM_X3, data_LSTM_Y, m1[folds], m2[folds])\n",
    "            testYcopy=testY\n",
    "            for j in range(0, len(units_vec)):\n",
    "                print(\" \")\n",
    "                print(\" \")\n",
    "                print(\" \")\n",
    "                print(\"------------------------------------------------\")\n",
    "                print(\"fold: {0}, lag: {1}, units: {2}\".format(folds, lag_vec[i], units_vec[j]))\n",
    "                units = units_vec[j]\n",
    "                rmse, mape, mae, r2, his = model_build(trainX1, trainX2, testX1, testX2, trainY, testY, units, EarlyStop = True) \n",
    "                results[0,folds,i,j] = rmse \n",
    "                results[2,folds,i,j], results[1,folds,i,j] = mae, mape\n",
    "                results[3,folds,i,j] = r2\n",
    "    return results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 2\n",
      "(32720, 4, 7)\n",
      "(32720, 32)\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 6s 198us/sample - loss: 0.1032 - val_loss: 0.0478\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 48us/sample - loss: 0.0600 - val_loss: 0.0509\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 47us/sample - loss: 0.0580 - val_loss: 0.0475\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.0578 - val_loss: 0.0462\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.0573 - val_loss: 0.0462\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0571 - val_loss: 0.0470\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0570 - val_loss: 0.0455\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0567 - val_loss: 0.0473\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0567 - val_loss: 0.0463\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0566 - val_loss: 0.0444\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0566 - val_loss: 0.0476\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0564 - val_loss: 0.0461\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0564 - val_loss: 0.0453\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0562 - val_loss: 0.0477\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0564 - val_loss: 0.0453\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0564 - val_loss: 0.0473\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0563 - val_loss: 0.0467\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0563 - val_loss: 0.0465\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0562 - val_loss: 0.0457\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0473\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 4\n",
      "(32720, 4, 7)\n",
      "(32720, 32)\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 5s 153us/sample - loss: 0.0925 - val_loss: 0.0464\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0602 - val_loss: 0.0509\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0581 - val_loss: 0.0467\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0575 - val_loss: 0.0457\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0570 - val_loss: 0.0460\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.0569 - val_loss: 0.0471\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0568 - val_loss: 0.0457\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0565 - val_loss: 0.0473\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0566 - val_loss: 0.0469\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0563 - val_loss: 0.0444\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0565 - val_loss: 0.0470\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0562 - val_loss: 0.0465\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0562 - val_loss: 0.0458\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0561 - val_loss: 0.0472\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0562 - val_loss: 0.0448\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0562 - val_loss: 0.0476\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0562 - val_loss: 0.0461\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0467\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0561 - val_loss: 0.0455\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0561 - val_loss: 0.0466\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 8\n",
      "(32720, 4, 7)\n",
      "(32720, 32)\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 5s 154us/sample - loss: 0.0959 - val_loss: 0.0475\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0604 - val_loss: 0.0518\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.0580 - val_loss: 0.0473\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0571 - val_loss: 0.0455\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0566 - val_loss: 0.0463\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0565 - val_loss: 0.0476\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0566 - val_loss: 0.0448\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0564 - val_loss: 0.0474\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0565 - val_loss: 0.0469\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0563 - val_loss: 0.0449\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0565 - val_loss: 0.0467\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0563 - val_loss: 0.0463\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0562 - val_loss: 0.0457\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0561 - val_loss: 0.0474\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0561 - val_loss: 0.0447\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0472\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0562 - val_loss: 0.0468\n",
      "Epoch 18/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0561 - val_loss: 0.0463\n",
      "Epoch 19/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0459\n",
      "Epoch 20/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0459\n",
      "Epoch 21/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0467\n",
      "Epoch 22/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0560 - val_loss: 0.0484\n",
      "Epoch 23/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0448\n",
      "Epoch 24/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0561 - val_loss: 0.0457\n",
      "Epoch 25/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0468\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 16\n",
      "(32720, 4, 7)\n",
      "(32720, 32)\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 5s 151us/sample - loss: 0.0935 - val_loss: 0.0486\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0582 - val_loss: 0.0514\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0571 - val_loss: 0.0469\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.0572 - val_loss: 0.0447\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0567 - val_loss: 0.0465\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 61us/sample - loss: 0.0565 - val_loss: 0.0470\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 61us/sample - loss: 0.0565 - val_loss: 0.0444\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0563 - val_loss: 0.0470\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0564 - val_loss: 0.0462\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 61us/sample - loss: 0.0560 - val_loss: 0.0452\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0563 - val_loss: 0.0485\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0561 - val_loss: 0.0465\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0561 - val_loss: 0.0461\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0560 - val_loss: 0.0481\n",
      "Epoch 15/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0560 - val_loss: 0.0451\n",
      "Epoch 16/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0561 - val_loss: 0.0471\n",
      "Epoch 17/100\n",
      "32720/32720 [==============================] - 2s 59us/sample - loss: 0.0560 - val_loss: 0.0459\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 32\n",
      "(32720, 4, 7)\n",
      "(32720, 32)\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 5s 155us/sample - loss: 0.0888 - val_loss: 0.0473\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0589 - val_loss: 0.0507\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0572 - val_loss: 0.0472\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0567 - val_loss: 0.0445\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0563 - val_loss: 0.0459\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0562 - val_loss: 0.0462\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0563 - val_loss: 0.0454\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0560 - val_loss: 0.0471\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0562 - val_loss: 0.0457\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0560 - val_loss: 0.0452\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0475\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0559 - val_loss: 0.0465\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0559 - val_loss: 0.0461\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 58us/sample - loss: 0.0558 - val_loss: 0.0469\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 64\n",
      "(32720, 4, 7)\n",
      "(32720, 32)\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 5s 154us/sample - loss: 0.0889 - val_loss: 0.0470\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0580 - val_loss: 0.0493\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0567 - val_loss: 0.0469\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0566 - val_loss: 0.0446\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0560 - val_loss: 0.0460\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0460\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0562 - val_loss: 0.0452\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0471\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0560 - val_loss: 0.0457\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0558 - val_loss: 0.0449\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0561 - val_loss: 0.0470\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0559 - val_loss: 0.0463\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0558 - val_loss: 0.0453\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0466\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 7, units: 128\n",
      "(32720, 4, 7)\n",
      "(32720, 32)\n",
      "Train on 32720 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32720/32720 [==============================] - 4s 137us/sample - loss: 0.0832 - val_loss: 0.0476\n",
      "Epoch 2/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0596 - val_loss: 0.0483\n",
      "Epoch 3/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0578 - val_loss: 0.0476\n",
      "Epoch 4/100\n",
      "32720/32720 [==============================] - 2s 51us/sample - loss: 0.0571 - val_loss: 0.0453\n",
      "Epoch 5/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0563 - val_loss: 0.0458\n",
      "Epoch 6/100\n",
      "32720/32720 [==============================] - 2s 56us/sample - loss: 0.0562 - val_loss: 0.0463\n",
      "Epoch 7/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.0561 - val_loss: 0.0454\n",
      "Epoch 8/100\n",
      "32720/32720 [==============================] - 2s 53us/sample - loss: 0.0560 - val_loss: 0.0468\n",
      "Epoch 9/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0465\n",
      "Epoch 10/100\n",
      "32720/32720 [==============================] - 2s 55us/sample - loss: 0.0558 - val_loss: 0.0454\n",
      "Epoch 11/100\n",
      "32720/32720 [==============================] - 2s 52us/sample - loss: 0.0560 - val_loss: 0.0465\n",
      "Epoch 12/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0558 - val_loss: 0.0465\n",
      "Epoch 13/100\n",
      "32720/32720 [==============================] - 2s 54us/sample - loss: 0.0556 - val_loss: 0.0456\n",
      "Epoch 14/100\n",
      "32720/32720 [==============================] - 2s 57us/sample - loss: 0.0556 - val_loss: 0.0466\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 2\n",
      "(32580, 4, 14)\n",
      "(32580, 32)\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 5s 151us/sample - loss: 0.1051 - val_loss: 0.0479\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0599 - val_loss: 0.0471\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 51us/sample - loss: 0.0584 - val_loss: 0.0463\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0574 - val_loss: 0.0480\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0568 - val_loss: 0.0471\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0567 - val_loss: 0.0462\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0566 - val_loss: 0.0470\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0563 - val_loss: 0.0487\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0447\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 60us/sample - loss: 0.0562 - val_loss: 0.0477\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0563 - val_loss: 0.0470\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0499\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0563 - val_loss: 0.0472\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0561 - val_loss: 0.0472\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0475\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0480\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0560 - val_loss: 0.0491\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0465\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 4\n",
      "(32580, 4, 14)\n",
      "(32580, 32)\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 5s 149us/sample - loss: 0.1070 - val_loss: 0.0502\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0622 - val_loss: 0.0471\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0586 - val_loss: 0.0469\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0573 - val_loss: 0.0463\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 53us/sample - loss: 0.0568 - val_loss: 0.0471\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0567 - val_loss: 0.0458\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0566 - val_loss: 0.0467\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 53us/sample - loss: 0.0564 - val_loss: 0.0477\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0562 - val_loss: 0.0439\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0563 - val_loss: 0.0462\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0563 - val_loss: 0.0466\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0561 - val_loss: 0.0498\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0561 - val_loss: 0.0467\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0465\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0455\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0464\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0458\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0558 - val_loss: 0.0468\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0458\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 8\n",
      "(32580, 4, 14)\n",
      "(32580, 32)\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 5s 146us/sample - loss: 0.1101 - val_loss: 0.0471\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0586 - val_loss: 0.0465\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0572 - val_loss: 0.0478\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0568 - val_loss: 0.0466\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0564 - val_loss: 0.0462\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0563 - val_loss: 0.0462\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0562 - val_loss: 0.0467\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0561 - val_loss: 0.0481\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0443\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 49us/sample - loss: 0.0560 - val_loss: 0.0457\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 51us/sample - loss: 0.0562 - val_loss: 0.0465\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 48us/sample - loss: 0.0558 - val_loss: 0.0490\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 49us/sample - loss: 0.0559 - val_loss: 0.0469\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0557 - val_loss: 0.0465\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0453\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0558 - val_loss: 0.0473\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0557 - val_loss: 0.0463\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0556 - val_loss: 0.0476\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0459\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 16\n",
      "(32580, 4, 14)\n",
      "(32580, 32)\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 5s 144us/sample - loss: 0.1158 - val_loss: 0.0474\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0588 - val_loss: 0.0461\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0571 - val_loss: 0.0469\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0566 - val_loss: 0.0465\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0463\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0560 - val_loss: 0.0456\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0464\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0559 - val_loss: 0.0474\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0439\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0558 - val_loss: 0.0467\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0559 - val_loss: 0.0461\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0557 - val_loss: 0.0484\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 53us/sample - loss: 0.0557 - val_loss: 0.0460\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0466\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0458\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0468\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0458\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0469\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0461\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 32\n",
      "(32580, 4, 14)\n",
      "(32580, 32)\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 5s 154us/sample - loss: 0.0824 - val_loss: 0.0458\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0588 - val_loss: 0.0454\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 53us/sample - loss: 0.0574 - val_loss: 0.0467\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0562 - val_loss: 0.0461\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0454\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0556 - val_loss: 0.0450\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0555 - val_loss: 0.0465\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0555 - val_loss: 0.0475\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0552 - val_loss: 0.0435\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0554 - val_loss: 0.0452\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0456\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 61us/sample - loss: 0.0553 - val_loss: 0.0488\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0473\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0552 - val_loss: 0.0460\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0552 - val_loss: 0.0452\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0551 - val_loss: 0.0450\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0551 - val_loss: 0.0463\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0550 - val_loss: 0.0467\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 61us/sample - loss: 0.0550 - val_loss: 0.0451\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 64\n",
      "(32580, 4, 14)\n",
      "(32580, 32)\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 5s 155us/sample - loss: 0.0877 - val_loss: 0.0454\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0579 - val_loss: 0.0456\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0568 - val_loss: 0.0466\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0559 - val_loss: 0.0466\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 60us/sample - loss: 0.0558 - val_loss: 0.0462\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 53us/sample - loss: 0.0557 - val_loss: 0.0449\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0555 - val_loss: 0.0471\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0468\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0445\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0555 - val_loss: 0.0455\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0555 - val_loss: 0.0462\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0553 - val_loss: 0.0479\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0475\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0553 - val_loss: 0.0460\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0552 - val_loss: 0.0452\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 53us/sample - loss: 0.0552 - val_loss: 0.0446\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0552 - val_loss: 0.0465\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 53us/sample - loss: 0.0551 - val_loss: 0.0466\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0551 - val_loss: 0.0458\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 0, lag: 14, units: 128\n",
      "(32580, 4, 14)\n",
      "(32580, 32)\n",
      "Train on 32580 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "32580/32580 [==============================] - 5s 162us/sample - loss: 0.0809 - val_loss: 0.0453\n",
      "Epoch 2/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0593 - val_loss: 0.0457\n",
      "Epoch 3/100\n",
      "32580/32580 [==============================] - 2s 54us/sample - loss: 0.0580 - val_loss: 0.0458\n",
      "Epoch 4/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0568 - val_loss: 0.0462\n",
      "Epoch 5/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0560 - val_loss: 0.0462\n",
      "Epoch 6/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0455\n",
      "Epoch 7/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0459\n",
      "Epoch 8/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0556 - val_loss: 0.0473\n",
      "Epoch 9/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0553 - val_loss: 0.0443\n",
      "Epoch 10/100\n",
      "32580/32580 [==============================] - 2s 53us/sample - loss: 0.0555 - val_loss: 0.0459\n",
      "Epoch 11/100\n",
      "32580/32580 [==============================] - 2s 55us/sample - loss: 0.0554 - val_loss: 0.0452\n",
      "Epoch 12/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0478\n",
      "Epoch 13/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0554 - val_loss: 0.0480\n",
      "Epoch 14/100\n",
      "32580/32580 [==============================] - 2s 59us/sample - loss: 0.0552 - val_loss: 0.0463\n",
      "Epoch 15/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0551 - val_loss: 0.0450\n",
      "Epoch 16/100\n",
      "32580/32580 [==============================] - 2s 57us/sample - loss: 0.0551 - val_loss: 0.0443\n",
      "Epoch 17/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0550 - val_loss: 0.0463\n",
      "Epoch 18/100\n",
      "32580/32580 [==============================] - 2s 58us/sample - loss: 0.0550 - val_loss: 0.0466\n",
      "Epoch 19/100\n",
      "32580/32580 [==============================] - 2s 56us/sample - loss: 0.0550 - val_loss: 0.0458\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 2\n",
      "(33340, 4, 7)\n",
      "(33340, 32)\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 141us/sample - loss: 0.0920 - val_loss: 0.0492\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0610 - val_loss: 0.0481\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0588 - val_loss: 0.0483\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0580 - val_loss: 0.0455\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0575 - val_loss: 0.0446\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0574 - val_loss: 0.0458\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0570 - val_loss: 0.0460\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 59us/sample - loss: 0.0570 - val_loss: 0.0455\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0566 - val_loss: 0.0440\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0565 - val_loss: 0.0459\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0564 - val_loss: 0.0458\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0566 - val_loss: 0.0460\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.0563 - val_loss: 0.0451\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0562 - val_loss: 0.0451\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.0562 - val_loss: 0.0429\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 47us/sample - loss: 0.0562 - val_loss: 0.0441\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0448\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0561 - val_loss: 0.0453\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0561 - val_loss: 0.0455\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 59us/sample - loss: 0.0560 - val_loss: 0.0439\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0560 - val_loss: 0.0438\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 58us/sample - loss: 0.0562 - val_loss: 0.0440\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0442\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0460\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 58us/sample - loss: 0.0560 - val_loss: 0.0453\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 4\n",
      "(33340, 4, 7)\n",
      "(33340, 32)\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 149us/sample - loss: 0.1113 - val_loss: 0.0472\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 58us/sample - loss: 0.0612 - val_loss: 0.0468\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 58us/sample - loss: 0.0589 - val_loss: 0.0466\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0576 - val_loss: 0.0458\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0571 - val_loss: 0.0443\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.0570 - val_loss: 0.0454\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.0568 - val_loss: 0.0461\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 49us/sample - loss: 0.0567 - val_loss: 0.0454\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 47us/sample - loss: 0.0565 - val_loss: 0.0440\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.0564 - val_loss: 0.0462\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.0563 - val_loss: 0.0453\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 50us/sample - loss: 0.0565 - val_loss: 0.0451\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0562 - val_loss: 0.0451\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0462\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0560 - val_loss: 0.0444\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0561 - val_loss: 0.0442\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0440\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0560 - val_loss: 0.0447\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.0560 - val_loss: 0.0453\n",
      "Epoch 20/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.0559 - val_loss: 0.0440\n",
      "Epoch 21/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0559 - val_loss: 0.0446\n",
      "Epoch 22/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0560 - val_loss: 0.0439\n",
      "Epoch 23/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0558 - val_loss: 0.0440\n",
      "Epoch 24/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0452\n",
      "Epoch 25/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0559 - val_loss: 0.0458\n",
      "Epoch 26/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0559 - val_loss: 0.0454\n",
      "Epoch 27/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0438\n",
      "Epoch 28/100\n",
      "33340/33340 [==============================] - 2s 58us/sample - loss: 0.0557 - val_loss: 0.0441\n",
      "Epoch 29/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0450\n",
      "Epoch 30/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0449\n",
      "Epoch 31/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0557 - val_loss: 0.0440\n",
      "Epoch 32/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0557 - val_loss: 0.0445\n",
      "Epoch 33/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0460\n",
      "Epoch 34/100\n",
      "33340/33340 [==============================] - 2s 61us/sample - loss: 0.0556 - val_loss: 0.0454\n",
      "Epoch 35/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0446\n",
      "Epoch 36/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0438\n",
      "Epoch 37/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0555 - val_loss: 0.0441\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 8\n",
      "(33340, 4, 7)\n",
      "(33340, 32)\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 155us/sample - loss: 0.1005 - val_loss: 0.0471\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0596 - val_loss: 0.0460\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0576 - val_loss: 0.0461\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0567 - val_loss: 0.0445\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0563 - val_loss: 0.0442\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.0564 - val_loss: 0.0452\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0561 - val_loss: 0.0450\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0561 - val_loss: 0.0452\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0434\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0451\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0559 - val_loss: 0.0448\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0561 - val_loss: 0.0443\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0451\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.0559 - val_loss: 0.0452\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0437\n",
      "Epoch 16/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0438\n",
      "Epoch 17/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0557 - val_loss: 0.0441\n",
      "Epoch 18/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0558 - val_loss: 0.0445\n",
      "Epoch 19/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0558 - val_loss: 0.0451\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 16\n",
      "(33340, 4, 7)\n",
      "(33340, 32)\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 155us/sample - loss: 0.1231 - val_loss: 0.0461\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0582 - val_loss: 0.0455\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0570 - val_loss: 0.0458\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0568 - val_loss: 0.0442\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0564 - val_loss: 0.0429\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0564 - val_loss: 0.0449\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0563 - val_loss: 0.0448\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0463\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0561 - val_loss: 0.0436\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0561 - val_loss: 0.0461\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0450\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0562 - val_loss: 0.0449\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0561 - val_loss: 0.0443\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0467\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0560 - val_loss: 0.0440\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 32\n",
      "(33340, 4, 7)\n",
      "(33340, 32)\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 145us/sample - loss: 0.0906 - val_loss: 0.0476\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0580 - val_loss: 0.0451\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0567 - val_loss: 0.0447\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0564 - val_loss: 0.0438\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0560 - val_loss: 0.0431\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0447\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0559 - val_loss: 0.0449\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0560 - val_loss: 0.0454\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0557 - val_loss: 0.0439\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 52us/sample - loss: 0.0559 - val_loss: 0.0455\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0446\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0452\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0557 - val_loss: 0.0444\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0458\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0557 - val_loss: 0.0442\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 64\n",
      "(33340, 4, 7)\n",
      "(33340, 32)\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 143us/sample - loss: 0.0901 - val_loss: 0.0445\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0571 - val_loss: 0.0448\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0455\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0445\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0432\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0561 - val_loss: 0.0448\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0442\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0558 - val_loss: 0.0455\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0438\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0456\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0556 - val_loss: 0.0444\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0558 - val_loss: 0.0451\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0442\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0453\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0447\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 7, units: 128\n",
      "(33340, 4, 7)\n",
      "(33340, 32)\n",
      "Train on 33340 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33340/33340 [==============================] - 5s 161us/sample - loss: 0.0810 - val_loss: 0.0443\n",
      "Epoch 2/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0583 - val_loss: 0.0442\n",
      "Epoch 3/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0571 - val_loss: 0.0442\n",
      "Epoch 4/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0564 - val_loss: 0.0440\n",
      "Epoch 5/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0560 - val_loss: 0.0432\n",
      "Epoch 6/100\n",
      "33340/33340 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0457\n",
      "Epoch 7/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0558 - val_loss: 0.0448\n",
      "Epoch 8/100\n",
      "33340/33340 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0450\n",
      "Epoch 9/100\n",
      "33340/33340 [==============================] - 2s 54us/sample - loss: 0.0556 - val_loss: 0.0437\n",
      "Epoch 10/100\n",
      "33340/33340 [==============================] - 2s 51us/sample - loss: 0.0557 - val_loss: 0.0451\n",
      "Epoch 11/100\n",
      "33340/33340 [==============================] - 2s 53us/sample - loss: 0.0556 - val_loss: 0.0442\n",
      "Epoch 12/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0451\n",
      "Epoch 13/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0440\n",
      "Epoch 14/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0464\n",
      "Epoch 15/100\n",
      "33340/33340 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0445\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 2\n",
      "(33200, 4, 14)\n",
      "(33200, 32)\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 5s 144us/sample - loss: 0.0945 - val_loss: 0.0521\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0600 - val_loss: 0.0487\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0577 - val_loss: 0.0485\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0570 - val_loss: 0.0469\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0568 - val_loss: 0.0458\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 53us/sample - loss: 0.0566 - val_loss: 0.0449\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0566 - val_loss: 0.0466\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0564 - val_loss: 0.0460\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 53us/sample - loss: 0.0562 - val_loss: 0.0448\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0562 - val_loss: 0.0454\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0561 - val_loss: 0.0460\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0561 - val_loss: 0.0459\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0561 - val_loss: 0.0472\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0444\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0559 - val_loss: 0.0436\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0558 - val_loss: 0.0429\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0448\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0558 - val_loss: 0.0438\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 50us/sample - loss: 0.0557 - val_loss: 0.0445\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0558 - val_loss: 0.0443\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0447\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 51us/sample - loss: 0.0556 - val_loss: 0.0450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0444\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0438\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0438\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0446\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 4\n",
      "(33200, 4, 14)\n",
      "(33200, 32)\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 5s 141us/sample - loss: 0.1003 - val_loss: 0.0502\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0602 - val_loss: 0.0476\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0578 - val_loss: 0.0463\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0567 - val_loss: 0.0467\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0564 - val_loss: 0.0454\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0561 - val_loss: 0.0442\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0455\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0561 - val_loss: 0.0450\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0445\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0558 - val_loss: 0.0446\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0460\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0451\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0458\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0444\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0555 - val_loss: 0.0432\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0426\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0556 - val_loss: 0.0447\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0433\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0446\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0556 - val_loss: 0.0446\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0555 - val_loss: 0.0443\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0553 - val_loss: 0.0446\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0444\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0555 - val_loss: 0.0443\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0555 - val_loss: 0.0444\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0553 - val_loss: 0.0445\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 8\n",
      "(33200, 4, 14)\n",
      "(33200, 32)\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 5s 160us/sample - loss: 0.0921 - val_loss: 0.0486\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0591 - val_loss: 0.0461\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0570 - val_loss: 0.0477\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0563 - val_loss: 0.0458\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0444\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0445\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0559 - val_loss: 0.0452\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0559 - val_loss: 0.0441\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0557 - val_loss: 0.0440\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0557 - val_loss: 0.0449\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0456\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0558 - val_loss: 0.0456\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0459\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0447\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0445\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0427\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0555 - val_loss: 0.0445\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0427\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0553 - val_loss: 0.0447\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0435\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0554 - val_loss: 0.0448\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0553 - val_loss: 0.0444\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0441\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0553 - val_loss: 0.0450\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0438\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0552 - val_loss: 0.0445\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 16\n",
      "(33200, 4, 14)\n",
      "(33200, 32)\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 5s 139us/sample - loss: 0.1008 - val_loss: 0.0459\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0597 - val_loss: 0.0462\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0577 - val_loss: 0.0451\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0563 - val_loss: 0.0465\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0558 - val_loss: 0.0459\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0449\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 52us/sample - loss: 0.0557 - val_loss: 0.0459\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0447\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0443\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0555 - val_loss: 0.0453\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0455\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0556 - val_loss: 0.0454\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0553 - val_loss: 0.0459\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0554 - val_loss: 0.0442\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0552 - val_loss: 0.0446\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0427\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0552 - val_loss: 0.0451\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0552 - val_loss: 0.0440\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0551 - val_loss: 0.0448\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0552 - val_loss: 0.0441\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0551 - val_loss: 0.0445\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0550 - val_loss: 0.0444\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0552 - val_loss: 0.0441\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0551 - val_loss: 0.0448\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0551 - val_loss: 0.0439\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0549 - val_loss: 0.0446\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 32\n",
      "(33200, 4, 14)\n",
      "(33200, 32)\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 5s 144us/sample - loss: 0.0899 - val_loss: 0.0456\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0591 - val_loss: 0.0444\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0574 - val_loss: 0.0464\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0562 - val_loss: 0.0456\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0454\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0445\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0556 - val_loss: 0.0456\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0556 - val_loss: 0.0443\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0554 - val_loss: 0.0438\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0446\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0458\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0555 - val_loss: 0.0453\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0551 - val_loss: 0.0450\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0553 - val_loss: 0.0438\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0551 - val_loss: 0.0440\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 52us/sample - loss: 0.0552 - val_loss: 0.0428\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0551 - val_loss: 0.0437\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0551 - val_loss: 0.0433\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0550 - val_loss: 0.0436\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0551 - val_loss: 0.0440\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0550 - val_loss: 0.0449\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0549 - val_loss: 0.0449\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0551 - val_loss: 0.0441\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 52us/sample - loss: 0.0550 - val_loss: 0.0450\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 52us/sample - loss: 0.0549 - val_loss: 0.0433\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 53us/sample - loss: 0.0548 - val_loss: 0.0445\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 64\n",
      "(33200, 4, 14)\n",
      "(33200, 32)\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 5s 162us/sample - loss: 0.0821 - val_loss: 0.0447\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0584 - val_loss: 0.0454\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0571 - val_loss: 0.0454\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0561 - val_loss: 0.0463\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0556 - val_loss: 0.0447\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0553 - val_loss: 0.0442\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0451\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0446\n",
      "Epoch 9/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0553 - val_loss: 0.0453\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0555 - val_loss: 0.0447\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0553 - val_loss: 0.0453\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0440\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0551 - val_loss: 0.0455\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0552 - val_loss: 0.0443\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0550 - val_loss: 0.0436\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0550 - val_loss: 0.0430\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0550 - val_loss: 0.0434\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0550 - val_loss: 0.0431\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0549 - val_loss: 0.0442\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0549 - val_loss: 0.0441\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0549 - val_loss: 0.0448\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0548 - val_loss: 0.0451\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0549 - val_loss: 0.0435\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 53us/sample - loss: 0.0548 - val_loss: 0.0454\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0547 - val_loss: 0.0439\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0547 - val_loss: 0.0445\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 1, lag: 14, units: 128\n",
      "(33200, 4, 14)\n",
      "(33200, 32)\n",
      "Train on 33200 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "33200/33200 [==============================] - 5s 144us/sample - loss: 0.0807 - val_loss: 0.0459\n",
      "Epoch 2/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0574 - val_loss: 0.0444\n",
      "Epoch 3/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0459\n",
      "Epoch 4/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0557 - val_loss: 0.0454\n",
      "Epoch 5/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0555 - val_loss: 0.0451\n",
      "Epoch 6/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0554 - val_loss: 0.0442\n",
      "Epoch 7/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0553 - val_loss: 0.0452\n",
      "Epoch 8/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0554 - val_loss: 0.0446\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33200/33200 [==============================] - 2s 53us/sample - loss: 0.0552 - val_loss: 0.0452\n",
      "Epoch 10/100\n",
      "33200/33200 [==============================] - 2s 54us/sample - loss: 0.0554 - val_loss: 0.0446\n",
      "Epoch 11/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0552 - val_loss: 0.0460\n",
      "Epoch 12/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0551 - val_loss: 0.0442\n",
      "Epoch 13/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0550 - val_loss: 0.0451\n",
      "Epoch 14/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0551 - val_loss: 0.0438\n",
      "Epoch 15/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0550 - val_loss: 0.0438\n",
      "Epoch 16/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0551 - val_loss: 0.0429\n",
      "Epoch 17/100\n",
      "33200/33200 [==============================] - 2s 59us/sample - loss: 0.0550 - val_loss: 0.0443\n",
      "Epoch 18/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0550 - val_loss: 0.0436\n",
      "Epoch 19/100\n",
      "33200/33200 [==============================] - 2s 57us/sample - loss: 0.0547 - val_loss: 0.0440\n",
      "Epoch 20/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0548 - val_loss: 0.0444\n",
      "Epoch 21/100\n",
      "33200/33200 [==============================] - 2s 48us/sample - loss: 0.0548 - val_loss: 0.0462\n",
      "Epoch 22/100\n",
      "33200/33200 [==============================] - 2s 58us/sample - loss: 0.0547 - val_loss: 0.0443\n",
      "Epoch 23/100\n",
      "33200/33200 [==============================] - 2s 61us/sample - loss: 0.0549 - val_loss: 0.0437\n",
      "Epoch 24/100\n",
      "33200/33200 [==============================] - 2s 56us/sample - loss: 0.0548 - val_loss: 0.0451\n",
      "Epoch 25/100\n",
      "33200/33200 [==============================] - 2s 60us/sample - loss: 0.0547 - val_loss: 0.0437\n",
      "Epoch 26/100\n",
      "33200/33200 [==============================] - 2s 55us/sample - loss: 0.0546 - val_loss: 0.0446\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 2\n",
      "(33960, 4, 7)\n",
      "(33960, 32)\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 5s 151us/sample - loss: 0.0990 - val_loss: 0.0623\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 50us/sample - loss: 0.0632 - val_loss: 0.0560\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 49us/sample - loss: 0.0594 - val_loss: 0.0558\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0579 - val_loss: 0.0562\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0573 - val_loss: 0.0555\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0569 - val_loss: 0.0530\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0567 - val_loss: 0.0522\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 53us/sample - loss: 0.0564 - val_loss: 0.0528\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0563 - val_loss: 0.0517\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 53us/sample - loss: 0.0563 - val_loss: 0.0541\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0563 - val_loss: 0.0529\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0561 - val_loss: 0.0514\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0562 - val_loss: 0.0529\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0560 - val_loss: 0.0528\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0534\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0544\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0519\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0534\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0513\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0520\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0526\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0553\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0527\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0537\n",
      "Epoch 25/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0557 - val_loss: 0.0530\n",
      "Epoch 26/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0544\n",
      "Epoch 27/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.0556 - val_loss: 0.0530\n",
      "Epoch 28/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0555 - val_loss: 0.0528\n",
      "Epoch 29/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0555 - val_loss: 0.0529\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 4\n",
      "(33960, 4, 7)\n",
      "(33960, 32)\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 5s 136us/sample - loss: 0.1420 - val_loss: 0.0621\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0615 - val_loss: 0.0540\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0587 - val_loss: 0.0553\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0571 - val_loss: 0.0556\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0565 - val_loss: 0.0539\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0561 - val_loss: 0.0526\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0561 - val_loss: 0.0519\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0559 - val_loss: 0.0537\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0560 - val_loss: 0.0514\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0533\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0530\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0558 - val_loss: 0.0520\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0528\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0513\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0557 - val_loss: 0.0528\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0557 - val_loss: 0.0535\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 53us/sample - loss: 0.0557 - val_loss: 0.0524\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0558 - val_loss: 0.0535\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0557 - val_loss: 0.0515\n",
      "Epoch 20/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0556 - val_loss: 0.0515\n",
      "Epoch 21/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0556 - val_loss: 0.0522\n",
      "Epoch 22/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0556 - val_loss: 0.0543\n",
      "Epoch 23/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0531\n",
      "Epoch 24/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0555 - val_loss: 0.0527\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 8\n",
      "(33960, 4, 7)\n",
      "(33960, 32)\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33960/33960 [==============================] - 5s 143us/sample - loss: 0.0797 - val_loss: 0.0567\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0582 - val_loss: 0.0554\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0568 - val_loss: 0.0554\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0566 - val_loss: 0.0553\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0564 - val_loss: 0.0545\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0535\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0526\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0535\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0558 - val_loss: 0.0519\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 53us/sample - loss: 0.0557 - val_loss: 0.0538\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0559 - val_loss: 0.0530\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0556 - val_loss: 0.0520\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 61us/sample - loss: 0.0557 - val_loss: 0.0529\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0556 - val_loss: 0.0534\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0535\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0554\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0556 - val_loss: 0.0531\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0555 - val_loss: 0.0545\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0524\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 16\n",
      "(33960, 4, 7)\n",
      "(33960, 32)\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 5s 152us/sample - loss: 0.0778 - val_loss: 0.0548\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0576 - val_loss: 0.0518\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0566 - val_loss: 0.0539\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0534\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0530\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0558 - val_loss: 0.0522\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0512\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0557 - val_loss: 0.0531\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0556 - val_loss: 0.0504\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 60us/sample - loss: 0.0557 - val_loss: 0.0527\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 51us/sample - loss: 0.0558 - val_loss: 0.0533\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0557 - val_loss: 0.0508\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0515\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 53us/sample - loss: 0.0557 - val_loss: 0.0521\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 53us/sample - loss: 0.0556 - val_loss: 0.0532\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0540\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0518\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0555 - val_loss: 0.0529\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0513\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 32\n",
      "(33960, 4, 7)\n",
      "(33960, 32)\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 5s 139us/sample - loss: 0.0846 - val_loss: 0.0542\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0579 - val_loss: 0.0512\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0568 - val_loss: 0.0540\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0531\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0529\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 53us/sample - loss: 0.0557 - val_loss: 0.0521\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 52us/sample - loss: 0.0556 - val_loss: 0.0514\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0519\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0555 - val_loss: 0.0506\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0555 - val_loss: 0.0527\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0526\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 52us/sample - loss: 0.0555 - val_loss: 0.0508\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 53us/sample - loss: 0.0555 - val_loss: 0.0511\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 53us/sample - loss: 0.0555 - val_loss: 0.0513\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0523\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0554 - val_loss: 0.0547\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0555 - val_loss: 0.0509\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0524\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 52us/sample - loss: 0.0553 - val_loss: 0.0515\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 64\n",
      "(33960, 4, 7)\n",
      "(33960, 32)\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 5s 157us/sample - loss: 0.0769 - val_loss: 0.0540\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0581 - val_loss: 0.0517\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0568 - val_loss: 0.0535\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0560 - val_loss: 0.0534\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0558 - val_loss: 0.0524\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0555 - val_loss: 0.0528\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0512\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0518\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 54us/sample - loss: 0.0554 - val_loss: 0.0507\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0554 - val_loss: 0.0534\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0554 - val_loss: 0.0531\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0515\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 49us/sample - loss: 0.0554 - val_loss: 0.0510\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 51us/sample - loss: 0.0554 - val_loss: 0.0515\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33960/33960 [==============================] - 2s 59us/sample - loss: 0.0553 - val_loss: 0.0534\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 51us/sample - loss: 0.0552 - val_loss: 0.0537\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0553 - val_loss: 0.0512\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0552 - val_loss: 0.0523\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0552 - val_loss: 0.0515\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 7, units: 128\n",
      "(33960, 4, 7)\n",
      "(33960, 32)\n",
      "Train on 33960 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33960/33960 [==============================] - 5s 141us/sample - loss: 0.0844 - val_loss: 0.0538\n",
      "Epoch 2/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0581 - val_loss: 0.0525\n",
      "Epoch 3/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0568 - val_loss: 0.0533\n",
      "Epoch 4/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0562 - val_loss: 0.0526\n",
      "Epoch 5/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0557 - val_loss: 0.0529\n",
      "Epoch 6/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0556 - val_loss: 0.0512\n",
      "Epoch 7/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0555 - val_loss: 0.0508\n",
      "Epoch 8/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0554 - val_loss: 0.0530\n",
      "Epoch 9/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0503\n",
      "Epoch 10/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0553 - val_loss: 0.0539\n",
      "Epoch 11/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0517\n",
      "Epoch 12/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0511\n",
      "Epoch 13/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0553 - val_loss: 0.0507\n",
      "Epoch 14/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0520\n",
      "Epoch 15/100\n",
      "33960/33960 [==============================] - 2s 55us/sample - loss: 0.0553 - val_loss: 0.0530\n",
      "Epoch 16/100\n",
      "33960/33960 [==============================] - 2s 56us/sample - loss: 0.0552 - val_loss: 0.0541\n",
      "Epoch 17/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0552 - val_loss: 0.0510\n",
      "Epoch 18/100\n",
      "33960/33960 [==============================] - 2s 58us/sample - loss: 0.0552 - val_loss: 0.0531\n",
      "Epoch 19/100\n",
      "33960/33960 [==============================] - 2s 57us/sample - loss: 0.0551 - val_loss: 0.0516\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 2\n",
      "(33820, 4, 14)\n",
      "(33820, 32)\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 5s 158us/sample - loss: 0.1176 - val_loss: 0.0597\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0625 - val_loss: 0.0558\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0594 - val_loss: 0.0550\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0574 - val_loss: 0.0539\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0567 - val_loss: 0.0533\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0563 - val_loss: 0.0534\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0563 - val_loss: 0.0521\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0559 - val_loss: 0.0524\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 58us/sample - loss: 0.0559 - val_loss: 0.0533\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 60us/sample - loss: 0.0561 - val_loss: 0.0502\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0525\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0568\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0529\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0530\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0535\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0510\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0516\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0555 - val_loss: 0.0537\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0556 - val_loss: 0.0508\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0517\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 4\n",
      "(33820, 4, 14)\n",
      "(33820, 32)\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 5s 141us/sample - loss: 0.0857 - val_loss: 0.0628\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0604 - val_loss: 0.0543\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 58us/sample - loss: 0.0576 - val_loss: 0.0552\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0566 - val_loss: 0.0524\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0563 - val_loss: 0.0533\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0562 - val_loss: 0.0544\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0564 - val_loss: 0.0517\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0534\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0560 - val_loss: 0.0532\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0505\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0559 - val_loss: 0.0537\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0575\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0557 - val_loss: 0.0535\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0524\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0557 - val_loss: 0.0537\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0518\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0555 - val_loss: 0.0521\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0556 - val_loss: 0.0540\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 52us/sample - loss: 0.0556 - val_loss: 0.0511\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0555 - val_loss: 0.0523\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 8\n",
      "(33820, 4, 14)\n",
      "(33820, 32)\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 5s 143us/sample - loss: 0.0899 - val_loss: 0.0596\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0591 - val_loss: 0.0562\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0572 - val_loss: 0.0561\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0560 - val_loss: 0.0551\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0558 - val_loss: 0.0549\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0558 - val_loss: 0.0545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0534\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0538\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0530\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0508\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 61us/sample - loss: 0.0555 - val_loss: 0.0534\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 58us/sample - loss: 0.0553 - val_loss: 0.0567\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0541\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0553 - val_loss: 0.0536\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0554 - val_loss: 0.0537\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0554 - val_loss: 0.0527\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0551 - val_loss: 0.0519\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0551 - val_loss: 0.0532\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 52us/sample - loss: 0.0552 - val_loss: 0.0515\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0552 - val_loss: 0.0526\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 16\n",
      "(33820, 4, 14)\n",
      "(33820, 32)\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 5s 139us/sample - loss: 0.0912 - val_loss: 0.0561\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0577 - val_loss: 0.0541\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0563 - val_loss: 0.0544\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0557 - val_loss: 0.0546\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0535\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0556 - val_loss: 0.0537\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0556 - val_loss: 0.0525\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0552 - val_loss: 0.0536\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0528\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0553 - val_loss: 0.0508\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0542\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 58us/sample - loss: 0.0551 - val_loss: 0.0560\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0552 - val_loss: 0.0535\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0551 - val_loss: 0.0539\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0552 - val_loss: 0.0535\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 48us/sample - loss: 0.0551 - val_loss: 0.0521\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 51us/sample - loss: 0.0550 - val_loss: 0.0523\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0550 - val_loss: 0.0539\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0550 - val_loss: 0.0512\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0550 - val_loss: 0.0526\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 32\n",
      "(33820, 4, 14)\n",
      "(33820, 32)\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 5s 142us/sample - loss: 0.0942 - val_loss: 0.0592\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0581 - val_loss: 0.0545\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0566 - val_loss: 0.0547\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0556 - val_loss: 0.0531\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 50us/sample - loss: 0.0556 - val_loss: 0.0538\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0554 - val_loss: 0.0538\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 50us/sample - loss: 0.0556 - val_loss: 0.0514\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 50us/sample - loss: 0.0551 - val_loss: 0.0532\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0553 - val_loss: 0.0525\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0553 - val_loss: 0.0500\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0532\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 58us/sample - loss: 0.0551 - val_loss: 0.0564\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 50us/sample - loss: 0.0551 - val_loss: 0.0527\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 52us/sample - loss: 0.0551 - val_loss: 0.0521\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 52us/sample - loss: 0.0552 - val_loss: 0.0537\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0551 - val_loss: 0.0518\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0549 - val_loss: 0.0515\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0549 - val_loss: 0.0533\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0549 - val_loss: 0.0508\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0550 - val_loss: 0.0535\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 64\n",
      "(33820, 4, 14)\n",
      "(33820, 32)\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 5s 144us/sample - loss: 0.0975 - val_loss: 0.0578\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0579 - val_loss: 0.0535\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0565 - val_loss: 0.0554\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0555 - val_loss: 0.0520\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0535\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0553 - val_loss: 0.0533\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0553 - val_loss: 0.0518\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0550 - val_loss: 0.0541\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0552 - val_loss: 0.0522\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0551 - val_loss: 0.0509\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0553 - val_loss: 0.0533\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0550 - val_loss: 0.0552\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0550 - val_loss: 0.0530\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0550 - val_loss: 0.0524\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 59us/sample - loss: 0.0549 - val_loss: 0.0532\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0550 - val_loss: 0.0519\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0548 - val_loss: 0.0513\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0548 - val_loss: 0.0527\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0547 - val_loss: 0.0519\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0549 - val_loss: 0.0531\n",
      " \n",
      " \n",
      " \n",
      "------------------------------------------------\n",
      "fold: 2, lag: 14, units: 128\n",
      "(33820, 4, 14)\n",
      "(33820, 32)\n",
      "Train on 33820 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "33820/33820 [==============================] - 5s 142us/sample - loss: 0.0828 - val_loss: 0.0557\n",
      "Epoch 2/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0578 - val_loss: 0.0517\n",
      "Epoch 3/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0568 - val_loss: 0.0530\n",
      "Epoch 4/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0525\n",
      "Epoch 5/100\n",
      "33820/33820 [==============================] - 2s 54us/sample - loss: 0.0555 - val_loss: 0.0526\n",
      "Epoch 6/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0552 - val_loss: 0.0549\n",
      "Epoch 7/100\n",
      "33820/33820 [==============================] - 2s 55us/sample - loss: 0.0552 - val_loss: 0.0519\n",
      "Epoch 8/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0551 - val_loss: 0.0553\n",
      "Epoch 9/100\n",
      "33820/33820 [==============================] - 2s 56us/sample - loss: 0.0552 - val_loss: 0.0516\n",
      "Epoch 10/100\n",
      "33820/33820 [==============================] - 2s 53us/sample - loss: 0.0551 - val_loss: 0.0497\n",
      "Epoch 11/100\n",
      "33820/33820 [==============================] - 2s 49us/sample - loss: 0.0551 - val_loss: 0.0540\n",
      "Epoch 12/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0549 - val_loss: 0.0571\n",
      "Epoch 13/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0549 - val_loss: 0.0533\n",
      "Epoch 14/100\n",
      "33820/33820 [==============================] - 2s 57us/sample - loss: 0.0551 - val_loss: 0.0530\n",
      "Epoch 15/100\n",
      "33820/33820 [==============================] - 2s 49us/sample - loss: 0.0549 - val_loss: 0.0529\n",
      "Epoch 16/100\n",
      "33820/33820 [==============================] - 2s 49us/sample - loss: 0.0549 - val_loss: 0.0520\n",
      "Epoch 17/100\n",
      "33820/33820 [==============================] - 2s 52us/sample - loss: 0.0546 - val_loss: 0.0513\n",
      "Epoch 18/100\n",
      "33820/33820 [==============================] - 2s 49us/sample - loss: 0.0546 - val_loss: 0.0532\n",
      "Epoch 19/100\n",
      "33820/33820 [==============================] - 2s 45us/sample - loss: 0.0546 - val_loss: 0.0514\n",
      "Epoch 20/100\n",
      "33820/33820 [==============================] - 2s 50us/sample - loss: 0.0547 - val_loss: 0.0549\n"
     ]
    }
   ],
   "source": [
    "lag_vec = [7,14]\n",
    "units_vec = [2,4,8,16,32,64,128]\n",
    "results = cross_validation(lag_vec, units_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2230.07202582 2227.60004647 2241.9073888  2226.84950502 2235.17100791\n",
      "   2236.38006277 2273.04289334]\n",
      "  [2243.29887656 2202.4095904  2225.0415358  2202.02976192 2180.85641735\n",
      "   2232.5752402  2220.69183074]]\n",
      "\n",
      " [[2152.62884521 2196.89255706 2180.27103409 2154.6271067  2163.19931818\n",
      "   2166.15857426 2166.94276182]\n",
      "  [2153.30577786 2140.06402037 2140.9067753  2144.28009821 2149.70217206\n",
      "   2158.30675836 2152.20136286]]\n",
      "\n",
      " [[2572.34536926 2572.41192607 2602.30591919 2528.93474609 2538.41148926\n",
      "   2543.05783691 2526.10300374]\n",
      "  [2518.30060628 2535.32968262 2549.98212199 2550.51872681 2508.69866089\n",
      "   2552.90308187 2495.56422648]]]\n",
      "14 32\n"
     ]
    }
   ],
   "source": [
    "MAE = results[2,:]\n",
    "print(MAE)\n",
    "MAE = numpy.sum(MAE, axis=0)\n",
    "ind = numpy.unravel_index(numpy.argmin(MAE, axis=None), MAE.shape)\n",
    "\n",
    "lag = lag_vec[ind[0]]\n",
    "units = units_vec[ind[1]]\n",
    "print(lag,units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34400, 4, 14)\n",
      "(34400, 32)\n",
      "Train on 34400 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "34400/34400 [==============================] - 5s 139us/sample - loss: 0.0848 - val_loss: 0.1088\n",
      "Epoch 2/100\n",
      "34400/34400 [==============================] - 2s 55us/sample - loss: 0.0593 - val_loss: 0.1109\n",
      "Epoch 3/100\n",
      "34400/34400 [==============================] - 2s 59us/sample - loss: 0.0580 - val_loss: 0.1139\n",
      "Epoch 4/100\n",
      "34400/34400 [==============================] - 2s 55us/sample - loss: 0.0565 - val_loss: 0.1161\n",
      "Epoch 5/100\n",
      "34400/34400 [==============================] - 2s 54us/sample - loss: 0.0555 - val_loss: 0.1132\n",
      "Epoch 6/100\n",
      "34400/34400 [==============================] - 2s 61us/sample - loss: 0.0553 - val_loss: 0.1159\n",
      "Epoch 7/100\n",
      "34400/34400 [==============================] - 2s 55us/sample - loss: 0.0553 - val_loss: 0.1187\n",
      "Epoch 8/100\n",
      "34400/34400 [==============================] - 2s 53us/sample - loss: 0.0552 - val_loss: 0.1156\n",
      "Epoch 9/100\n",
      "34400/34400 [==============================] - 2s 55us/sample - loss: 0.0552 - val_loss: 0.1198\n",
      "Epoch 10/100\n",
      "34400/34400 [==============================] - 2s 53us/sample - loss: 0.0549 - val_loss: 0.1198\n",
      "Epoch 11/100\n",
      "34400/34400 [==============================] - 2s 56us/sample - loss: 0.0552 - val_loss: 0.1237\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 9005.521484375\n",
      "MAPE: 0.13544052459240652\n",
      "MAE: 5458.952734768775\n",
      "R2 score: 0.5750997619809313\n",
      " \n",
      " \n",
      "---------------------------------------------------\n",
      "(35020, 4, 14)\n",
      "(35020, 32)\n",
      "Train on 35020 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "35020/35020 [==============================] - 5s 139us/sample - loss: 0.0924 - val_loss: 0.1267\n",
      "Epoch 2/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0613 - val_loss: 0.1115\n",
      "Epoch 3/100\n",
      "35020/35020 [==============================] - 2s 58us/sample - loss: 0.0595 - val_loss: 0.0942\n",
      "Epoch 4/100\n",
      "35020/35020 [==============================] - 2s 56us/sample - loss: 0.0580 - val_loss: 0.0824\n",
      "Epoch 5/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0569 - val_loss: 0.0771\n",
      "Epoch 6/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0564 - val_loss: 0.0748\n",
      "Epoch 7/100\n",
      "35020/35020 [==============================] - 2s 59us/sample - loss: 0.0563 - val_loss: 0.0741\n",
      "Epoch 8/100\n",
      "35020/35020 [==============================] - 2s 58us/sample - loss: 0.0561 - val_loss: 0.0729\n",
      "Epoch 9/100\n",
      "35020/35020 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0732\n",
      "Epoch 10/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0559 - val_loss: 0.0731\n",
      "Epoch 11/100\n",
      "35020/35020 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0719\n",
      "Epoch 12/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0557 - val_loss: 0.0754\n",
      "Epoch 13/100\n",
      "35020/35020 [==============================] - 2s 61us/sample - loss: 0.0556 - val_loss: 0.0708\n",
      "Epoch 14/100\n",
      "35020/35020 [==============================] - 2s 56us/sample - loss: 0.0559 - val_loss: 0.0721\n",
      "Epoch 15/100\n",
      "35020/35020 [==============================] - 2s 56us/sample - loss: 0.0557 - val_loss: 0.0729\n",
      "Epoch 16/100\n",
      "35020/35020 [==============================] - 2s 55us/sample - loss: 0.0556 - val_loss: 0.0732\n",
      "Epoch 17/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0558 - val_loss: 0.0708\n",
      "Epoch 18/100\n",
      "35020/35020 [==============================] - 2s 54us/sample - loss: 0.0556 - val_loss: 0.0734\n",
      "Epoch 19/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0557 - val_loss: 0.0692\n",
      "Epoch 20/100\n",
      "35020/35020 [==============================] - 2s 58us/sample - loss: 0.0555 - val_loss: 0.0748\n",
      "Epoch 21/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0698\n",
      "Epoch 22/100\n",
      "35020/35020 [==============================] - 2s 54us/sample - loss: 0.0555 - val_loss: 0.0712\n",
      "Epoch 23/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0555 - val_loss: 0.0704\n",
      "Epoch 24/100\n",
      "35020/35020 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0728\n",
      "Epoch 25/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0553 - val_loss: 0.0706\n",
      "Epoch 26/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0702\n",
      "Epoch 27/100\n",
      "35020/35020 [==============================] - 2s 57us/sample - loss: 0.0554 - val_loss: 0.0712\n",
      "Epoch 28/100\n",
      "35020/35020 [==============================] - 2s 61us/sample - loss: 0.0553 - val_loss: 0.0695\n",
      "Epoch 29/100\n",
      "35020/35020 [==============================] - 2s 56us/sample - loss: 0.0554 - val_loss: 0.0705\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 6912.88232421875\n",
      "MAPE: 0.5684369888264701\n",
      "MAE: 3474.889806315104\n",
      "R2 score: 0.36550463928954613\n",
      " \n",
      " \n",
      "---------------------------------------------------\n",
      "(35620, 4, 14)\n",
      "(35620, 32)\n",
      "Train on 35620 samples, validate on 620 samples\n",
      "Epoch 1/100\n",
      "35620/35620 [==============================] - 5s 133us/sample - loss: 0.0943 - val_loss: 0.0633\n",
      "Epoch 2/100\n",
      "35620/35620 [==============================] - 2s 52us/sample - loss: 0.0595 - val_loss: 0.0505\n",
      "Epoch 3/100\n",
      "35620/35620 [==============================] - 2s 53us/sample - loss: 0.0577 - val_loss: 0.0470\n",
      "Epoch 4/100\n",
      "35620/35620 [==============================] - 2s 59us/sample - loss: 0.0571 - val_loss: 0.0487\n",
      "Epoch 5/100\n",
      "35620/35620 [==============================] - 2s 57us/sample - loss: 0.0564 - val_loss: 0.0483\n",
      "Epoch 6/100\n",
      "35620/35620 [==============================] - 2s 56us/sample - loss: 0.0561 - val_loss: 0.0448\n",
      "Epoch 7/100\n",
      "35620/35620 [==============================] - 2s 55us/sample - loss: 0.0563 - val_loss: 0.0481\n",
      "Epoch 8/100\n",
      "35620/35620 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0476\n",
      "Epoch 9/100\n",
      "35620/35620 [==============================] - 2s 56us/sample - loss: 0.0560 - val_loss: 0.0453\n",
      "Epoch 10/100\n",
      "35620/35620 [==============================] - 2s 58us/sample - loss: 0.0562 - val_loss: 0.0483\n",
      "Epoch 11/100\n",
      "35620/35620 [==============================] - 2s 55us/sample - loss: 0.0562 - val_loss: 0.0482\n",
      "Epoch 12/100\n",
      "35620/35620 [==============================] - 2s 61us/sample - loss: 0.0559 - val_loss: 0.0459\n",
      "Epoch 13/100\n",
      "35620/35620 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0450\n",
      "Epoch 14/100\n",
      "35620/35620 [==============================] - 2s 54us/sample - loss: 0.0560 - val_loss: 0.0498\n",
      "Epoch 15/100\n",
      "35620/35620 [==============================] - 2s 55us/sample - loss: 0.0557 - val_loss: 0.0467\n",
      "Epoch 16/100\n",
      "35620/35620 [==============================] - 2s 55us/sample - loss: 0.0558 - val_loss: 0.0486\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 3637.6484375\n",
      "MAPE: 0.08769717987120056\n",
      "MAE: 2247.9561098160284\n",
      "R2 score: 0.7782731195330087\n",
      " \n",
      " \n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# lag=14\n",
    "ds=['october','november','december']\n",
    "\n",
    "data_LSTM_X1,data_LSTM_X2,data_LSTM_Y = sequence_data_build(data, lag)\n",
    "totday=int(data_LSTM_X1.shape[0]/20)\n",
    "m1 = [totday-61, totday-31, totday, ]\n",
    "m2 = [m1[0]-31, m1[1]-30, m1[2]-31, ]\n",
    "for i in range(len(m1)):\n",
    "    trainX1, trainY, testX1, testY= train_test_build(data_LSTM_X1, data_LSTM_Y, m1[i], m2[i])\n",
    "    trainX2, trainY, testX2, testY= train_test_build(data_LSTM_X2, data_LSTM_Y, m1[i], m2[i])\n",
    "    testYcopy=testY\n",
    "    saving = True\n",
    "    EarlyStop = True\n",
    "    rmse, mape, mae, r2, history = model_build(trainX1, trainX2, testX1, testX2, trainY, testY, units, saving, ds[i], EarlyStop)\n",
    "        \n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"Epochs\")\n",
    "    pyplot.ylabel(\"Mean absolute error\")\n",
    "    pyplot.savefig(\"hybrid_\" + ds[i] + \".png\")\n",
    "    pyplot.show()\n",
    "    \n",
    "    \n",
    "    print(\"Root mean square error: {0}\".format(rmse))\n",
    "    print(\"MAPE: {0}\".format(mape))\n",
    "    print(\"MAE: {0}\".format(mae))\n",
    "    print(\"R2 score: {0}\".format(r2))\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lag vector\n",
      "[7, 14]\n",
      "LSTM units\n",
      "[2, 4, 8, 16, 32, 64, 128]\n",
      "Cross validation results\n",
      "14 32\n",
      "----------------------------\n",
      "RMSE\n",
      "[[[3460.43310547 3434.00024414 3458.55175781 3413.17285156 3465.35400391\n",
      "   3445.47143555 3430.81958008]\n",
      "  [3427.24243164 3427.05126953 3443.65893555 3434.14477539 3427.80322266\n",
      "   3439.95507812 3408.34545898]]\n",
      "\n",
      " [[3383.65917969 3374.73876953 3307.85253906 3355.54760742 3372.09448242\n",
      "   3374.58447266 3347.2019043 ]\n",
      "  [3349.43505859 3341.66040039 3317.90429688 3335.59838867 3336.06640625\n",
      "   3328.46191406 3335.07470703]]\n",
      "\n",
      " [[4750.3984375  4552.99609375 5085.54248047 4656.02148438 4572.23046875\n",
      "   4593.26855469 4583.40820312]\n",
      "  [4475.43603516 4646.84716797 4923.10644531 4971.53173828 4588.23779297\n",
      "   4627.74511719 4553.87060547]]]\n",
      "----------------------------\n",
      "MAE\n",
      "[[[0.08596932 0.08549564 0.09036711 0.08804301 0.08720547 0.08830619\n",
      "   0.09711706]\n",
      "  [0.08577004 0.08458736 0.08665278 0.08677802 0.08582715 0.08804015\n",
      "   0.08732119]]\n",
      "\n",
      " [[0.08881125 0.09047517 0.08990961 0.08805847 0.09064738 0.09072737\n",
      "   0.08997544]\n",
      "  [0.09024627 0.08942253 0.09003587 0.08886342 0.08974146 0.08902469\n",
      "   0.08926488]]\n",
      "\n",
      " [[0.09722139 0.09284804 0.09301947 0.09179803 0.09170451 0.09202878\n",
      "   0.0914982 ]\n",
      "  [0.08830285 0.08849099 0.09033175 0.09180601 0.09121403 0.09873353\n",
      "   0.09162075]]]\n",
      "----------------------------\n",
      "MAPE\n",
      "[[[2230.07202582 2227.60004647 2241.9073888  2226.84950502 2235.17100791\n",
      "   2236.38006277 2273.04289334]\n",
      "  [2243.29887656 2202.4095904  2225.0415358  2202.02976192 2180.85641735\n",
      "   2232.5752402  2220.69183074]]\n",
      "\n",
      " [[2152.62884521 2196.89255706 2180.27103409 2154.6271067  2163.19931818\n",
      "   2166.15857426 2166.94276182]\n",
      "  [2153.30577786 2140.06402037 2140.9067753  2144.28009821 2149.70217206\n",
      "   2158.30675836 2152.20136286]]\n",
      "\n",
      " [[2572.34536926 2572.41192607 2602.30591919 2528.93474609 2538.41148926\n",
      "   2543.05783691 2526.10300374]\n",
      "  [2518.30060628 2535.32968262 2549.98212199 2550.51872681 2508.69866089\n",
      "   2552.90308187 2495.56422648]]]\n",
      "----------------------------\n",
      "R2-score\n",
      "[[[0.9119676  0.91330736 0.9120633  0.91435576 0.91171705 0.9127272\n",
      "   0.91346788]\n",
      "  [0.91364823 0.91365785 0.91281899 0.91330007 0.91361996 0.91300644\n",
      "   0.91459785]]\n",
      "\n",
      " [[0.91594463 0.91638724 0.91966874 0.9173355  0.91651822 0.91639489\n",
      "   0.91774618]\n",
      "  [0.91763638 0.9180183  0.9191798  0.91831549 0.91829255 0.91866463\n",
      "   0.91834113]]\n",
      "\n",
      " [[0.86621391 0.87710181 0.84667059 0.87147698 0.87606124 0.87491805\n",
      "   0.87545451]\n",
      "  [0.88125329 0.87198297 0.85630906 0.85346839 0.87519191 0.87303333\n",
      "   0.87705459]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"lag vector\")\n",
    "print(lag_vec)\n",
    "print(\"LSTM units\")\n",
    "print(units_vec)\n",
    "print(\"Cross validation results\")\n",
    "print(lag,units)\n",
    "print(\"----------------------------\")\n",
    "print(\"RMSE\")\n",
    "print(results[0,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"MAE\")\n",
    "print(results[1,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"MAPE\")\n",
    "print(results[2,:])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"R2-score\")\n",
    "print(results[3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
